[
  {
    "objectID": "CS5805.html",
    "href": "CS5805.html",
    "title": "CS5805",
    "section": "",
    "text": "The blog posts presented here are created as part of the requirmetns for CS5805 offerd for fall 24 at Virginia Tech. More infomration about me can be found on my personal website"
  },
  {
    "objectID": "posts/student-grade-prediction.html",
    "href": "posts/student-grade-prediction.html",
    "title": "Student Grade Prediction",
    "section": "",
    "text": "Banner Image Credit: Jernej Furman\nIn this blog I will use three different machine learning classification algorithms provided in skit-learn to predict student grade using data about students’ performance in quizzes, assignments and other features collected in Moodle LMS"
  },
  {
    "objectID": "posts/student-grade-prediction.html#preparation",
    "href": "posts/student-grade-prediction.html#preparation",
    "title": "Student Grade Prediction",
    "section": "1. Preparation",
    "text": "1. Preparation\n\n1.1 Install required packages\n\n\nRequirement already satisfied: numpy in /Users/yoseph/anaconda3/lib/python3.11/site-packages (1.24.3)\nCollecting sklearn\n  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... -\b \berror\n  error: subprocess-exited-with-error\n  \n  × python setup.py egg_info did not run successfully.\n  │ exit code: 1\n  ╰─&gt; [15 lines of output]\n      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n      rather than 'sklearn' for pip commands.\n      \n      Here is how to fix this error in the main use cases:\n      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n      - if the 'sklearn' package is used by one of your dependencies,\n        it would be great if you take some time to track which package uses\n        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n      - as a last resort, set the environment variable\n        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n      \n      More information is available at\n      https://github.com/scikit-learn/sklearn-pypi-package\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n× Encountered error while generating package metadata.\n╰─&gt; See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\n1.2 Perform the necessary imports\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn import preprocessing\n\n\nfrom sklearn.linear_model import (\n    LogisticRegression,\n    SGDClassifier,\n)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import (\n    BaggingClassifier,\n    GradientBoostingClassifier,\n    RandomForestClassifier,\n)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score,\n    # classification_report,\n    # mean_squared_error,\n    # r2_score,\n    # mean_absolute_error,\n    # confusion_matrix\n)\n\nSuppress Warnings\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)"
  },
  {
    "objectID": "posts/student-grade-prediction.html#data-understanding-and-preprocessing",
    "href": "posts/student-grade-prediction.html#data-understanding-and-preprocessing",
    "title": "Student Grade Prediction",
    "section": "2. Data Understanding and Preprocessing",
    "text": "2. Data Understanding and Preprocessing\n\n2.1 Data description\nThe data was collected from a fully online nine-week-long course on, hosted on the online learning management system Moodle. The dataset containes anonymized information relating to 107 enrolled students. The data included students’ grades (from 3 mini projects, 3 quizzes and 3 peer reviews and the final overall grade) as well as the course logs. The deadline for the three mini projects fell within weeks 3, 5 and 8 of the course, whereas the deadline for the quizzes fell within weeks 2, 4 and 8.\n\nStatus0: course / lectures / content related (Course module viewed, Course viewed, Course ac2vity comple2on updated, Course module instance list viewed, Content page viewed, Lesson started, Lesson resumed, Lesson restarted, Lesson ended)\nStatus1: assignment related (Quiz aPempt reviewed, Quiz aPempt submiPed, Quiz aPempt summary viewed, Quiz aPempt viewed, Quiz aPempt started, Ques2on answered, Ques2on viewed, Submission re-assessed, Submission assessed, Submission updated, Submission created, Submission viewed)\nStatus2: grade related (Grade user report viewed, Grade overview report viewed, User graded, Grade deleted, User profile viewed, Recent ac2vity viewed, User report viewed, Course user report viewed, Outline report viewed)\nStatus3: forum related (Post updated, Post created, Discussion created, Some content has been posted, Discussion viewed)\n9 grades (Week2_Quiz1, Week3_MP1, … Week7_MP3)\n36 logs (Week1_Stat0, Week1_Stat1, Week1_Stat2, Week1_Stat3, … Week9_Stat0, Week9_Stat1, Week9_Stat2, Week9_Stat3)\n\n\n\n2.2 Load the data\n\ndf = pd.read_csv(\"../data/MP2_Data.csv\")"
  },
  {
    "objectID": "posts/student-grade-prediction.html#try-to-learn-some-information-about-the-data",
    "href": "posts/student-grade-prediction.html#try-to-learn-some-information-about-the-data",
    "title": "Student Grade Prediction",
    "section": "2.3 Try to learn some information about the data",
    "text": "2.3 Try to learn some information about the data\n\nDisplay the first and last 5 rows\n\ndf.head()\n\n\n\n\n\n\n\n\nID\nWeek2_Quiz1\nWeek3_MP1\nWeek3_PR1\nWeek5_MP2\nWeek5_PR2\nWeek7_MP3\nWeek7_PR3\nWeek4_Quiz2\nWeek6_Quiz3\n...\nWeek7_Stat3\nWeek8_Stat0\nWeek8_Stat1\nWeek8_Stat2\nWeek8_Stat3\nWeek9_Stat0\nWeek9_Stat1\nWeek9_Stat2\nWeek9_Stat3\nGrade\n\n\n\n\n0\nML-2020-1\n5.00\n15.0\n5.0\n16.09\n5.00\n21.88\n5.0\n5.00\n5.0\n...\n0\n5\n4\n0\n4\n8\n6\n1\n0\n4\n\n\n1\nML-2020-2\n3.33\n15.0\n5.0\n17.83\n5.00\n22.27\n5.0\n4.00\n5.0\n...\n8\n5\n2\n0\n0\n25\n3\n2\n5\n4\n\n\n2\nML-2020-3\n1.67\n13.0\n5.0\n15.22\n5.00\n27.05\n2.5\n5.00\n5.0\n...\n0\n8\n2\n0\n0\n9\n0\n1\n0\n3\n\n\n3\nML-2020-4\n2.50\n14.0\n5.0\n10.00\n5.00\n31.02\n5.0\n3.13\n5.0\n...\n4\n10\n0\n0\n0\n7\n6\n0\n0\n3\n\n\n4\nML-2020-6\n0.00\n15.0\n5.0\n12.17\n4.93\n15.91\n5.0\n4.67\n5.0\n...\n6\n8\n5\n1\n1\n5\n3\n1\n0\n2\n\n\n\n\n5 rows × 48 columns\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nID\nWeek2_Quiz1\nWeek3_MP1\nWeek3_PR1\nWeek5_MP2\nWeek5_PR2\nWeek7_MP3\nWeek7_PR3\nWeek4_Quiz2\nWeek6_Quiz3\n...\nWeek7_Stat3\nWeek8_Stat0\nWeek8_Stat1\nWeek8_Stat2\nWeek8_Stat3\nWeek9_Stat0\nWeek9_Stat1\nWeek9_Stat2\nWeek9_Stat3\nGrade\n\n\n\n\n102\nML-2020-60\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0\n28\n0\n22\n0\n1\n0\n0\n0\n0\n\n\n103\nML-2020-58\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0\n0\n0\n0\n0\n3\n0\n0\n0\n0\n\n\n104\nML-2020-94\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n105\nML-2020-9\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0\n0\n0\n0\n0\n6\n0\n0\n0\n0\n\n\n106\nML-2020-86\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 48 columns\n\n\n\n\n\nDisplay details about each attribute\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 107 entries, 0 to 106\nData columns (total 48 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   ID           107 non-null    object \n 1   Week2_Quiz1  107 non-null    float64\n 2   Week3_MP1    107 non-null    float64\n 3   Week3_PR1    107 non-null    float64\n 4   Week5_MP2    107 non-null    float64\n 5   Week5_PR2    107 non-null    float64\n 6   Week7_MP3    107 non-null    float64\n 7   Week7_PR3    107 non-null    float64\n 8   Week4_Quiz2  107 non-null    float64\n 9   Week6_Quiz3  107 non-null    float64\n 10  Week8_Total  107 non-null    float64\n 11  Week1_Stat0  107 non-null    int64  \n 12  Week1_Stat1  107 non-null    int64  \n 13  Week1_Stat2  107 non-null    int64  \n 14  Week1_Stat3  107 non-null    int64  \n 15  Week2_Stat0  107 non-null    int64  \n 16  Week2_Stat1  107 non-null    int64  \n 17  Week2_Stat2  107 non-null    int64  \n 18  Week2_Stat3  107 non-null    int64  \n 19  Week3_Stat0  107 non-null    int64  \n 20  Week3_Stat1  107 non-null    int64  \n 21  Week3_Stat2  107 non-null    int64  \n 22  Week3_Stat3  107 non-null    int64  \n 23  Week4_Stat0  107 non-null    int64  \n 24  Week4_Stat1  107 non-null    int64  \n 25  Week4_Stat2  107 non-null    int64  \n 26  Week4_Stat3  107 non-null    int64  \n 27  Week5_Stat0  107 non-null    int64  \n 28  Week5_Stat1  107 non-null    int64  \n 29  Week5_Stat2  107 non-null    int64  \n 30  Week5_Stat3  107 non-null    int64  \n 31  Week6_Stat0  107 non-null    int64  \n 32  Week6_Stat1  107 non-null    int64  \n 33  Week6_Stat2  107 non-null    int64  \n 34  Week6_Stat3  107 non-null    int64  \n 35  Week7_Stat0  107 non-null    int64  \n 36  Week7_Stat1  107 non-null    int64  \n 37  Week7_Stat2  107 non-null    int64  \n 38  Week7_Stat3  107 non-null    int64  \n 39  Week8_Stat0  107 non-null    int64  \n 40  Week8_Stat1  107 non-null    int64  \n 41  Week8_Stat2  107 non-null    int64  \n 42  Week8_Stat3  107 non-null    int64  \n 43  Week9_Stat0  107 non-null    int64  \n 44  Week9_Stat1  107 non-null    int64  \n 45  Week9_Stat2  107 non-null    int64  \n 46  Week9_Stat3  107 non-null    int64  \n 47  Grade        107 non-null    int64  \ndtypes: float64(10), int64(37), object(1)\nmemory usage: 40.3+ KB\n\n\n\n\nSet ‘ID’ as the index column\n\ndf.set_index('ID', inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nWeek2_Quiz1\nWeek3_MP1\nWeek3_PR1\nWeek5_MP2\nWeek5_PR2\nWeek7_MP3\nWeek7_PR3\nWeek4_Quiz2\nWeek6_Quiz3\nWeek8_Total\n...\nWeek7_Stat3\nWeek8_Stat0\nWeek8_Stat1\nWeek8_Stat2\nWeek8_Stat3\nWeek9_Stat0\nWeek9_Stat1\nWeek9_Stat2\nWeek9_Stat3\nGrade\n\n\nID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML-2020-1\n5.00\n15.0\n5.0\n16.09\n5.00\n21.88\n5.0\n5.00\n5.0\n82.97\n...\n0\n5\n4\n0\n4\n8\n6\n1\n0\n4\n\n\nML-2020-2\n3.33\n15.0\n5.0\n17.83\n5.00\n22.27\n5.0\n4.00\n5.0\n82.43\n...\n8\n5\n2\n0\n0\n25\n3\n2\n5\n4\n\n\nML-2020-3\n1.67\n13.0\n5.0\n15.22\n5.00\n27.05\n2.5\n5.00\n5.0\n79.44\n...\n0\n8\n2\n0\n0\n9\n0\n1\n0\n3\n\n\nML-2020-4\n2.50\n14.0\n5.0\n10.00\n5.00\n31.02\n5.0\n3.13\n5.0\n80.65\n...\n4\n10\n0\n0\n0\n7\n6\n0\n0\n3\n\n\nML-2020-6\n0.00\n15.0\n5.0\n12.17\n4.93\n15.91\n5.0\n4.67\n5.0\n67.68\n...\n6\n8\n5\n1\n1\n5\n3\n1\n0\n2\n\n\n\n\n5 rows × 47 columns\n\n\n\nGet additional information about each column\n\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nWeek2_Quiz1\n107.0\n2.406636\n2.000317\n0.0\n0.0\n3.33\n4.170\n5.00\n\n\nWeek3_MP1\n107.0\n7.949626\n6.892312\n0.0\n0.0\n12.00\n14.305\n15.00\n\n\nWeek3_PR1\n107.0\n2.803738\n2.493158\n0.0\n0.0\n5.00\n5.000\n5.00\n\n\nWeek5_MP2\n107.0\n9.237757\n8.640610\n0.0\n0.0\n10.87\n18.045\n20.00\n\n\nWeek5_PR2\n107.0\n2.844673\n2.482099\n0.0\n0.0\n5.00\n5.000\n5.00\n\n\nWeek7_MP3\n107.0\n14.481869\n14.080211\n0.0\n0.0\n15.91\n27.440\n35.00\n\n\nWeek7_PR3\n107.0\n2.383178\n2.437501\n0.0\n0.0\n2.50\n5.000\n5.00\n\n\nWeek4_Quiz2\n107.0\n2.609439\n2.229419\n0.0\n0.0\n3.17\n4.710\n5.00\n\n\nWeek6_Quiz3\n107.0\n2.663551\n2.414359\n0.0\n0.0\n4.00\n5.000\n5.00\n\n\nWeek8_Total\n107.0\n47.380467\n41.035589\n0.0\n0.0\n71.53\n83.550\n99.71\n\n\nWeek1_Stat0\n107.0\n6.785047\n7.157300\n0.0\n1.0\n4.00\n12.000\n27.00\n\n\nWeek1_Stat1\n107.0\n0.000000\n0.000000\n0.0\n0.0\n0.00\n0.000\n0.00\n\n\nWeek1_Stat2\n107.0\n0.598131\n1.966031\n0.0\n0.0\n0.00\n0.000\n11.00\n\n\nWeek1_Stat3\n107.0\n0.224299\n0.730836\n0.0\n0.0\n0.00\n0.000\n6.00\n\n\nWeek2_Stat0\n107.0\n16.887850\n16.307960\n0.0\n6.5\n15.00\n24.000\n104.00\n\n\nWeek2_Stat1\n107.0\n13.299065\n11.308049\n0.0\n10.0\n11.00\n19.000\n57.00\n\n\nWeek2_Stat2\n107.0\n1.252336\n1.505295\n0.0\n1.0\n1.00\n1.000\n10.00\n\n\nWeek2_Stat3\n107.0\n1.841121\n4.202761\n0.0\n0.0\n0.00\n2.000\n25.00\n\n\nWeek3_Stat0\n107.0\n31.728972\n28.686191\n0.0\n3.0\n27.00\n47.500\n108.00\n\n\nWeek3_Stat1\n107.0\n2.822430\n3.206165\n0.0\n0.0\n4.00\n4.000\n15.00\n\n\nWeek3_Stat2\n107.0\n0.953271\n2.689544\n0.0\n0.0\n0.00\n0.000\n15.00\n\n\nWeek3_Stat3\n107.0\n1.130841\n2.269919\n0.0\n0.0\n0.00\n1.000\n14.00\n\n\nWeek4_Stat0\n107.0\n41.915888\n47.164330\n0.0\n0.5\n27.00\n74.500\n240.00\n\n\nWeek4_Stat1\n107.0\n16.046729\n17.040675\n0.0\n0.0\n15.00\n27.500\n87.00\n\n\nWeek4_Stat2\n107.0\n1.943925\n2.790928\n0.0\n0.0\n1.00\n2.000\n13.00\n\n\nWeek4_Stat3\n107.0\n1.009346\n2.806648\n0.0\n0.0\n0.00\n1.000\n24.00\n\n\nWeek5_Stat0\n107.0\n26.074766\n31.159269\n0.0\n0.0\n20.00\n40.500\n185.00\n\n\nWeek5_Stat1\n107.0\n5.009346\n6.568213\n0.0\n0.0\n4.00\n6.500\n39.00\n\n\nWeek5_Stat2\n107.0\n1.588785\n3.954823\n0.0\n0.0\n0.00\n1.000\n23.00\n\n\nWeek5_Stat3\n107.0\n0.663551\n1.822003\n0.0\n0.0\n0.00\n0.000\n11.00\n\n\nWeek6_Stat0\n107.0\n37.607477\n47.851334\n0.0\n0.0\n18.00\n65.000\n208.00\n\n\nWeek6_Stat1\n107.0\n14.271028\n14.009815\n0.0\n0.0\n15.00\n25.000\n51.00\n\n\nWeek6_Stat2\n107.0\n2.775701\n7.206271\n0.0\n0.0\n1.00\n2.000\n45.00\n\n\nWeek6_Stat3\n107.0\n0.411215\n1.220526\n0.0\n0.0\n0.00\n0.000\n7.00\n\n\nWeek7_Stat0\n107.0\n16.355140\n22.242341\n0.0\n0.0\n11.00\n27.000\n145.00\n\n\nWeek7_Stat1\n107.0\n3.242991\n5.001587\n0.0\n0.0\n0.00\n5.000\n24.00\n\n\nWeek7_Stat2\n107.0\n1.813084\n4.895379\n0.0\n0.0\n0.00\n1.000\n35.00\n\n\nWeek7_Stat3\n107.0\n1.252336\n2.399267\n0.0\n0.0\n0.00\n2.000\n12.00\n\n\nWeek8_Stat0\n107.0\n10.514019\n15.563846\n0.0\n0.0\n5.00\n14.000\n90.00\n\n\nWeek8_Stat1\n107.0\n3.130841\n4.841028\n0.0\n0.0\n0.00\n5.000\n27.00\n\n\nWeek8_Stat2\n107.0\n1.112150\n3.658351\n0.0\n0.0\n0.00\n0.000\n22.00\n\n\nWeek8_Stat3\n107.0\n0.355140\n1.191577\n0.0\n0.0\n0.00\n0.000\n9.00\n\n\nWeek9_Stat0\n107.0\n7.663551\n9.277630\n0.0\n1.0\n5.00\n11.000\n62.00\n\n\nWeek9_Stat1\n107.0\n1.607477\n2.687346\n0.0\n0.0\n0.00\n2.000\n12.00\n\n\nWeek9_Stat2\n107.0\n1.093458\n3.368928\n0.0\n0.0\n0.00\n0.500\n25.00\n\n\nWeek9_Stat3\n107.0\n0.046729\n0.483368\n0.0\n0.0\n0.00\n0.000\n5.00\n\n\nGrade\n107.0\n2.074766\n1.993863\n0.0\n0.0\n3.00\n4.000\n5.00\n\n\n\n\n\n\n\nFrom the description (in the pdf file) Week8_total seems to be a sum of the 9 grades, to check, I create a temporary column called sum and compare\n\ngrade_cols = ['Week2_Quiz1','Week4_Quiz2','Week6_Quiz3','Week3_MP1','Week5_MP2','Week7_MP3','Week3_PR1','Week5_PR2','Week7_PR3']\ndf['sum'] = df[grade_cols].sum(1)\ndf[['sum', 'Week8_Total']]\n\n\n\n\n\n\n\n\nsum\nWeek8_Total\n\n\nID\n\n\n\n\n\n\nML-2020-1\n82.97\n82.97\n\n\nML-2020-2\n82.43\n82.43\n\n\nML-2020-3\n79.44\n79.44\n\n\nML-2020-4\n80.65\n80.65\n\n\nML-2020-6\n67.68\n67.68\n\n\n...\n...\n...\n\n\nML-2020-60\n0.00\n0.00\n\n\nML-2020-58\n0.00\n0.00\n\n\nML-2020-94\n0.00\n0.00\n\n\nML-2020-9\n0.00\n0.00\n\n\nML-2020-86\n0.00\n0.00\n\n\n\n\n107 rows × 2 columns\n\n\n\nThese columns are the same representing the same data, hence, I can drop the individual values and keep to total. Also, since I do not plan to perform any time serious analysis (i.e., I’m not interested in the changes over weeks on the grades) I can keep the Week8_Total and drop the individual grade to reduce the features I need to consider. P.S: I also drop the temporary ‘sum’ colum\n\ngrade_cols.append('sum')\nprint(grade_cols)\ndf=df.drop(columns=grade_cols)\ndf.describe().T\n\n['Week2_Quiz1', 'Week4_Quiz2', 'Week6_Quiz3', 'Week3_MP1', 'Week5_MP2', 'Week7_MP3', 'Week3_PR1', 'Week5_PR2', 'Week7_PR3', 'sum']\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nWeek8_Total\n107.0\n47.380467\n41.035589\n0.0\n0.0\n71.53\n83.55\n99.71\n\n\nWeek1_Stat0\n107.0\n6.785047\n7.157300\n0.0\n1.0\n4.00\n12.00\n27.00\n\n\nWeek1_Stat1\n107.0\n0.000000\n0.000000\n0.0\n0.0\n0.00\n0.00\n0.00\n\n\nWeek1_Stat2\n107.0\n0.598131\n1.966031\n0.0\n0.0\n0.00\n0.00\n11.00\n\n\nWeek1_Stat3\n107.0\n0.224299\n0.730836\n0.0\n0.0\n0.00\n0.00\n6.00\n\n\nWeek2_Stat0\n107.0\n16.887850\n16.307960\n0.0\n6.5\n15.00\n24.00\n104.00\n\n\nWeek2_Stat1\n107.0\n13.299065\n11.308049\n0.0\n10.0\n11.00\n19.00\n57.00\n\n\nWeek2_Stat2\n107.0\n1.252336\n1.505295\n0.0\n1.0\n1.00\n1.00\n10.00\n\n\nWeek2_Stat3\n107.0\n1.841121\n4.202761\n0.0\n0.0\n0.00\n2.00\n25.00\n\n\nWeek3_Stat0\n107.0\n31.728972\n28.686191\n0.0\n3.0\n27.00\n47.50\n108.00\n\n\nWeek3_Stat1\n107.0\n2.822430\n3.206165\n0.0\n0.0\n4.00\n4.00\n15.00\n\n\nWeek3_Stat2\n107.0\n0.953271\n2.689544\n0.0\n0.0\n0.00\n0.00\n15.00\n\n\nWeek3_Stat3\n107.0\n1.130841\n2.269919\n0.0\n0.0\n0.00\n1.00\n14.00\n\n\nWeek4_Stat0\n107.0\n41.915888\n47.164330\n0.0\n0.5\n27.00\n74.50\n240.00\n\n\nWeek4_Stat1\n107.0\n16.046729\n17.040675\n0.0\n0.0\n15.00\n27.50\n87.00\n\n\nWeek4_Stat2\n107.0\n1.943925\n2.790928\n0.0\n0.0\n1.00\n2.00\n13.00\n\n\nWeek4_Stat3\n107.0\n1.009346\n2.806648\n0.0\n0.0\n0.00\n1.00\n24.00\n\n\nWeek5_Stat0\n107.0\n26.074766\n31.159269\n0.0\n0.0\n20.00\n40.50\n185.00\n\n\nWeek5_Stat1\n107.0\n5.009346\n6.568213\n0.0\n0.0\n4.00\n6.50\n39.00\n\n\nWeek5_Stat2\n107.0\n1.588785\n3.954823\n0.0\n0.0\n0.00\n1.00\n23.00\n\n\nWeek5_Stat3\n107.0\n0.663551\n1.822003\n0.0\n0.0\n0.00\n0.00\n11.00\n\n\nWeek6_Stat0\n107.0\n37.607477\n47.851334\n0.0\n0.0\n18.00\n65.00\n208.00\n\n\nWeek6_Stat1\n107.0\n14.271028\n14.009815\n0.0\n0.0\n15.00\n25.00\n51.00\n\n\nWeek6_Stat2\n107.0\n2.775701\n7.206271\n0.0\n0.0\n1.00\n2.00\n45.00\n\n\nWeek6_Stat3\n107.0\n0.411215\n1.220526\n0.0\n0.0\n0.00\n0.00\n7.00\n\n\nWeek7_Stat0\n107.0\n16.355140\n22.242341\n0.0\n0.0\n11.00\n27.00\n145.00\n\n\nWeek7_Stat1\n107.0\n3.242991\n5.001587\n0.0\n0.0\n0.00\n5.00\n24.00\n\n\nWeek7_Stat2\n107.0\n1.813084\n4.895379\n0.0\n0.0\n0.00\n1.00\n35.00\n\n\nWeek7_Stat3\n107.0\n1.252336\n2.399267\n0.0\n0.0\n0.00\n2.00\n12.00\n\n\nWeek8_Stat0\n107.0\n10.514019\n15.563846\n0.0\n0.0\n5.00\n14.00\n90.00\n\n\nWeek8_Stat1\n107.0\n3.130841\n4.841028\n0.0\n0.0\n0.00\n5.00\n27.00\n\n\nWeek8_Stat2\n107.0\n1.112150\n3.658351\n0.0\n0.0\n0.00\n0.00\n22.00\n\n\nWeek8_Stat3\n107.0\n0.355140\n1.191577\n0.0\n0.0\n0.00\n0.00\n9.00\n\n\nWeek9_Stat0\n107.0\n7.663551\n9.277630\n0.0\n1.0\n5.00\n11.00\n62.00\n\n\nWeek9_Stat1\n107.0\n1.607477\n2.687346\n0.0\n0.0\n0.00\n2.00\n12.00\n\n\nWeek9_Stat2\n107.0\n1.093458\n3.368928\n0.0\n0.0\n0.00\n0.50\n25.00\n\n\nWeek9_Stat3\n107.0\n0.046729\n0.483368\n0.0\n0.0\n0.00\n0.00\n5.00\n\n\nGrade\n107.0\n2.074766\n1.993863\n0.0\n0.0\n3.00\n4.00\n5.00\n\n\n\n\n\n\n\n\n\nPlot the number of students who scored each grade\n\nsns.countplot(df, x=\"Grade\")\n\n&lt;Axes: xlabel='Grade', ylabel='count'&gt;\n\n\n\n\n\n\n\nSee if there are missng values\n\nsns.heatmap(df.isnull())\n\n&lt;Axes: ylabel='ID'&gt;\n\n\n\n\n\nThere are no missing values for any of the students, perhaps missing a quiz or an assignment has been equated to a grade of zero\n\ncor = df.corr()\nsns.heatmap(cor)\n\ncor_target = abs(cor['Grade'])\n# Selecting highly correlated features\nrelevant_features = cor_target[cor_target&gt;0.7]\nrelevant_features\n\nWeek8_Total    0.972348\nWeek6_Stat1    0.771988\nGrade          1.000000\nName: Grade, dtype: float64\n\n\n\n\n\nWe see, as expected, Week8_total is a highly correlated to final Grade. Interestingly Week6_Stat1 seems to have a high correlation to Grade. This means, students who had high activity during the sixth week of the course had scored better on the final grade (I say they scored better because we can see below the correlation is positive ) ## 2.4 Preprocessing\n\ncor[\"Week6_Stat1\"][\"Grade\"]\n\n0.7719882914970566\n\n\nI can remove the Week8_Total grade from the features list if I can be sure that the final grade is full based on the this total grade or I can keep it in case other factors were considered in the final grade calculation. In this case the machine learning model I build will be highly dependent on this variable to the point the other features will became irrelevant given the high correlation value observed between this feature and the target (i.e., Grade)\n\n#df = df.drop(columns=['Week8_Total'], axis=1)\n\n\n#irrelevant_features = cor_target[(cor_target&gt;=0.4) & (cor_target&lt;=0.5)]\nirrelevant_features = cor_target[(cor_target&gt;=0.45) & (cor_target&lt;=0.55)]\nirrelevant_features\n\nWeek5_Stat1    0.484030\nWeek8_Stat0    0.450807\nWeek9_Stat0    0.545532\nWeek9_Stat1    0.496753\nName: Grade, dtype: float64\n\n\n\n#df = df.drop(columns=list(irrelevant_features.index), axis=1)"
  },
  {
    "objectID": "posts/student-grade-prediction.html#split-features-and-target-column",
    "href": "posts/student-grade-prediction.html#split-features-and-target-column",
    "title": "Student Grade Prediction",
    "section": "Split Features and target column",
    "text": "Split Features and target column\n\nx = df.drop(columns=['Grade'], axis=1).to_numpy()\ny = df['Grade'].to_numpy()\n\n\nNormalize the features to bring them all to 0-1 range\n\nx_normalized = preprocessing.normalize(x)\nx_normalized\n\narray([[0.72952054, 0.        , 0.        , ..., 0.05275549, 0.00879258,\n        0.        ],\n       [0.3833634 , 0.0372062 , 0.        , ..., 0.01395233, 0.00930155,\n        0.02325388],\n       [0.72356708, 0.03643339, 0.        , ..., 0.        , 0.00910835,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.29411765, 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.41380294, 0.        , ..., 0.        , 0.        ,\n        0.        ]])\n\n\n\n\nSplit training and Test data\n\nx_train, x_test, y_train, y_test = train_test_split(x_normalized,y, test_size=.20)\n\n\nprint(\"Training X Shape: {}\".format(x_train.shape))\nprint(\"Training Y Shape: {}\".format(y_train.shape))\nprint(\"Test X Shape: {}\".format(x_test.shape))\nprint(\"Test y Shape: {}\".format(y_test.shape))\n\nTraining X Shape: (85, 37)\nTraining Y Shape: (85,)\nTest X Shape: (22, 37)\nTest y Shape: (22,)\n\n\n\nSummary of Preprocessing\n\nFeature Selection\nGiven 107 records with 47 attributes I have removed 11 of the attributes during the pre-processing for the following reasons\n\nID: THis is a unique identifier of each row of the data that dose not contain relevant information for our prediction task.\ngrade_columns: These are columns that add up to give the Week8_Total column, since there is a high correlation between these and the total column I removed them in favor of Week8_Total which provides the same information in a single feature.\nWeek8_Total: I finally remove Week8_Total because it has a high correlation with the target feature which will make our prediction very skewed towards this feature. The is a design decision to avoid a deterministic prediction.\n\n\n\nNormalization\nThen I normalized the data to bring values for each feature between [0-1].\n\n\nTraining/Test Split\nI split the data into training and testing set of, first, 90% to 10% ratio. Even though it is recommended to use a 20% testing split, given our small data set I started with 10% and I came back to update this ratio based on my finding later which showed over-fitting."
  },
  {
    "objectID": "posts/student-grade-prediction.html#training",
    "href": "posts/student-grade-prediction.html#training",
    "title": "Student Grade Prediction",
    "section": "3. Training",
    "text": "3. Training\nThe problem of predicting one of the six grades given a set of features is a classification problem. I’ll first try to train and test multiple classifiers with their default hyperparameter values.\n\nmodels = {\n    'LogisticRegression':LogisticRegression(),\n    'MultinomialNB': MultinomialNB(),\n    'BaggingClassifier': BaggingClassifier(), \n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'LinearSVC':LinearSVC(),\n    'SGDClassifier':SGDClassifier(),\n    'KNeighborsClassifier':KNeighborsClassifier(), \n    'RandomForestClassifier':RandomForestClassifier(), \n    'GradientBoostingClassifier':GradientBoostingClassifier(),\n    \"SVC\":SVC(),\n    \"GaussianProcessClassifier\": GaussianProcessClassifier(),\n    \"MLPClassifier\": MLPClassifier(),\n    \"AdaBoostClassifier\": AdaBoostClassifier(),\n    \"GaussianNB\":GaussianNB(),\n    \"QuadraticDiscriminantAnalysis\":QuadraticDiscriminantAnalysis(),\n}\n\n\nperformance = pd.DataFrame(columns=[\"Train Accuracy\",\"Test Accuracy\"], index=list(models.keys()))\n\n\nfor name, model in models.items():\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Make predictions on the test set\n    y_train_pred = model.predict(x_train)\n    y_test_pred = model.predict(x_test)\n\n    # Calculate evaluation metrics\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n\n    #Save Test and Training Accuracies\n    performance.loc[name, :] = [train_accuracy,test_accuracy]"
  },
  {
    "objectID": "posts/student-grade-prediction.html#evaluation",
    "href": "posts/student-grade-prediction.html#evaluation",
    "title": "Student Grade Prediction",
    "section": "4. Evaluation",
    "text": "4. Evaluation\n\nperformance\n\n\n\n\n\n\n\n\nTrain Accuracy\nTest Accuracy\n\n\n\n\nLogisticRegression\n0.705882\n0.636364\n\n\nMultinomialNB\n0.611765\n0.772727\n\n\nBaggingClassifier\n0.976471\n0.636364\n\n\nDecisionTreeClassifier\n1.0\n0.636364\n\n\nLinearSVC\n0.764706\n0.590909\n\n\nSGDClassifier\n0.717647\n0.636364\n\n\nKNeighborsClassifier\n0.647059\n0.636364\n\n\nRandomForestClassifier\n1.0\n0.590909\n\n\nGradientBoostingClassifier\n1.0\n0.772727\n\n\nSVC\n0.682353\n0.681818\n\n\nGaussianProcessClassifier\n0.682353\n0.681818\n\n\nMLPClassifier\n0.776471\n0.636364\n\n\nAdaBoostClassifier\n0.647059\n0.772727\n\n\nGaussianNB\n0.752941\n0.5\n\n\nQuadraticDiscriminantAnalysis\n1.0\n0.636364"
  },
  {
    "objectID": "posts/student-grade-prediction.html#optimization",
    "href": "posts/student-grade-prediction.html#optimization",
    "title": "Student Grade Prediction",
    "section": "5. Optimization",
    "text": "5. Optimization\nFrom ourt trial run above SGDClassifier,LinearSVC and MLPClassifier seem to have the best accuracy score. Here I’ll try to optimize these algorithms with different hyperparameter values to improve accuracy. We do this with the GirdSearch algorithm for hyperparameter tuning.\n\nfrom sklearn.model_selection import GridSearchCV\n\n\nSGDClassifier\n\nmodel =  SGDClassifier()\n\ngrid = {\n        'alpha' :[0.0001,0.001, 0.01, 0.1, 1, 10]\n        }\ngridSearch = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, scoring=\"accuracy\")\nsearchResults = gridSearch.fit(x_train, y_train)\n\nscore = accuracy_score(searchResults.best_estimator_.predict(x_test),y_test)\nparams = searchResults.best_estimator_.get_params()\nprint(\"SGDClassifier  best estimator \\n alpha = {} \\n accuracy_score = {}\".format(params[\"alpha\"],score))\n\nSGDClassifier  best estimator \n alpha = 0.01 \n accuracy_score = 0.6363636363636364\n\n\n\n\nLinearSVC\n\nmodel = LinearSVC ()\ngrid = {'C':[0.0001,0.001, 0.01, 0.1, 1, 10]}\ngridSearch = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, scoring=\"accuracy\")\nsearchResults = gridSearch.fit(x_train, y_train)\n\nscore = accuracy_score(searchResults.best_estimator_.predict(x_test),y_test)\nparams = searchResults.best_estimator_.get_params()\nprint(\"LinearSVC best estimator \\n C = {} \\n accuracy_score = {}\".format(params[\"C\"],score))\n\nLinearSVC best estimator \n C = 0.1 \n accuracy_score = 0.6818181818181818\n\n\n\n\nMLPClassifier\n\nmodel = MLPClassifier()\n\ngrid={\n'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"],\n'alpha': [0.0001,0.001, 0.01, 0.1, 1, 10],\n}\n\ngridSearch = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, scoring=\"accuracy\")\nsearchResults = gridSearch.fit(x_train, y_train)\n\nscore = accuracy_score(searchResults.best_estimator_.predict(x_test),y_test)\nparams = searchResults.best_estimator_.get_params()\nlearning_rate = params[\"learning_rate\"]\nprint(\"MLPClassifier best estimator\")\nprint(\" alpha = {} \\n learning_rate = {}\".format(params['alpha'],params['learning_rate']))\nprint(\" accuracy_score = {}\".format(score))\n\nMLPClassifier best estimator\n alpha = 1 \n learning_rate = invscaling\n accuracy_score = 0.8181818181818182\n\n\nThe Best performance is obtained by LinearSVC classifier which is 82%"
  },
  {
    "objectID": "posts/pca.html",
    "href": "posts/pca.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Banner Image Credit: Jernej Furman"
  },
  {
    "objectID": "posts/pca.html#introduction",
    "href": "posts/pca.html#introduction",
    "title": "Principal Component Analysis",
    "section": "Introduction",
    "text": "Introduction\nPrincipal component analysis (PCA) is a widely used statistical method for examining extensive datasets characterized by numerous dimensions or features per observation. PCA is a relatively well-established theory with its roots dating back to the beginning of the 1900s. Its primary aim is to enhance data interpretability while conserving the maximum information possible, thereby enabling the visualization of multidimensional data. Essentially, PCA serves as a statistical approach to diminish dataset dimensionality. This is achieved by transforming the data through linear methods into a fresh coordinate system where a reduced number of dimensions can effectively encapsulate (most of) the data variation compared to the original dataset.\nIn various studies, researchers often employ the initial two principal components to plot data in a two-dimensional space, facilitating the identification of clusters among closely associated data points. The applications of principal component analysis span across diverse fields including population genetics, microbiome studies, and atmospheric science. As Machine Learning(ML) is a data-driven approach to building algorithms, PCA plays an important role in ML for Dimensionality Reduction, Data Preprocessing, Visualization, Decorrelation and Feature Extraction, Speeding Up Learning Algorithms, and Noise Filtering.\nThe idea behind PCA is, that given a dataset with some statistical distribution (i.e., not deterministic), we would like to find features that can best describe as much of the data as possible. By doing so we will be able to explain the data with fewer sets of features than the actual data has. In data involving numerous variables and dimensions, not all variables hold equal importance. Some variables are key while others are less critical. The Principal Component Analysis (PCA) method offers a systematic way to identify and eliminate less significant variables. By transforming the original variables into uncorrelated principal components, PCA retains essential information while discarding less important variables. This process simplifies complex datasets by focusing on the principal components that capture the majority of the dataset’s variance. Consequently, PCA enhances data transparency by emphasizing the critical factors while reducing unnecessary complexity.\nIn this blog, we will go through the steps for calculating PCA with an example data set demonstrating PCA in action to perform dimensionality reduction.\n\nWhat Exactly is Principal Component\nThe principal component represents a novel feature or feature formed by combining the original features linearly. By crafting one or more of these new features, the objective is to ensure that these combinations result in uncorrelated variables, known as principal components. This process involves condensing or compressing most of the information from the initial variables into the first component. In essence, when dealing with n-dimensional data, PCA generates n principal components. However, the primary goal of PCA is to maximize information encapsulation in the initial component, followed by retaining the maximum remaining information in subsequent components. This systematic approach prioritizes the encapsulation of significant data details within these newly constructed components in a step-by-step manner.\nArranging information within principal components enables effective dimensionality reduction without significant loss of information. This process involves discarding components that hold minimal information while regarding the remaining components as the new variables. By prioritizing the retention of informative components and disregarding those with lower significance, one can streamline the dataset and create a new set of variables that effectively captures the essential information from the original dataset. This approach facilitates a more concise representation of the data while preserving the most critical information contained within the retained components. While representing data with less number of features reduces the complexity of the data aiding in different aspects of machine learning it complicates understanding of the data as the newly formed features (i.e., principal components) do not have a one-to-one mapping with the original feature set. ## Data The data we will use in this post comes with seaborn plotting package. Specifically, we’ll use the car_crashes dataset from seaborn package. If you’re new to seaborn, I’ve another post that demonstrates different plot types available in seaborn.\nThe car_crashes dataset is data about car accidents, their cause, and cost to insurance companies in the states of the USA. It consists of the following features.\n\ntotal: Number of drivers involved in fatal collisions per billion miles (5.900–23.900)\nspeeding: Percentage Of Drivers Involved In Fatal Collisions Who Were Speeding (1.792–9.450)\nalcohol: Percentage Of Drivers Involved In Fatal Collisions Who Were Alcohol-Impaired (1.593–10.038)\nnot_distracted: Percentage Of Drivers Involved In Fatal Collisions Who Were Not Distracted (1.760–23.661)\nno_previous: Percentage Of Drivers Involved In Fatal Collisions Who Had Not Been Involved In Any Previous Accidents (5.900–21.280)\nins_premium: Car Insurance Premiums (641.960–1301.520)\nins_losses: Losses incurred by insurance companies for collisions per insured driver (82.75–194.780)\nabbrev: A two-letter abbreviation of US state name the data stands for\n\n\nImport the required packages\nLet’s start by importing the necessary packages\n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n\n\n\n\nLoad the data\nWe can now load the data and try to understand it\n\ncar_crashes = sns.load_dataset(\"car_crashes\")\ncar_crashes.head()\n\n\n\n\n\n\n\n\ntotal\nspeeding\nalcohol\nnot_distracted\nno_previous\nins_premium\nins_losses\nabbrev\n\n\n\n\n0\n18.8\n7.332\n5.640\n18.048\n15.040\n784.55\n145.08\nAL\n\n\n1\n18.1\n7.421\n4.525\n16.290\n17.014\n1053.48\n133.93\nAK\n\n\n2\n18.6\n6.510\n5.208\n15.624\n17.856\n899.47\n110.35\nAZ\n\n\n3\n22.4\n4.032\n5.824\n21.056\n21.280\n827.34\n142.39\nAR\n\n\n4\n12.0\n4.200\n3.360\n10.920\n10.680\n878.41\n165.63\nCA\n\n\n\n\n\n\n\nWe can see all features except the abbreviation (which represents the US state the sample stands for) are continuous-valued.\n\ncar_crashes.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 51 entries, 0 to 50\nData columns (total 8 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   total           51 non-null     float64\n 1   speeding        51 non-null     float64\n 2   alcohol         51 non-null     float64\n 3   not_distracted  51 non-null     float64\n 4   no_previous     51 non-null     float64\n 5   ins_premium     51 non-null     float64\n 6   ins_losses      51 non-null     float64\n 7   abbrev          51 non-null     object \ndtypes: float64(7), object(1)\nmemory usage: 3.3+ KB\n\n\nWe’ll remove the apprev column as it is not relevant to our demonstration in this post\n\ncar_crashes = car_crashes.drop('abbrev', axis=1)\n\n\n\nMaximum, Minimum, Standard deviation and Average Values\nThe maximum value in each of the columns\n\ncar_crashes.max()\n\ntotal               23.900\nspeeding             9.450\nalcohol             10.038\nnot_distracted      23.661\nno_previous         21.280\nins_premium       1301.520\nins_losses         194.780\ndtype: float64\n\n\nThe minimum value in each of the columns\n\ncar_crashes.min()\n\ntotal               5.900\nspeeding            1.792\nalcohol             1.593\nnot_distracted      1.760\nno_previous         5.900\nins_premium       641.960\nins_losses         82.750\ndtype: float64\n\n\nThe range in each column is the difference between the max and minimum values\n\ncar_crashes_range = car_crashes.max() - car_crashes.min()\ncar_crashes_range\n\ntotal              18.000\nspeeding            7.658\nalcohol             8.445\nnot_distracted     21.901\nno_previous        15.380\nins_premium       659.560\nins_losses        112.030\ndtype: float64\n\n\nWe can visualize the difference in the range\n\np1 = car_crashes_range.plot(\n    legend=False,\n    kind=\"bar\",\n    rot=45,\n    color=\"blue\",\n    fontsize=16,\n)\np1.set_title(\"Range of values in our car crash data\", fontsize=16)\np1.set_xlabel(\"Feature\", fontsize=14)\np1.set_ylabel(\"Range\", fontsize=14)\n\nText(0, 0.5, 'Range')\n\n\n\n\n\nWe can get the mean value and standard deviation of each column as well\n\ncar_crashes.mean(axis=0)\n\ntotal              15.790196\nspeeding            4.998196\nalcohol             4.886784\nnot_distracted     13.573176\nno_previous        14.004882\nins_premium       886.957647\nins_losses        134.493137\ndtype: float64\n\n\n\ncar_crashes.std(axis=0)\n\ntotal               4.122002\nspeeding            2.017747\nalcohol             1.729133\nnot_distracted      4.508977\nno_previous         3.764672\nins_premium       178.296285\nins_losses         24.835922\ndtype: float64"
  },
  {
    "objectID": "posts/pca.html#computing-pca",
    "href": "posts/pca.html#computing-pca",
    "title": "Principal Component Analysis",
    "section": "Computing PCA",
    "text": "Computing PCA\nCalculating PCA involves five steps. These are\n\nStandardization\nCovariance Matrix Computation\nEigenvectors and Eigenvalues\nFeature Vector\nRecast the Data\n\nNow that we’ve explained the data we’ll use for demonstrating PCA and the steps involved, let’s go through each step with the aforementioned data and see PCA in action."
  },
  {
    "objectID": "posts/pca.html#pca-in-action",
    "href": "posts/pca.html#pca-in-action",
    "title": "Principal Component Analysis",
    "section": "PCA in Action",
    "text": "PCA in Action\n\nStep 1. Standardization\nPCA is sensitive to variance, meaning if features different range in values the PCA calculation will be dominated by features with a larger range. In our dataset, we have features that represent a percentage value (hence the potential values range from 0 to 100, but the actual maximum value in the dataset is way below 100). We also have other features that have a more inconsistent range. For example, the ins_premium has a range of 659.560 while speading has a range of 7.658. As a result, the PCA analysis will be greatly (and incorrectly) influenced by a change in ins_premium value.\nTo address this issue we perform standardization before PCA, that is we bring values of all features to a standard range. The one common way to standardize any data is called mean centering where we subtract the mean from each value.\n\\[\nz = v - \\mu\n\\] Where \\(z\\) is the standardized value, \\(v\\) is the original value, \\(\\mu\\) stands for mean and \\(\\sigma\\) is the standard deviation\nWe can perform this calculation in python as follows\n\ncar_crashes_std = (car_crashes - car_crashes.mean(axis=0)) / car_crashes.std(axis=0)\ncar_crashes_std.head()\n\n\n\n\n\n\n\n\ntotal\nspeeding\nalcohol\nnot_distracted\nno_previous\nins_premium\nins_losses\n\n\n\n\n0\n0.730180\n1.156638\n0.435603\n0.992425\n0.274956\n-0.574368\n0.426272\n\n\n1\n0.560360\n1.200747\n-0.209229\n0.602537\n0.799304\n0.933964\n-0.022674\n\n\n2\n0.681660\n0.749253\n0.185767\n0.454831\n1.022962\n0.070177\n-0.972106\n\n\n3\n1.603542\n-0.478849\n0.542015\n1.659539\n1.932471\n-0.334374\n0.317961\n\n\n4\n-0.919504\n-0.395588\n-0.882977\n-0.588421\n-0.883180\n-0.047941\n1.253703\n\n\n\n\n\n\n\nLet’s perform a similar visualization on the range as we did earlier\n\np2 = (car_crashes_std.max() - car_crashes_std.min()).plot(\n    legend=False, kind=\"bar\", rot=45, color=\"blue\", fontsize=16\n)\np2.set_title(\"Range of values in our car crash data after standardization\", fontsize=16)\np2.set_xlabel(\"Features\", fontsize=14)\np2.set_ylabel(\"Range\", fontsize=14)\n\nText(0, 0.5, 'Range')\n\n\n\n\n\nWe can see that now the range is more consistent across the features\n\n\nStep 2. Covariance Matrix\nCovariance Matrix To get a sense of how the values of the input dataset vary from the mean to each other we compute the covariance matrix. The covariance matrix is a \\(m × m\\) symmetric matrix (where m is the number of features we’ve in our input dataset). Features might be highly correlated and through computing the covariance matrix we can identify this relationship.\nFor example, a covariance matrix of an input dataset with 4 features \\((f_1,f_2,f_3,f_4)\\) will be a 4x4 matrix of the following form\n\\[\nCovariance Matrix = \\begin{bmatrix}\n      cov(f_1,f_1) & cov(f_1,f_2) & cov(f_1,f_3) & cov(f_1,f_4) \\\\\n      cov(f_2,f_1) & cov(f_2,f_2) & cov(f_2,f_3) & cov(f_2,f_4) \\\\\n      cov(f_3,f_1) & cov(f_3,f_2) & cov(f_3,f_3) & cov(f_3,f_4) \\\\\n      cov(f_4,f_1) & cov(f_4,f_2) & cov(f_3,f_3) & cov(f_4,f_4)\n\\end{bmatrix}\n\\]\nWe compute the covariance between to features \\(f_x\\) and \\(f_y\\) as follows \\[\ncov(f_x,f_y) = \\dfrac{\\sum(x_i-\\mu_x) (y_i-\\mu_y)}{n}\n\\] Where \\(x_i\\) and \\(y_i\\) are the \\(i^{th}\\) values for feature \\(x\\) and \\(y\\) and \\(n\\) is the total number of data points.\nWith this equation notice how\n\ncovariance of a feature f with itself (i.e., \\(Cov(f_x,f_x)\\)) the variance of the feature (i.e., \\(Var(f_x)\\))\ncovariance is symmetric, i.e., \\(Cov(fx,fy) = Cov(f_y,f_x)\\)\n\nNow that we know that the covariance matrix is not more than a table that summarizes the correlations between all the possible pairs of variables, let’s move to the next step. While we can use these equations to calculate the covariance matrix pandas provide a built-in method for calculating it in one go.\n\ncov_mat = car_crashes_std.cov()\n\nWe can visualize this matrix using seaborn’s heatmap\n\nsns.heatmap(cov_mat, annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nStep 3. Eigenvectors and Eigenvalues\nEigenvectors and eigenvalues, fundamental concepts from linear algebra, play a crucial role in computing the principal components of a dataset from its covariance matrix. In an n-dimensional dataset comprising n features, there exist n eigenvectors accompanied by n corresponding eigenvalues.\nEigenvectors derived from the Covariance matrix represent the axes’ directions where the highest variance or most information is concentrated. These eigenvectors are termed Principal Components. Eigenvalues, on the other hand, serve as coefficients associated with eigenvectors, signifying the quantity of variance encapsulated within each Principal Component. Essentially, eigenvalues provide a measure of the amount of variance carried by the respective eigenvectors, highlighting their significance in determining the variance distribution across the dataset’s principal axes.\nArranging your eigenvectors based on their corresponding eigenvalues, from highest to lowest, provides the principal components in descending order of significance. This ranking scheme ensures that the principal components are prioritized based on the amount of variance or information they encapsulate. The eigenvector associated with the highest eigenvalue represents the most significant principal component, followed by subsequent components in decreasing order of importance, allowing for a systematic determination of the most influential axes within the dataset.\nWe can use numpy’s linalg.eigh( ) method to calcuate eigenvectors and eigenvalues a matrix.\n\neigen_values, eigen_vectors = np.linalg.eigh(cov_mat)\n\n\neigen_values\n\narray([0.02747434, 0.19865996, 0.28077   , 0.350529  , 0.55060199,\n       1.57801295, 4.01395176])\n\n\n\neigen_vectors\n\narray([[ 0.80082487, -0.16970508, -0.14597659, -0.0389558 ,  0.26908802,\n        -0.06893769, -0.47947078],\n       [ 0.01784783, -0.22479246,  0.02282818,  0.36374887, -0.81826935,\n        -0.0765846 , -0.37534719],\n       [-0.15285774,  0.7837677 , -0.35479821, -0.14834351, -0.08293253,\n        -0.03345835, -0.45437635],\n       [-0.14247844,  0.08510479,  0.85646854, -0.1712655 ,  0.12616845,\n        -0.04237473, -0.4380328 ],\n       [-0.55875371, -0.50401185, -0.33611019, -0.03948141,  0.31798812,\n        -0.0961294 , -0.45703414],\n       [ 0.04126619, -0.11577348, -0.04214531, -0.65639617, -0.25614247,\n        -0.6852266 ,  0.1308319 ],\n       [-0.02804966,  0.17805184,  0.06327152,  0.61839859,  0.26173503,\n        -0.71252436,  0.06996048]])\n\n\nWe should now sort the Eigenvalues in the descending order along with their corresponding Eigenvector.\n\n# sort the eigenvalues in descending order\nsorted_index = np.argsort(eigen_values)[::-1]\n\nsorted_eigenvalue = eigen_values[sorted_index]\n\n# Similarly sort the eigenvectors\nsorted_eigenvectors = eigen_vectors[:, sorted_index]\n\n\n\nStep 4. Feature Vector\nOnce sorted, we can select the subset of the Eigenvalue as per our requirement. In this case, since we are interested in the two principal components we take the first two values (i.e. n_components = 2). This forms our feature vector which is a matrix that has as columns the eigenvectors of the components that we decide to keep. By only keeping a subset of the Eigenvectors we are reducing the number of features hence the notion of dimensionality reduction.\n\nn_components = 2 \nfeature_vector = sorted_eigenvectors[:,0:n_components]\n\nfeature_vector\n\narray([[-0.47947078, -0.06893769],\n       [-0.37534719, -0.0765846 ],\n       [-0.45437635, -0.03345835],\n       [-0.4380328 , -0.04237473],\n       [-0.45703414, -0.0961294 ],\n       [ 0.1308319 , -0.6852266 ],\n       [ 0.06996048, -0.71252436]])\n\n\n\n\nStep 5. Recast the Data\n\ncar_crash_reduced = -np.dot(\n    feature_vector.transpose(), car_crashes_std.transpose()\n).transpose()\ncars_pca = pd.DataFrame(data=car_crash_reduced, columns=[\"PC 1\", \"PC 2\"])\ncars_pca.head()\n\n\n\n\n\n\n\n\nPC 1\nPC 2\n\n\n\n\n0\n1.587871\n0.132134\n\n\n1\n1.132939\n0.849778\n\n\n2\n1.418062\n-0.416363\n\n\n3\n2.467035\n0.345530\n\n\n4\n-1.733390\n0.627382\n\n\n\n\n\n\n\n\n\nUsing sklearn package\nThe sklearn package comes with an API to calculate PCA without having to follow the above steps\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\n\npcs = pca.fit_transform(car_crashes_std)\n\nprincipal_cars = pd.DataFrame(data=pcs, columns=[\"PC 1\", \"PC 2\"])\n\nprincipal_cars.head()\n\n\n\n\n\n\n\n\nPC 1\nPC 2\n\n\n\n\n0\n1.587871\n0.132134\n\n\n1\n1.132939\n0.849778\n\n\n2\n1.418062\n-0.416363\n\n\n3\n2.467035\n0.345530\n\n\n4\n-1.733390\n0.627382"
  },
  {
    "objectID": "posts/Anomaly Detection.html",
    "href": "posts/Anomaly Detection.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Banner Image Credit Evidently AI\nExcepteur qui velit commodo non cillum enim duis occaecat laborum tempor sint do anim. In nostrud ex aliqua commodo sunt quis velit. Enim laborum officia officia nostrud sunt velit dolore Lorem occaecat ipsum.\nPariatur mollit in eiusmod irure velit veniam aliquip tempor culpa minim. Nulla proident eiusmod ipsum aute eiusmod aliqua. Amet cupidatat adipisicing nostrud exercitation amet anim aliqua sint mollit in dolor culpa. Adipisicing nostrud culpa excepteur ullamco. Eiusmod eu laboris excepteur sint eu excepteur et. Ullamco eu cupidatat consectetur fugiat minim fugiat sunt ut minim. Pariatur dolor exercitation sunt ipsum Lorem velit sit Lorem laboris Lorem.\nQui Lorem fugiat do commodo exercitation anim commodo incididunt sint aute deserunt nulla. Ex incididunt do ut ullamco exercitation in laborum cupidatat. Dolor laboris cupidatat anim sint. Culpa elit voluptate officia ex elit ullamco fugiat pariatur mollit labore eiusmod id anim."
  },
  {
    "objectID": "posts/clustering.html",
    "href": "posts/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Banner Image Credit: University of Rochester\nAdipisicing sunt nisi nulla pariatur ex ipsum mollit. Dolor commodo cillum sint amet commodo. Nulla culpa aliqua enim ad anim. Aliquip ex duis minim laborum enim aliquip ad aliqua. Sit consectetur excepteur ea aliqua exercitation Lorem ex elit incididunt consequat ea. Excepteur qui velit commodo non cillum enim duis occaecat laborum tempor sint do anim. In nostrud ex aliqua commodo sunt quis velit. Enim laborum officia officia nostrud sunt velit dolore Lorem occaecat ipsum.\nPariatur mollit in eiusmod irure velit veniam aliquip tempor culpa minim. Nulla proident eiusmod ipsum aute eiusmod aliqua. Amet cupidatat adipisicing nostrud exercitation amet anim aliqua sint mollit in dolor culpa. Adipisicing nostrud culpa excepteur ullamco. Eiusmod eu laboris excepteur sint eu excepteur et. Ullamco eu cupidatat consectetur fugiat minim fugiat sunt ut minim. Pariatur dolor exercitation sunt ipsum Lorem velit sit Lorem laboris Lorem.\nQui Lorem fugiat do commodo exercitation anim commodo incididunt sint aute deserunt nulla. Ex incididunt do ut ullamco exercitation in laborum cupidatat. Dolor laboris cupidatat anim sint. Culpa elit voluptate officia ex elit ullamco fugiat pariatur mollit labore eiusmod id anim."
  },
  {
    "objectID": "posts/regression.html",
    "href": "posts/regression.html",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "Adipisicing sunt nisi nulla pariatur ex ipsum mollit. Dolor commodo cillum sint amet commodo. Nulla culpa aliqua enim ad anim. Aliquip ex duis minim laborum enim aliquip ad aliqua. Sit consectetur excepteur ea aliqua exercitation Lorem ex elit incididunt consequat ea. Excepteur qui velit commodo non cillum enim duis occaecat laborum tempor sint do anim. In nostrud ex aliqua commodo sunt quis velit. Enim laborum officia officia nostrud sunt velit dolore Lorem occaecat ipsum.\nPariatur mollit in eiusmod irure velit veniam aliquip tempor culpa minim. Nulla proident eiusmod ipsum aute eiusmod aliqua. Amet cupidatat adipisicing nostrud exercitation amet anim aliqua sint mollit in dolor culpa. Adipisicing nostrud culpa excepteur ullamco. Eiusmod eu laboris excepteur sint eu excepteur et. Ullamco eu cupidatat consectetur fugiat minim fugiat sunt ut minim. Pariatur dolor exercitation sunt ipsum Lorem velit sit Lorem laboris Lorem.\nQui Lorem fugiat do commodo exercitation anim commodo incididunt sint aute deserunt nulla. Ex incididunt do ut ullamco exercitation in laborum cupidatat. Dolor laboris cupidatat anim sint. Culpa elit voluptate officia ex elit ullamco fugiat pariatur mollit labore eiusmod id anim."
  },
  {
    "objectID": "posts/seaborn_demo.html",
    "href": "posts/seaborn_demo.html",
    "title": "Seaborn Examples",
    "section": "",
    "text": "Seaborn python package build on top of matplotlib for plotting statistical data. It has great support for the commonly used pandas data structure in handling tabular data. It’s more opinionated than matplotlib which makes it easier to get started with and provides higher level API for plotting which will help us create plots with fewer number of lines and configuration that we need in matplotlib for the same graphics.\n\nVisualizing data using tools like seaborn help us explore and understand the data. In this post we’ll explore some of the available graphics demonstrate how we can use to plot different data types we might have.\n\nTo help with the clarity of this post, I’ve organized it into six sections. Following this introductory section, we’ll see the prerequisite setup and imports. In the setup section I’ll discuss the sample datasets that come with seaborn. The third and fourth sections are about seaborn plots for Categorical and Continues Data, respectively. Then I’ll discuss how we can plot comparative plots followed by a section on utilities such as styling and saving plots."
  },
  {
    "objectID": "posts/seaborn_demo.html#setup",
    "href": "posts/seaborn_demo.html#setup",
    "title": "Seaborn Examples",
    "section": "Setup",
    "text": "Setup\n\nInstallation\nFirst step is t install the seaborn package. For that, we can use pip as\n\n%pip install seaborn\n\nRequirement already satisfied: seaborn in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (0.13.0)\nRequirement already satisfied: numpy!=1.24.0,&gt;=1.20 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from seaborn) (1.23.5)\nRequirement already satisfied: pandas&gt;=1.2 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from seaborn) (2.0.3)\nRequirement already satisfied: matplotlib!=3.6.1,&gt;=3.3 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from seaborn) (3.7.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.0.5)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (4.25.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.4.4)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (23.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (9.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2022.7)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nImports\nTo get started with seaborn we need to import the following packages. We import pyplot form matplotlib to use the lower level APIs later for styling and customization. While working with Jupyter notebooks we might want to suppers warnings in the output.\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\n\n\nJupyter Configurations\nAsk Jupyter to display plots within the notebook\n\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\n\nSuppress warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\nData\nPlotting only makes sense if we have some data to visualize. We can plot any data stored in pandas data structure using seaborn. This let’s us load and process data using pandas and use seaborn for visualization in parallel. For this post, I’ll make use of sample datasets shipped with seaborn. To get the full list of datasets available in our installation we can as seaborn itself\n\nprint(sns.get_dataset_names())\n\n['anagrams', 'anscombe', 'attention', 'brain_networks', 'car_crashes', 'diamonds', 'dots', 'dowjones', 'exercise', 'flights', 'fmri', 'geyser', 'glue', 'healthexp', 'iris', 'mpg', 'penguins', 'planets', 'seaice', 'taxis', 'tips', 'titanic']\n\n\nLet’s load two datasets and examine their content. More information on the datasets can be found here and here\n\ncrashes = sns.load_dataset('car_crashes')\ntitanic = sns.load_dataset('titanic')\n\nExamine the content of the data\n\ncrashes.head()\n\n\n\n\n\n\n\n\ntotal\nspeeding\nalcohol\nnot_distracted\nno_previous\nins_premium\nins_losses\nabbrev\n\n\n\n\n0\n18.8\n7.332\n5.640\n18.048\n15.040\n784.55\n145.08\nAL\n\n\n1\n18.1\n7.421\n4.525\n16.290\n17.014\n1053.48\n133.93\nAK\n\n\n2\n18.6\n6.510\n5.208\n15.624\n17.856\n899.47\n110.35\nAZ\n\n\n3\n22.4\n4.032\n5.824\n21.056\n21.280\n827.34\n142.39\nAR\n\n\n4\n12.0\n4.200\n3.360\n10.920\n10.680\n878.41\n165.63\nCA\n\n\n\n\n\n\n\n\ntitanic.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue"
  },
  {
    "objectID": "posts/seaborn_demo.html#categorical-data",
    "href": "posts/seaborn_demo.html#categorical-data",
    "title": "Seaborn Examples",
    "section": "Categorical Data",
    "text": "Categorical Data\nCategorical data is form of qualitative data that can be stored and identified based on distinct labels. Instead of being measured numerically, it is a type of information that can be grouped into categories. For example, the sex column in our titanic dataset has two labels: male and female. Similarly the abbrev column is a list of state name abbreviations in the US which has a total of 50 labels.  In this section, we will see bar and count plots from seaborn as two graphics to visualize categorical data.\n\nIn this section, we will see bar and count plots from seaborn as two graphics to visualize categorical data.\n\nBar Plots\nAlso referred to as bar chart, bar plot is, a bar plot allows us to visualize the comparisons between the discrete labels or categories in our data. Bar chart is a graph that represents the category of data with horizontally or vertically rectangular bars with lengths and heights that is proportional to the values which they represent. One of the axis of the plot represents the specific categories being compared, while the other axis represents the measured values corresponding to those categories.\n\nsns.barplot(data = titanic, x = 'sex', y='fare')\n\n&lt;Axes: xlabel='sex', ylabel='fare'&gt;\n\n\n\n\n\nBy Default the data is aggregated by mean of y\n\nsns.barplot(data = titanic, x = 'sex', y='fare', estimator=np.median)\n\n&lt;Axes: xlabel='sex', ylabel='fare'&gt;\n\n\n\n\n\n\n\nCount Plots\nSimilar to bar plot but uses count as the estimator\n\nsns.countplot(data = titanic, x = 'sex')\n\n&lt;Axes: xlabel='sex', ylabel='count'&gt;\n\n\n\n\n\nWe can display a count of the number rows in each category of values in the alive column\n\nsns.countplot(data = titanic, x = 'alive')\n\n&lt;Axes: xlabel='alive', ylabel='count'&gt;"
  },
  {
    "objectID": "posts/seaborn_demo.html#distribution-plots",
    "href": "posts/seaborn_demo.html#distribution-plots",
    "title": "Seaborn Examples",
    "section": "Distribution Plots",
    "text": "Distribution Plots\nDistribution plots for continuous data variables.In the past, Seaborn had a distplot method which supported displaying a histogram plot by with kde on top default. distplot is deprecated and it is recommended we use displot or histplot for find grained control. distplot allow as to display a histogram of univariate or bivariate distribution of the data in a dataset.\n\nsns.distplot(a=crashes['alcohol'])\n \n\n&lt;Axes: xlabel='alcohol', ylabel='Density'&gt;\n\n\n\n\n\nIf we don’t what the kde plot to be visible we can tell seaborn not to show it\n\nsns.distplot(a=crashes['alcohol'],kde=False)\n\n&lt;Axes: xlabel='alcohol'&gt;\n\n\n\n\n\n\nsns.displot(data=crashes['alcohol'])\n\n\n\n\nThe equivalent plot can be displayed using the new displot method\n\n# We can specify name of the data column in the dataset if there are more than one\nsns.displot(data=crashes, x='alcohol')\n\n\n\n\ndisplot combines a histogram with optional components, such as a Kernel Density Estimation (KDE) line or rug plot. We can specify which type we want to plot using the kind key (default is hist)\n\nsns.displot(data=crashes['alcohol'], kind='kde')\n\n\n\n\nWe can enable an overlay of other visualization on top of the default. We can do this by passing a boolean value for the parameters hist, ecdf, kde, rug\n\nsns.displot(data=crashes['alcohol'], kde=True, rug=True)\n\n\n\n\n\nsns.displot(data=titanic, x='age', hue='sex', kind='kde', multiple='stack')\n\n\n\n\n\nKDE Plot\nWe can display kde plots using the kdeplot function as well\n\nsns.kdeplot(data=titanic, x='fare')\n\n&lt;Axes: xlabel='fare', ylabel='Density'&gt;"
  },
  {
    "objectID": "posts/seaborn_demo.html#categorical-plots",
    "href": "posts/seaborn_demo.html#categorical-plots",
    "title": "Seaborn Examples",
    "section": "Categorical Plots",
    "text": "Categorical Plots\n\nBox Plot\nCompare different variables\n\nsns.boxplot(data = titanic, x = 'alive', y='fare', hue='sex')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\n\nViolin Plot\nCompare different variables in a different visualization\n\nsns.violinplot(data = titanic, x = 'alive', y='fare', hue='sex')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.violinplot(data = titanic, x = 'alive', y='fare', hue='sex', split=True)\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\n#survived   pclass  sex age sibsp   parch   fare    embarked    class   who adult_male  deck    embark_town alive   alone\nsns.violinplot(data = titanic, x = 'alive', y='fare')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\n\nStrip Plot\n\nsns.stripplot(data = titanic, x = 'class', y='fare')\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.stripplot(data = titanic, x = 'class', y='fare', hue='sex')\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\n\nSwarm Plot\n\nsns.swarmplot(data = titanic, x = 'alive', y='fare')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;"
  },
  {
    "objectID": "posts/seaborn_demo.html#comparing-data",
    "href": "posts/seaborn_demo.html#comparing-data",
    "title": "Seaborn Examples",
    "section": "Comparing Data",
    "text": "Comparing Data\n\nsns.displot(data=titanic, x='age', col='survived',  kind='kde')\n\n\n\n\n\nsns.displot(data=titanic, x='age', col='survived', hue='sex', kind='kde', multiple='stack')\n\n\n\n\n\nJoint Plot\nUsed for comparing two distributions. By default it uses scatter plot\n\nsns.jointplot(data=crashes, x='speeding',y='alcohol')\n\n\n\n\n\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='kde')\n\n\n\n\n\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg')\n\n\n\n\n\nsns.jointplot(data=titanic, x='fare',y='age')\n\n\n\n\n\n\nPair Plot\nWe can display pair plots across the entire dataset for each pair of numeric attributes\n\nsns.pairplot(data=crashes)\n\n\n\n\nWe can use hue to have color palettes of categorical data\n\nsns.pairplot(data=titanic, hue='sex')\n\n\n\n\n\n\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\n\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.stripplot(data = titanic, x = 'class', y='fare', hue='sex',jitter=True)\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.stripplot(data = titanic, x = 'class', y='fare', hue='sex',jitter=True, dodge=True)\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\n\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.swarmplot(data = titanic, x = 'alive', y='fare', color='red')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;"
  },
  {
    "objectID": "posts/seaborn_demo.html#other-feature",
    "href": "posts/seaborn_demo.html#other-feature",
    "title": "Seaborn Examples",
    "section": "Other Feature",
    "text": "Other Feature\n\nResizing\nWe can resize the plot using height, width and aspect parameters\n\nsns.displot(data = crashes, x = 'total', height = 2 , aspect = 1.6)\n\n\n\n\n\n\nStyling\n\nsns.set_style('darkgrid')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\nsns.set_style('whitegrid')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\nsns.set_style('ticks')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\n\nLabel Styling\n\nsns.set_context('poster')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\nsns.set_context('paper')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\nsns.despine(left=True, bottom=True) # False turns off the boundary \n\n\n\n\n\n\nSave Plot\nSince seaboarn is built on top of the matplotlib package, we can use matplotlib’s savefig() function to save the generated plot into image file.\nNote: The savefig() function should come before the show() function since the later closes and deletes the image from the memory to save space.\n\nsns.displot(crashes['alcohol'])\nplt.savefig('picture.png')\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yoseph’s ML Blog",
    "section": "",
    "text": "Seaborn Examples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\nDBScan\n\n\nClustering\n\n\n\nDBSCAN labels for scatter plot\n\n\n\n\n\n\nDec 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly Detection\n\n\n\nAnomaly Detection\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Component Analysis\n\n\n\nProbability theory\n\n\nPCA\n\n\n\nWhat’s PCA, and how to calculate it\n\n\n\nYoseph Berhanu\n\n\nDec 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudent Grade Prediction\n\n\n\nConfusion Matrix\n\n\nClassification\n\n\n\nUsing three ML classifier to predict student grade\n\n\n\nYoseph Berhanu\n\n\nDec 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear and nonlinear regression\n\n\n\nRegression\n\n\n\nline on scatter plot\n\n\n\n\n\n\nNov 6, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]