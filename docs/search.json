[
  {
    "objectID": "CS5805.html",
    "href": "CS5805.html",
    "title": "CS5805",
    "section": "",
    "text": "The blog posts presented here are created as part of the requirmetns for CS5805 offerd for fall 24 at Virginia Tech. More infomration about me can be found on my personal website"
  },
  {
    "objectID": "posts/student-grade-prediction.html",
    "href": "posts/student-grade-prediction.html",
    "title": "Student Grade Prediction",
    "section": "",
    "text": "Banner Image Credit: Jernej Furman\nIn this blog I will use three different machine learning classification algorithms provided in skit-learn to predict student grade using data about students’ performance in quizzes, assignments and other features collected in Moodle LMS"
  },
  {
    "objectID": "posts/student-grade-prediction.html#preparation",
    "href": "posts/student-grade-prediction.html#preparation",
    "title": "Student Grade Prediction",
    "section": "1. Preparation",
    "text": "1. Preparation\n\n1.1 Install required packages\n```{python}\n%pip install numpy sklearn pandas matplotlib seaborn\n```\n\n\n1.2 Perform the necessary imports\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn import preprocessing\n\nfrom sklearn.linear_model import (\n    LogisticRegression,\n    SGDClassifier,\n)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import (\n    BaggingClassifier,\n    GradientBoostingClassifier,\n    RandomForestClassifier,\n)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score,\n    # classification_report,\n    # mean_squared_error,\n    # r2_score,\n    # mean_absolute_error,\n    # confusion_matrix\n)\n\nSuppress Warnings\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)"
  },
  {
    "objectID": "posts/student-grade-prediction.html#data-understanding-and-preprocessing",
    "href": "posts/student-grade-prediction.html#data-understanding-and-preprocessing",
    "title": "Student Grade Prediction",
    "section": "2. Data Understanding and Preprocessing",
    "text": "2. Data Understanding and Preprocessing\n\n2.1 Data description\nThe data was collected from a fully online nine-week-long course on, hosted on the online learning management system Moodle. The dataset containes anonymized information relating to 107 enrolled students. The data included students’ grades (from 3 mini projects, 3 quizzes and 3 peer reviews and the final overall grade) as well as the course logs. The deadline for the three mini projects fell within weeks 3, 5 and 8 of the course, whereas the deadline for the quizzes fell within weeks 2, 4 and 8.\n\nStatus0: course / lectures / content related (Course module viewed, Course viewed, Course ac2vity comple2on updated, Course module instance list viewed, Content page viewed, Lesson started, Lesson resumed, Lesson restarted, Lesson ended)\nStatus1: assignment related (Quiz aPempt reviewed, Quiz aPempt submiPed, Quiz aPempt summary viewed, Quiz aPempt viewed, Quiz aPempt started, Ques2on answered, Ques2on viewed, Submission re-assessed, Submission assessed, Submission updated, Submission created, Submission viewed)\nStatus2: grade related (Grade user report viewed, Grade overview report viewed, User graded, Grade deleted, User profile viewed, Recent ac2vity viewed, User report viewed, Course user report viewed, Outline report viewed)\nStatus3: forum related (Post updated, Post created, Discussion created, Some content has been posted, Discussion viewed)\n9 grades (Week2_Quiz1, Week3_MP1, … Week7_MP3)\n36 logs (Week1_Stat0, Week1_Stat1, Week1_Stat2, Week1_Stat3, … Week9_Stat0, Week9_Stat1, Week9_Stat2, Week9_Stat3)\n\n\n\n2.2 Load the data\n\ndf = pd.read_csv(\"../data/MP2_Data.csv\")"
  },
  {
    "objectID": "posts/student-grade-prediction.html#try-to-learn-some-information-about-the-data",
    "href": "posts/student-grade-prediction.html#try-to-learn-some-information-about-the-data",
    "title": "Student Grade Prediction",
    "section": "2.3 Try to learn some information about the data",
    "text": "2.3 Try to learn some information about the data\n\nDisplay the first and last 5 rows\n\ndf.head()\n\n\n\n\n\n\n\n\nID\nWeek2_Quiz1\nWeek3_MP1\nWeek3_PR1\nWeek5_MP2\nWeek5_PR2\nWeek7_MP3\nWeek7_PR3\nWeek4_Quiz2\nWeek6_Quiz3\n...\nWeek7_Stat3\nWeek8_Stat0\nWeek8_Stat1\nWeek8_Stat2\nWeek8_Stat3\nWeek9_Stat0\nWeek9_Stat1\nWeek9_Stat2\nWeek9_Stat3\nGrade\n\n\n\n\n0\nML-2020-1\n5.00\n15.0\n5.0\n16.09\n5.00\n21.88\n5.0\n5.00\n5.0\n...\n0\n5\n4\n0\n4\n8\n6\n1\n0\n4\n\n\n1\nML-2020-2\n3.33\n15.0\n5.0\n17.83\n5.00\n22.27\n5.0\n4.00\n5.0\n...\n8\n5\n2\n0\n0\n25\n3\n2\n5\n4\n\n\n2\nML-2020-3\n1.67\n13.0\n5.0\n15.22\n5.00\n27.05\n2.5\n5.00\n5.0\n...\n0\n8\n2\n0\n0\n9\n0\n1\n0\n3\n\n\n3\nML-2020-4\n2.50\n14.0\n5.0\n10.00\n5.00\n31.02\n5.0\n3.13\n5.0\n...\n4\n10\n0\n0\n0\n7\n6\n0\n0\n3\n\n\n4\nML-2020-6\n0.00\n15.0\n5.0\n12.17\n4.93\n15.91\n5.0\n4.67\n5.0\n...\n6\n8\n5\n1\n1\n5\n3\n1\n0\n2\n\n\n\n\n5 rows × 48 columns\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nID\nWeek2_Quiz1\nWeek3_MP1\nWeek3_PR1\nWeek5_MP2\nWeek5_PR2\nWeek7_MP3\nWeek7_PR3\nWeek4_Quiz2\nWeek6_Quiz3\n...\nWeek7_Stat3\nWeek8_Stat0\nWeek8_Stat1\nWeek8_Stat2\nWeek8_Stat3\nWeek9_Stat0\nWeek9_Stat1\nWeek9_Stat2\nWeek9_Stat3\nGrade\n\n\n\n\n102\nML-2020-60\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0\n28\n0\n22\n0\n1\n0\n0\n0\n0\n\n\n103\nML-2020-58\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0\n0\n0\n0\n0\n3\n0\n0\n0\n0\n\n\n104\nML-2020-94\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n105\nML-2020-9\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0\n0\n0\n0\n0\n6\n0\n0\n0\n0\n\n\n106\nML-2020-86\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 48 columns\n\n\n\n\n\nDisplay details about each attribute\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 107 entries, 0 to 106\nData columns (total 48 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   ID           107 non-null    object \n 1   Week2_Quiz1  107 non-null    float64\n 2   Week3_MP1    107 non-null    float64\n 3   Week3_PR1    107 non-null    float64\n 4   Week5_MP2    107 non-null    float64\n 5   Week5_PR2    107 non-null    float64\n 6   Week7_MP3    107 non-null    float64\n 7   Week7_PR3    107 non-null    float64\n 8   Week4_Quiz2  107 non-null    float64\n 9   Week6_Quiz3  107 non-null    float64\n 10  Week8_Total  107 non-null    float64\n 11  Week1_Stat0  107 non-null    int64  \n 12  Week1_Stat1  107 non-null    int64  \n 13  Week1_Stat2  107 non-null    int64  \n 14  Week1_Stat3  107 non-null    int64  \n 15  Week2_Stat0  107 non-null    int64  \n 16  Week2_Stat1  107 non-null    int64  \n 17  Week2_Stat2  107 non-null    int64  \n 18  Week2_Stat3  107 non-null    int64  \n 19  Week3_Stat0  107 non-null    int64  \n 20  Week3_Stat1  107 non-null    int64  \n 21  Week3_Stat2  107 non-null    int64  \n 22  Week3_Stat3  107 non-null    int64  \n 23  Week4_Stat0  107 non-null    int64  \n 24  Week4_Stat1  107 non-null    int64  \n 25  Week4_Stat2  107 non-null    int64  \n 26  Week4_Stat3  107 non-null    int64  \n 27  Week5_Stat0  107 non-null    int64  \n 28  Week5_Stat1  107 non-null    int64  \n 29  Week5_Stat2  107 non-null    int64  \n 30  Week5_Stat3  107 non-null    int64  \n 31  Week6_Stat0  107 non-null    int64  \n 32  Week6_Stat1  107 non-null    int64  \n 33  Week6_Stat2  107 non-null    int64  \n 34  Week6_Stat3  107 non-null    int64  \n 35  Week7_Stat0  107 non-null    int64  \n 36  Week7_Stat1  107 non-null    int64  \n 37  Week7_Stat2  107 non-null    int64  \n 38  Week7_Stat3  107 non-null    int64  \n 39  Week8_Stat0  107 non-null    int64  \n 40  Week8_Stat1  107 non-null    int64  \n 41  Week8_Stat2  107 non-null    int64  \n 42  Week8_Stat3  107 non-null    int64  \n 43  Week9_Stat0  107 non-null    int64  \n 44  Week9_Stat1  107 non-null    int64  \n 45  Week9_Stat2  107 non-null    int64  \n 46  Week9_Stat3  107 non-null    int64  \n 47  Grade        107 non-null    int64  \ndtypes: float64(10), int64(37), object(1)\nmemory usage: 40.3+ KB\n\n\n\n\nSet ‘ID’ as the index column\n\ndf.set_index('ID', inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nWeek2_Quiz1\nWeek3_MP1\nWeek3_PR1\nWeek5_MP2\nWeek5_PR2\nWeek7_MP3\nWeek7_PR3\nWeek4_Quiz2\nWeek6_Quiz3\nWeek8_Total\n...\nWeek7_Stat3\nWeek8_Stat0\nWeek8_Stat1\nWeek8_Stat2\nWeek8_Stat3\nWeek9_Stat0\nWeek9_Stat1\nWeek9_Stat2\nWeek9_Stat3\nGrade\n\n\nID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML-2020-1\n5.00\n15.0\n5.0\n16.09\n5.00\n21.88\n5.0\n5.00\n5.0\n82.97\n...\n0\n5\n4\n0\n4\n8\n6\n1\n0\n4\n\n\nML-2020-2\n3.33\n15.0\n5.0\n17.83\n5.00\n22.27\n5.0\n4.00\n5.0\n82.43\n...\n8\n5\n2\n0\n0\n25\n3\n2\n5\n4\n\n\nML-2020-3\n1.67\n13.0\n5.0\n15.22\n5.00\n27.05\n2.5\n5.00\n5.0\n79.44\n...\n0\n8\n2\n0\n0\n9\n0\n1\n0\n3\n\n\nML-2020-4\n2.50\n14.0\n5.0\n10.00\n5.00\n31.02\n5.0\n3.13\n5.0\n80.65\n...\n4\n10\n0\n0\n0\n7\n6\n0\n0\n3\n\n\nML-2020-6\n0.00\n15.0\n5.0\n12.17\n4.93\n15.91\n5.0\n4.67\n5.0\n67.68\n...\n6\n8\n5\n1\n1\n5\n3\n1\n0\n2\n\n\n\n\n5 rows × 47 columns\n\n\n\nGet additional information about each column\n\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nWeek2_Quiz1\n107.0\n2.406636\n2.000317\n0.0\n0.0\n3.33\n4.170\n5.00\n\n\nWeek3_MP1\n107.0\n7.949626\n6.892312\n0.0\n0.0\n12.00\n14.305\n15.00\n\n\nWeek3_PR1\n107.0\n2.803738\n2.493158\n0.0\n0.0\n5.00\n5.000\n5.00\n\n\nWeek5_MP2\n107.0\n9.237757\n8.640610\n0.0\n0.0\n10.87\n18.045\n20.00\n\n\nWeek5_PR2\n107.0\n2.844673\n2.482099\n0.0\n0.0\n5.00\n5.000\n5.00\n\n\nWeek7_MP3\n107.0\n14.481869\n14.080211\n0.0\n0.0\n15.91\n27.440\n35.00\n\n\nWeek7_PR3\n107.0\n2.383178\n2.437501\n0.0\n0.0\n2.50\n5.000\n5.00\n\n\nWeek4_Quiz2\n107.0\n2.609439\n2.229419\n0.0\n0.0\n3.17\n4.710\n5.00\n\n\nWeek6_Quiz3\n107.0\n2.663551\n2.414359\n0.0\n0.0\n4.00\n5.000\n5.00\n\n\nWeek8_Total\n107.0\n47.380467\n41.035589\n0.0\n0.0\n71.53\n83.550\n99.71\n\n\nWeek1_Stat0\n107.0\n6.785047\n7.157300\n0.0\n1.0\n4.00\n12.000\n27.00\n\n\nWeek1_Stat1\n107.0\n0.000000\n0.000000\n0.0\n0.0\n0.00\n0.000\n0.00\n\n\nWeek1_Stat2\n107.0\n0.598131\n1.966031\n0.0\n0.0\n0.00\n0.000\n11.00\n\n\nWeek1_Stat3\n107.0\n0.224299\n0.730836\n0.0\n0.0\n0.00\n0.000\n6.00\n\n\nWeek2_Stat0\n107.0\n16.887850\n16.307960\n0.0\n6.5\n15.00\n24.000\n104.00\n\n\nWeek2_Stat1\n107.0\n13.299065\n11.308049\n0.0\n10.0\n11.00\n19.000\n57.00\n\n\nWeek2_Stat2\n107.0\n1.252336\n1.505295\n0.0\n1.0\n1.00\n1.000\n10.00\n\n\nWeek2_Stat3\n107.0\n1.841121\n4.202761\n0.0\n0.0\n0.00\n2.000\n25.00\n\n\nWeek3_Stat0\n107.0\n31.728972\n28.686191\n0.0\n3.0\n27.00\n47.500\n108.00\n\n\nWeek3_Stat1\n107.0\n2.822430\n3.206165\n0.0\n0.0\n4.00\n4.000\n15.00\n\n\nWeek3_Stat2\n107.0\n0.953271\n2.689544\n0.0\n0.0\n0.00\n0.000\n15.00\n\n\nWeek3_Stat3\n107.0\n1.130841\n2.269919\n0.0\n0.0\n0.00\n1.000\n14.00\n\n\nWeek4_Stat0\n107.0\n41.915888\n47.164330\n0.0\n0.5\n27.00\n74.500\n240.00\n\n\nWeek4_Stat1\n107.0\n16.046729\n17.040675\n0.0\n0.0\n15.00\n27.500\n87.00\n\n\nWeek4_Stat2\n107.0\n1.943925\n2.790928\n0.0\n0.0\n1.00\n2.000\n13.00\n\n\nWeek4_Stat3\n107.0\n1.009346\n2.806648\n0.0\n0.0\n0.00\n1.000\n24.00\n\n\nWeek5_Stat0\n107.0\n26.074766\n31.159269\n0.0\n0.0\n20.00\n40.500\n185.00\n\n\nWeek5_Stat1\n107.0\n5.009346\n6.568213\n0.0\n0.0\n4.00\n6.500\n39.00\n\n\nWeek5_Stat2\n107.0\n1.588785\n3.954823\n0.0\n0.0\n0.00\n1.000\n23.00\n\n\nWeek5_Stat3\n107.0\n0.663551\n1.822003\n0.0\n0.0\n0.00\n0.000\n11.00\n\n\nWeek6_Stat0\n107.0\n37.607477\n47.851334\n0.0\n0.0\n18.00\n65.000\n208.00\n\n\nWeek6_Stat1\n107.0\n14.271028\n14.009815\n0.0\n0.0\n15.00\n25.000\n51.00\n\n\nWeek6_Stat2\n107.0\n2.775701\n7.206271\n0.0\n0.0\n1.00\n2.000\n45.00\n\n\nWeek6_Stat3\n107.0\n0.411215\n1.220526\n0.0\n0.0\n0.00\n0.000\n7.00\n\n\nWeek7_Stat0\n107.0\n16.355140\n22.242341\n0.0\n0.0\n11.00\n27.000\n145.00\n\n\nWeek7_Stat1\n107.0\n3.242991\n5.001587\n0.0\n0.0\n0.00\n5.000\n24.00\n\n\nWeek7_Stat2\n107.0\n1.813084\n4.895379\n0.0\n0.0\n0.00\n1.000\n35.00\n\n\nWeek7_Stat3\n107.0\n1.252336\n2.399267\n0.0\n0.0\n0.00\n2.000\n12.00\n\n\nWeek8_Stat0\n107.0\n10.514019\n15.563846\n0.0\n0.0\n5.00\n14.000\n90.00\n\n\nWeek8_Stat1\n107.0\n3.130841\n4.841028\n0.0\n0.0\n0.00\n5.000\n27.00\n\n\nWeek8_Stat2\n107.0\n1.112150\n3.658351\n0.0\n0.0\n0.00\n0.000\n22.00\n\n\nWeek8_Stat3\n107.0\n0.355140\n1.191577\n0.0\n0.0\n0.00\n0.000\n9.00\n\n\nWeek9_Stat0\n107.0\n7.663551\n9.277630\n0.0\n1.0\n5.00\n11.000\n62.00\n\n\nWeek9_Stat1\n107.0\n1.607477\n2.687346\n0.0\n0.0\n0.00\n2.000\n12.00\n\n\nWeek9_Stat2\n107.0\n1.093458\n3.368928\n0.0\n0.0\n0.00\n0.500\n25.00\n\n\nWeek9_Stat3\n107.0\n0.046729\n0.483368\n0.0\n0.0\n0.00\n0.000\n5.00\n\n\nGrade\n107.0\n2.074766\n1.993863\n0.0\n0.0\n3.00\n4.000\n5.00\n\n\n\n\n\n\n\nFrom the description (in the pdf file) Week8_total seems to be a sum of the 9 grades, to check, I create a temporary column called sum and compare\n\ngrade_cols = ['Week2_Quiz1','Week4_Quiz2','Week6_Quiz3','Week3_MP1','Week5_MP2','Week7_MP3','Week3_PR1','Week5_PR2','Week7_PR3']\ndf['sum'] = df[grade_cols].sum(1)\ndf[['sum', 'Week8_Total']]\n\n\n\n\n\n\n\n\nsum\nWeek8_Total\n\n\nID\n\n\n\n\n\n\nML-2020-1\n82.97\n82.97\n\n\nML-2020-2\n82.43\n82.43\n\n\nML-2020-3\n79.44\n79.44\n\n\nML-2020-4\n80.65\n80.65\n\n\nML-2020-6\n67.68\n67.68\n\n\n...\n...\n...\n\n\nML-2020-60\n0.00\n0.00\n\n\nML-2020-58\n0.00\n0.00\n\n\nML-2020-94\n0.00\n0.00\n\n\nML-2020-9\n0.00\n0.00\n\n\nML-2020-86\n0.00\n0.00\n\n\n\n\n107 rows × 2 columns\n\n\n\nThese columns are the same representing the same data, hence, I can drop the individual values and keep to total. Also, since I do not plan to perform any time serious analysis (i.e., I’m not interested in the changes over weeks on the grades) I can keep the Week8_Total and drop the individual grade to reduce the features I need to consider. P.S: I also drop the temporary ‘sum’ colum\n\ngrade_cols.append('sum')\nprint(grade_cols)\ndf=df.drop(columns=grade_cols)\ndf.describe().T\n\n['Week2_Quiz1', 'Week4_Quiz2', 'Week6_Quiz3', 'Week3_MP1', 'Week5_MP2', 'Week7_MP3', 'Week3_PR1', 'Week5_PR2', 'Week7_PR3', 'sum']\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nWeek8_Total\n107.0\n47.380467\n41.035589\n0.0\n0.0\n71.53\n83.55\n99.71\n\n\nWeek1_Stat0\n107.0\n6.785047\n7.157300\n0.0\n1.0\n4.00\n12.00\n27.00\n\n\nWeek1_Stat1\n107.0\n0.000000\n0.000000\n0.0\n0.0\n0.00\n0.00\n0.00\n\n\nWeek1_Stat2\n107.0\n0.598131\n1.966031\n0.0\n0.0\n0.00\n0.00\n11.00\n\n\nWeek1_Stat3\n107.0\n0.224299\n0.730836\n0.0\n0.0\n0.00\n0.00\n6.00\n\n\nWeek2_Stat0\n107.0\n16.887850\n16.307960\n0.0\n6.5\n15.00\n24.00\n104.00\n\n\nWeek2_Stat1\n107.0\n13.299065\n11.308049\n0.0\n10.0\n11.00\n19.00\n57.00\n\n\nWeek2_Stat2\n107.0\n1.252336\n1.505295\n0.0\n1.0\n1.00\n1.00\n10.00\n\n\nWeek2_Stat3\n107.0\n1.841121\n4.202761\n0.0\n0.0\n0.00\n2.00\n25.00\n\n\nWeek3_Stat0\n107.0\n31.728972\n28.686191\n0.0\n3.0\n27.00\n47.50\n108.00\n\n\nWeek3_Stat1\n107.0\n2.822430\n3.206165\n0.0\n0.0\n4.00\n4.00\n15.00\n\n\nWeek3_Stat2\n107.0\n0.953271\n2.689544\n0.0\n0.0\n0.00\n0.00\n15.00\n\n\nWeek3_Stat3\n107.0\n1.130841\n2.269919\n0.0\n0.0\n0.00\n1.00\n14.00\n\n\nWeek4_Stat0\n107.0\n41.915888\n47.164330\n0.0\n0.5\n27.00\n74.50\n240.00\n\n\nWeek4_Stat1\n107.0\n16.046729\n17.040675\n0.0\n0.0\n15.00\n27.50\n87.00\n\n\nWeek4_Stat2\n107.0\n1.943925\n2.790928\n0.0\n0.0\n1.00\n2.00\n13.00\n\n\nWeek4_Stat3\n107.0\n1.009346\n2.806648\n0.0\n0.0\n0.00\n1.00\n24.00\n\n\nWeek5_Stat0\n107.0\n26.074766\n31.159269\n0.0\n0.0\n20.00\n40.50\n185.00\n\n\nWeek5_Stat1\n107.0\n5.009346\n6.568213\n0.0\n0.0\n4.00\n6.50\n39.00\n\n\nWeek5_Stat2\n107.0\n1.588785\n3.954823\n0.0\n0.0\n0.00\n1.00\n23.00\n\n\nWeek5_Stat3\n107.0\n0.663551\n1.822003\n0.0\n0.0\n0.00\n0.00\n11.00\n\n\nWeek6_Stat0\n107.0\n37.607477\n47.851334\n0.0\n0.0\n18.00\n65.00\n208.00\n\n\nWeek6_Stat1\n107.0\n14.271028\n14.009815\n0.0\n0.0\n15.00\n25.00\n51.00\n\n\nWeek6_Stat2\n107.0\n2.775701\n7.206271\n0.0\n0.0\n1.00\n2.00\n45.00\n\n\nWeek6_Stat3\n107.0\n0.411215\n1.220526\n0.0\n0.0\n0.00\n0.00\n7.00\n\n\nWeek7_Stat0\n107.0\n16.355140\n22.242341\n0.0\n0.0\n11.00\n27.00\n145.00\n\n\nWeek7_Stat1\n107.0\n3.242991\n5.001587\n0.0\n0.0\n0.00\n5.00\n24.00\n\n\nWeek7_Stat2\n107.0\n1.813084\n4.895379\n0.0\n0.0\n0.00\n1.00\n35.00\n\n\nWeek7_Stat3\n107.0\n1.252336\n2.399267\n0.0\n0.0\n0.00\n2.00\n12.00\n\n\nWeek8_Stat0\n107.0\n10.514019\n15.563846\n0.0\n0.0\n5.00\n14.00\n90.00\n\n\nWeek8_Stat1\n107.0\n3.130841\n4.841028\n0.0\n0.0\n0.00\n5.00\n27.00\n\n\nWeek8_Stat2\n107.0\n1.112150\n3.658351\n0.0\n0.0\n0.00\n0.00\n22.00\n\n\nWeek8_Stat3\n107.0\n0.355140\n1.191577\n0.0\n0.0\n0.00\n0.00\n9.00\n\n\nWeek9_Stat0\n107.0\n7.663551\n9.277630\n0.0\n1.0\n5.00\n11.00\n62.00\n\n\nWeek9_Stat1\n107.0\n1.607477\n2.687346\n0.0\n0.0\n0.00\n2.00\n12.00\n\n\nWeek9_Stat2\n107.0\n1.093458\n3.368928\n0.0\n0.0\n0.00\n0.50\n25.00\n\n\nWeek9_Stat3\n107.0\n0.046729\n0.483368\n0.0\n0.0\n0.00\n0.00\n5.00\n\n\nGrade\n107.0\n2.074766\n1.993863\n0.0\n0.0\n3.00\n4.00\n5.00\n\n\n\n\n\n\n\n\n\nPlot the number of students who scored each grade\n\nsns.countplot(df, x=\"Grade\")\n\n&lt;Axes: xlabel='Grade', ylabel='count'&gt;\n\n\n\n\n\n\n\nSee if there are missng values\n\nsns.heatmap(df.isnull())\n\n&lt;Axes: ylabel='ID'&gt;\n\n\n\n\n\nThere are no missing values for any of the students, perhaps missing a quiz or an assignment has been equated to a grade of zero\n\ncor = df.corr()\nsns.heatmap(cor)\n\ncor_target = abs(cor['Grade'])\n# Selecting highly correlated features\nrelevant_features = cor_target[cor_target&gt;0.7]\nrelevant_features\n\nWeek8_Total    0.972348\nWeek6_Stat1    0.771988\nGrade          1.000000\nName: Grade, dtype: float64\n\n\n\n\n\nWe see, as expected, Week8_total is a highly correlated to final Grade. Interestingly Week6_Stat1 seems to have a high correlation to Grade. This means, students who had high activity during the sixth week of the course had scored better on the final grade (I say they scored better because we can see below the correlation is positive ) ## 2.4 Preprocessing\n\ncor[\"Week6_Stat1\"][\"Grade\"]\n\n0.7719882914970566\n\n\nI can remove the Week8_Total grade from the features list if I can be sure that the final grade is full based on the this total grade or I can keep it in case other factors were considered in the final grade calculation. In this case the machine learning model I build will be highly dependent on this variable to the point the other features will became irrelevant given the high correlation value observed between this feature and the target (i.e., Grade)\n\n#df = df.drop(columns=['Week8_Total'], axis=1)\n\n\n#irrelevant_features = cor_target[(cor_target&gt;=0.4) & (cor_target&lt;=0.5)]\nirrelevant_features = cor_target[(cor_target&gt;=0.45) & (cor_target&lt;=0.55)]\nirrelevant_features\n\nWeek5_Stat1    0.484030\nWeek8_Stat0    0.450807\nWeek9_Stat0    0.545532\nWeek9_Stat1    0.496753\nName: Grade, dtype: float64\n\n\n\n#df = df.drop(columns=list(irrelevant_features.index), axis=1)"
  },
  {
    "objectID": "posts/student-grade-prediction.html#split-features-and-target-column",
    "href": "posts/student-grade-prediction.html#split-features-and-target-column",
    "title": "Student Grade Prediction",
    "section": "Split Features and target column",
    "text": "Split Features and target column\n\nx = df.drop(columns=['Grade'], axis=1).to_numpy()\ny = df['Grade'].to_numpy()\n\n\nNormalize the features to bring them all to 0-1 range\n\nx_normalized = preprocessing.normalize(x)\nx_normalized\n\narray([[0.72952054, 0.        , 0.        , ..., 0.05275549, 0.00879258,\n        0.        ],\n       [0.3833634 , 0.0372062 , 0.        , ..., 0.01395233, 0.00930155,\n        0.02325388],\n       [0.72356708, 0.03643339, 0.        , ..., 0.        , 0.00910835,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.29411765, 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.41380294, 0.        , ..., 0.        , 0.        ,\n        0.        ]])\n\n\n\n\nSplit training and Test data\n\nx_train, x_test, y_train, y_test = train_test_split(x_normalized,y, test_size=.20)\n\n\nprint(\"Training X Shape: {}\".format(x_train.shape))\nprint(\"Training Y Shape: {}\".format(y_train.shape))\nprint(\"Test X Shape: {}\".format(x_test.shape))\nprint(\"Test y Shape: {}\".format(y_test.shape))\n\nTraining X Shape: (85, 37)\nTraining Y Shape: (85,)\nTest X Shape: (22, 37)\nTest y Shape: (22,)\n\n\n\nSummary of Preprocessing\n\nFeature Selection\nGiven 107 records with 47 attributes I have removed 11 of the attributes during the pre-processing for the following reasons\n\nID: THis is a unique identifier of each row of the data that dose not contain relevant information for our prediction task.\ngrade_columns: These are columns that add up to give the Week8_Total column, since there is a high correlation between these and the total column, I removed them in favor of Week8_Total which provides the same information in a single feature.\nWeek8_Total: I finally remove Week8_Total because it has a high correlation with the target feature which will make our prediction very skewed towards this feature. The is a design decision to avoid a deterministic prediction.\n\n\n\nNormalization\nThen I normalized the data to bring values for each feature between [0-1].\n\n\nTraining/Test Split\nI split the data into training and testing set of, first, 90% to 10% ratio. Even though it is recommended to use a 20% testing split, given our small data set I started with 10% and I came back to update this ratio based on my finding later which showed over-fitting."
  },
  {
    "objectID": "posts/student-grade-prediction.html#training",
    "href": "posts/student-grade-prediction.html#training",
    "title": "Student Grade Prediction",
    "section": "3. Training",
    "text": "3. Training\nThe problem of predicting one of the six grades given a set of features is a classification problem. I’ll first try to train and test multiple classifiers with their default hyperparameter values.\n\nmodels = {\n    'LogisticRegression':LogisticRegression(),\n    'MultinomialNB': MultinomialNB(),\n    'BaggingClassifier': BaggingClassifier(), \n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'LinearSVC':LinearSVC(),\n    'SGDClassifier':SGDClassifier(),\n    'KNeighborsClassifier':KNeighborsClassifier(), \n    'RandomForestClassifier':RandomForestClassifier(), \n    'GradientBoostingClassifier':GradientBoostingClassifier(),\n    \"SVC\":SVC(),\n    \"GaussianProcessClassifier\": GaussianProcessClassifier(),\n    \"MLPClassifier\": MLPClassifier(),\n    \"AdaBoostClassifier\": AdaBoostClassifier(),\n    \"GaussianNB\":GaussianNB(),\n    \"QuadraticDiscriminantAnalysis\":QuadraticDiscriminantAnalysis(),\n}\n\n\nperformance = pd.DataFrame(columns=[\"Train Accuracy\",\"Test Accuracy\"], index=list(models.keys()))\n\n\nfor name, model in models.items():\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Make predictions on the test set\n    y_train_pred = model.predict(x_train)\n    y_test_pred = model.predict(x_test)\n\n    # Calculate evaluation metrics\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n\n    #Save Test and Training Accuracies\n    performance.loc[name, :] = [train_accuracy,test_accuracy]"
  },
  {
    "objectID": "posts/student-grade-prediction.html#evaluation",
    "href": "posts/student-grade-prediction.html#evaluation",
    "title": "Student Grade Prediction",
    "section": "4. Evaluation",
    "text": "4. Evaluation\n\nperformance\n\n\n\n\n\n\n\n\nTrain Accuracy\nTest Accuracy\n\n\n\n\nLogisticRegression\n0.647059\n0.818182\n\n\nMultinomialNB\n0.6\n0.818182\n\n\nBaggingClassifier\n0.964706\n0.863636\n\n\nDecisionTreeClassifier\n1.0\n0.681818\n\n\nLinearSVC\n0.752941\n0.772727\n\n\nSGDClassifier\n0.694118\n0.727273\n\n\nKNeighborsClassifier\n0.647059\n0.681818\n\n\nRandomForestClassifier\n1.0\n0.863636\n\n\nGradientBoostingClassifier\n1.0\n0.863636\n\n\nSVC\n0.658824\n0.818182\n\n\nGaussianProcessClassifier\n0.658824\n0.818182\n\n\nMLPClassifier\n0.729412\n0.772727\n\n\nAdaBoostClassifier\n0.647059\n0.681818\n\n\nGaussianNB\n0.741176\n0.636364\n\n\nQuadraticDiscriminantAnalysis\n1.0\n0.727273"
  },
  {
    "objectID": "posts/student-grade-prediction.html#optimization",
    "href": "posts/student-grade-prediction.html#optimization",
    "title": "Student Grade Prediction",
    "section": "5. Optimization",
    "text": "5. Optimization\nFrom our trial run above SGDClassifier,LinearSVC and MLPClassifier seem to have the best accuracy score. Here I’ll try to optimize these algorithms with different hyperparameter values to improve accuracy. We do this with the GirdSearch algorithm for hyperparameter tuning.\n\nfrom sklearn.model_selection import GridSearchCV\n\n\nSGDClassifier\n\nmodel =  SGDClassifier()\n\ngrid = {\n        'alpha' :[0.0001,0.001, 0.01, 0.1, 1, 10]\n        }\ngridSearch = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, scoring=\"accuracy\")\nsearchResults = gridSearch.fit(x_train, y_train)\n\nscore = accuracy_score(searchResults.best_estimator_.predict(x_test),y_test)\nparams = searchResults.best_estimator_.get_params()\nprint(\"SGDClassifier  best estimator \\n alpha = {} \\n accuracy_score = {}\".format(params[\"alpha\"],score))\n\nSGDClassifier  best estimator \n alpha = 0.01 \n accuracy_score = 0.6818181818181818\n\n\n\n\nLinearSVC\n\nmodel = LinearSVC ()\ngrid = {'C':[0.0001,0.001, 0.01, 0.1, 1, 10]}\ngridSearch = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, scoring=\"accuracy\")\nsearchResults = gridSearch.fit(x_train, y_train)\n\nscore = accuracy_score(searchResults.best_estimator_.predict(x_test),y_test)\nparams = searchResults.best_estimator_.get_params()\nprint(\"LinearSVC best estimator \\n C = {} \\n accuracy_score = {}\".format(params[\"C\"],score))\n\nLinearSVC best estimator \n C = 1 \n accuracy_score = 0.7727272727272727\n\n\n\n\nMLPClassifier\n\nmodel = MLPClassifier()\n\ngrid={\n'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"],\n'alpha': [0.0001,0.001, 0.01, 0.1, 1, 10],\n}\n\ngridSearch = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, scoring=\"accuracy\")\nsearchResults = gridSearch.fit(x_train, y_train)\n\nscore = accuracy_score(searchResults.best_estimator_.predict(x_test),y_test)\nparams = searchResults.best_estimator_.get_params()\nlearning_rate = params[\"learning_rate\"]\nprint(\"MLPClassifier best estimator\")\nprint(\" alpha = {} \\n learning_rate = {}\".format(params['alpha'],params['learning_rate']))\nprint(\" accuracy_score = {}\".format(score))\n\nMLPClassifier best estimator\n alpha = 1 \n learning_rate = invscaling\n accuracy_score = 0.8181818181818182\n\n\nThe Best performance is obtained by LinearSVC classifier which is 82%"
  },
  {
    "objectID": "posts/pca.html",
    "href": "posts/pca.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Banner Image Credit: Jernej Furman"
  },
  {
    "objectID": "posts/pca.html#introduction",
    "href": "posts/pca.html#introduction",
    "title": "Principal Component Analysis",
    "section": "Introduction",
    "text": "Introduction\nPrincipal component analysis (PCA) is a widely used statistical method for examining extensive datasets characterized by numerous dimensions or features per observation. PCA is a relatively well-established theory with its roots dating back to the beginning of the 1900s. Its primary aim is to enhance data interpretability while conserving the maximum information possible, thereby enabling the visualization of multidimensional data. Essentially, PCA serves as a statistical approach to diminish dataset dimensionality. This is achieved by transforming the data through linear methods into a fresh coordinate system where a reduced number of dimensions can effectively encapsulate (most of) the data variation compared to the original dataset.\nIn various studies, researchers often employ the initial two principal components to plot data in a two-dimensional space, facilitating the identification of clusters among closely associated data points. The applications of principal component analysis span across diverse fields including population genetics, microbiome studies, and atmospheric science. As Machine Learning(ML) is a data-driven approach to building algorithms, PCA plays an important role in ML for Dimensionality Reduction, Data Preprocessing, Visualization, Decorrelation and Feature Extraction, Speeding Up Learning Algorithms, and Noise Filtering.\nThe idea behind PCA is, that given a dataset with some statistical distribution (i.e., not deterministic), we would like to find features that can best describe as much of the data as possible. By doing so we will be able to explain the data with fewer sets of features than the actual data has. In data involving numerous variables and dimensions, not all variables hold equal importance. Some variables are key while others are less critical. The Principal Component Analysis (PCA) method offers a systematic way to identify and eliminate less significant variables. By transforming the original variables into uncorrelated principal components, PCA retains essential information while discarding less important variables. This process simplifies complex datasets by focusing on the principal components that capture the majority of the dataset’s variance. Consequently, PCA enhances data transparency by emphasizing the critical factors while reducing unnecessary complexity.\nIn this blog, we will go through the steps for calculating PCA with an example data set demonstrating PCA in action to perform dimensionality reduction.\n\nWhat Exactly is Principal Component\nThe principal component represents a novel feature or feature formed by combining the original features linearly. By crafting one or more of these new features, the objective is to ensure that these combinations result in uncorrelated variables, known as principal components. This process involves condensing or compressing most of the information from the initial variables into the first component. In essence, when dealing with n-dimensional data, PCA generates n principal components. However, the primary goal of PCA is to maximize information encapsulation in the initial component, followed by retaining the maximum remaining information in subsequent components. This systematic approach prioritizes the encapsulation of significant data details within these newly constructed components in a step-by-step manner.\nArranging information within principal components enables effective dimensionality reduction without significant loss of information. This process involves discarding components that hold minimal information while regarding the remaining components as the new variables. By prioritizing the retention of informative components and disregarding those with lower significance, one can streamline the dataset and create a new set of variables that effectively captures the essential information from the original dataset. This approach facilitates a more concise representation of the data while preserving the most critical information contained within the retained components. While representing data with less number of features reduces the complexity of the data aiding in different aspects of machine learning it complicates understanding of the data as the newly formed features (i.e., principal components) do not have a one-to-one mapping with the original feature set. ## Data The data we will use in this post comes with seaborn plotting package. Specifically, we’ll use the car_crashes dataset from seaborn package. If you’re new to seaborn, I’ve another post that demonstrates different plot types available in seaborn.\nThe car_crashes dataset is data about car accidents, their cause, and cost to insurance companies in the states of the USA. It consists of the following features.\n\ntotal: Number of drivers involved in fatal collisions per billion miles (5.900–23.900)\nspeeding: Percentage Of Drivers Involved In Fatal Collisions Who Were Speeding (1.792–9.450)\nalcohol: Percentage Of Drivers Involved In Fatal Collisions Who Were Alcohol-Impaired (1.593–10.038)\nnot_distracted: Percentage Of Drivers Involved In Fatal Collisions Who Were Not Distracted (1.760–23.661)\nno_previous: Percentage Of Drivers Involved In Fatal Collisions Who Had Not Been Involved In Any Previous Accidents (5.900–21.280)\nins_premium: Car Insurance Premiums (641.960–1301.520)\nins_losses: Losses incurred by insurance companies for collisions per insured driver (82.75–194.780)\nabbrev: A two-letter abbreviation of US state name the data stands for\n\n\nImport the required packages\nLet’s start by importing the necessary packages\n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n\n\n\n\nLoad the data\nWe can now load the data and try to understand it\n\ncar_crashes = sns.load_dataset(\"car_crashes\")\ncar_crashes.head()\n\n\n\n\n\n\n\n\ntotal\nspeeding\nalcohol\nnot_distracted\nno_previous\nins_premium\nins_losses\nabbrev\n\n\n\n\n0\n18.8\n7.332\n5.640\n18.048\n15.040\n784.55\n145.08\nAL\n\n\n1\n18.1\n7.421\n4.525\n16.290\n17.014\n1053.48\n133.93\nAK\n\n\n2\n18.6\n6.510\n5.208\n15.624\n17.856\n899.47\n110.35\nAZ\n\n\n3\n22.4\n4.032\n5.824\n21.056\n21.280\n827.34\n142.39\nAR\n\n\n4\n12.0\n4.200\n3.360\n10.920\n10.680\n878.41\n165.63\nCA\n\n\n\n\n\n\n\nWe can see all features except the abbreviation (which represents the US state the sample stands for) are continuous-valued.\n\ncar_crashes.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 51 entries, 0 to 50\nData columns (total 8 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   total           51 non-null     float64\n 1   speeding        51 non-null     float64\n 2   alcohol         51 non-null     float64\n 3   not_distracted  51 non-null     float64\n 4   no_previous     51 non-null     float64\n 5   ins_premium     51 non-null     float64\n 6   ins_losses      51 non-null     float64\n 7   abbrev          51 non-null     object \ndtypes: float64(7), object(1)\nmemory usage: 3.3+ KB\n\n\nWe’ll remove the apprev column as it is not relevant to our demonstration in this post\n\ncar_crashes = car_crashes.drop('abbrev', axis=1)\n\n\n\nMaximum, Minimum, Standard deviation and Average Values\nThe maximum value in each of the columns\n\ncar_crashes.max()\n\ntotal               23.900\nspeeding             9.450\nalcohol             10.038\nnot_distracted      23.661\nno_previous         21.280\nins_premium       1301.520\nins_losses         194.780\ndtype: float64\n\n\nThe minimum value in each of the columns\n\ncar_crashes.min()\n\ntotal               5.900\nspeeding            1.792\nalcohol             1.593\nnot_distracted      1.760\nno_previous         5.900\nins_premium       641.960\nins_losses         82.750\ndtype: float64\n\n\nThe range in each column is the difference between the max and minimum values\n\ncar_crashes_range = car_crashes.max() - car_crashes.min()\ncar_crashes_range\n\ntotal              18.000\nspeeding            7.658\nalcohol             8.445\nnot_distracted     21.901\nno_previous        15.380\nins_premium       659.560\nins_losses        112.030\ndtype: float64\n\n\nWe can visualize the difference in the range\n\np1 = car_crashes_range.plot(\n    legend=False,\n    kind=\"bar\",\n    rot=45,\n    color=\"blue\",\n    fontsize=16,\n)\np1.set_title(\"Range of values in our car crash data\", fontsize=16)\np1.set_xlabel(\"Feature\", fontsize=14)\np1.set_ylabel(\"Range\", fontsize=14)\n\nText(0, 0.5, 'Range')\n\n\n\n\n\nWe can get the mean value and standard deviation of each column as well\n\ncar_crashes.mean(axis=0)\n\ntotal              15.790196\nspeeding            4.998196\nalcohol             4.886784\nnot_distracted     13.573176\nno_previous        14.004882\nins_premium       886.957647\nins_losses        134.493137\ndtype: float64\n\n\n\ncar_crashes.std(axis=0)\n\ntotal               4.122002\nspeeding            2.017747\nalcohol             1.729133\nnot_distracted      4.508977\nno_previous         3.764672\nins_premium       178.296285\nins_losses         24.835922\ndtype: float64"
  },
  {
    "objectID": "posts/pca.html#computing-pca",
    "href": "posts/pca.html#computing-pca",
    "title": "Principal Component Analysis",
    "section": "Computing PCA",
    "text": "Computing PCA\nCalculating PCA involves five steps. These are\n\nStandardization\nCovariance Matrix Computation\nEigenvectors and Eigenvalues\nFeature Vector\nRecast the Data\n\nNow that we’ve explained the data we’ll use for demonstrating PCA and the steps involved, let’s go through each step with the aforementioned data and see PCA in action."
  },
  {
    "objectID": "posts/pca.html#pca-in-action",
    "href": "posts/pca.html#pca-in-action",
    "title": "Principal Component Analysis",
    "section": "PCA in Action",
    "text": "PCA in Action\n\nStep 1. Standardization\nPCA is sensitive to variance, meaning if features different range in values the PCA calculation will be dominated by features with a larger range. In our dataset, we have features that represent a percentage value (hence the potential values range from 0 to 100, but the actual maximum value in the dataset is way below 100). We also have other features that have a more inconsistent range. For example, the ins_premium has a range of 659.560 while speading has a range of 7.658. As a result, the PCA analysis will be greatly (and incorrectly) influenced by a change in ins_premium value.\nTo address this issue, we perform standardization before PCA, that is we bring values of all features to a standard range. The one common way to standardize any data is the following formula.\n\\[\nz = \\frac{v - \\mu}{\\sigma}\n\\] Where \\(z\\) is the standardized value, \\(v\\) is the original value, \\(\\mu\\) stands for mean and \\(\\sigma\\) is the standard deviation\nWe can perform this calculation in python as follows\n\ncar_crashes_std = (car_crashes - car_crashes.mean(axis=0)) / car_crashes.std(axis=0)\ncar_crashes_std.head()\n\n\n\n\n\n\n\n\ntotal\nspeeding\nalcohol\nnot_distracted\nno_previous\nins_premium\nins_losses\n\n\n\n\n0\n0.730180\n1.156638\n0.435603\n0.992425\n0.274956\n-0.574368\n0.426272\n\n\n1\n0.560360\n1.200747\n-0.209229\n0.602537\n0.799304\n0.933964\n-0.022674\n\n\n2\n0.681660\n0.749253\n0.185767\n0.454831\n1.022962\n0.070177\n-0.972106\n\n\n3\n1.603542\n-0.478849\n0.542015\n1.659539\n1.932471\n-0.334374\n0.317961\n\n\n4\n-0.919504\n-0.395588\n-0.882977\n-0.588421\n-0.883180\n-0.047941\n1.253703\n\n\n\n\n\n\n\nLet’s perform a similar visualization on the range as we did earlier\n\np2 = (car_crashes_std.max() - car_crashes_std.min()).plot(\n    legend=False, kind=\"bar\", rot=45, color=\"blue\", fontsize=16\n)\np2.set_title(\"Range of values in our car crash data after standardization\", fontsize=16)\np2.set_xlabel(\"Features\", fontsize=14)\np2.set_ylabel(\"Range\", fontsize=14)\n\nText(0, 0.5, 'Range')\n\n\n\n\n\nWe can see that now the range is more consistent across the features\n\n\nStep 2. Covariance Matrix\nCovariance Matrix To get a sense of how the values of the input dataset vary from the mean to each other we compute the covariance matrix. The covariance matrix is a \\(m × m\\) symmetric matrix (where m is the number of features, we’ve in our input dataset). Features might be highly correlated and through computing the covariance matrix we can identify this relationship.\nFor example, a covariance matrix of an input dataset with 4 features \\((f_1,f_2,f_3,f_4)\\) will be a 4x4 matrix of the following form\n\\[\nCovariance Matrix = \\begin{bmatrix}\n      cov(f_1,f_1) & cov(f_1,f_2) & cov(f_1,f_3) & cov(f_1,f_4) \\\\\n      cov(f_2,f_1) & cov(f_2,f_2) & cov(f_2,f_3) & cov(f_2,f_4) \\\\\n      cov(f_3,f_1) & cov(f_3,f_2) & cov(f_3,f_3) & cov(f_3,f_4) \\\\\n      cov(f_4,f_1) & cov(f_4,f_2) & cov(f_3,f_3) & cov(f_4,f_4)\n\\end{bmatrix}\n\\]\nWe compute the covariance between to features \\(f_x\\) and \\(f_y\\) as follows \\[\ncov(f_x,f_y) = \\dfrac{\\sum(x_i-\\mu_x) (y_i-\\mu_y)}{n}\n\\] Where \\(x_i\\) and \\(y_i\\) are the \\(i^{th}\\) values for feature \\(x\\) and \\(y\\) and \\(n\\) is the total number of data points.\nWith this equation notice how\n\ncovariance of a feature f with itself (i.e., \\(Cov(f_x,f_x)\\)) the variance of the feature (i.e., \\(Var(f_x)\\))\ncovariance is symmetric, i.e., \\(Cov(fx,fy) = Cov(f_y,f_x)\\)\n\nNow that we know that the covariance matrix is not more than a table that summarizes the correlations between all the possible pairs of variables, let’s move to the next step. While we can use these equations to calculate the covariance matrix pandas provide a built-in method for calculating it in one go.\n\ncov_mat = car_crashes_std.cov()\n\nWe can visualize this matrix using seaborn’s heatmap\n\nsns.heatmap(cov_mat, annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nStep 3. Eigenvectors and Eigenvalues\nEigenvectors and eigenvalues, fundamental concepts from linear algebra, play a crucial role in computing the principal components of a dataset from its covariance matrix. In an n-dimensional dataset comprising n features, there exist n eigenvectors accompanied by n corresponding eigenvalues.\nEigenvectors derived from the Covariance matrix represent the axes’ directions where the highest variance or most information is concentrated. These eigenvectors are termed Principal Components. Eigenvalues, on the other hand, serve as coefficients associated with eigenvectors, signifying the quantity of variance encapsulated within each Principal Component. Essentially, eigenvalues provide a measure of the amount of variance carried by the respective eigenvectors, highlighting their significance in determining the variance distribution across the dataset’s principal axes.\nArranging your eigenvectors based on their corresponding eigenvalues, from highest to lowest, provides the principal components in descending order of significance. This ranking scheme ensures that the principal components are prioritized based on the amount of variance or information they encapsulate. The eigenvector associated with the highest eigenvalue represents the most significant principal component, followed by subsequent components in decreasing order of importance, allowing for a systematic determination of the most influential axes within the dataset.\nWe can use numpy’s linalg.eigh( ) method to calcuate eigenvectors and eigenvalues a matrix.\n\neigen_values, eigen_vectors = np.linalg.eigh(cov_mat)\n\n\neigen_values\n\narray([0.02747434, 0.19865996, 0.28077   , 0.350529  , 0.55060199,\n       1.57801295, 4.01395176])\n\n\n\neigen_vectors\n\narray([[ 0.80082487, -0.16970508, -0.14597659, -0.0389558 ,  0.26908802,\n        -0.06893769, -0.47947078],\n       [ 0.01784783, -0.22479246,  0.02282818,  0.36374887, -0.81826935,\n        -0.0765846 , -0.37534719],\n       [-0.15285774,  0.7837677 , -0.35479821, -0.14834351, -0.08293253,\n        -0.03345835, -0.45437635],\n       [-0.14247844,  0.08510479,  0.85646854, -0.1712655 ,  0.12616845,\n        -0.04237473, -0.4380328 ],\n       [-0.55875371, -0.50401185, -0.33611019, -0.03948141,  0.31798812,\n        -0.0961294 , -0.45703414],\n       [ 0.04126619, -0.11577348, -0.04214531, -0.65639617, -0.25614247,\n        -0.6852266 ,  0.1308319 ],\n       [-0.02804966,  0.17805184,  0.06327152,  0.61839859,  0.26173503,\n        -0.71252436,  0.06996048]])\n\n\nWe should now sort the Eigenvalues in the descending order along with their corresponding Eigenvector.\n\n# sort the eigenvalues in descending order\nsorted_index = np.argsort(eigen_values)[::-1]\n\nsorted_eigenvalue = eigen_values[sorted_index]\n\n# Similarly sort the eigenvectors\nsorted_eigenvectors = eigen_vectors[:, sorted_index]\n\n\n\nStep 4. Feature Vector\nOnce sorted, we can select the subset of the Eigenvalue as per our requirement. In this case, since we are interested in the two principal components, we take the first two values (i.e. n_components = 2). This forms our feature vector which is a matrix that has as columns the eigenvectors of the components that we decide to keep. By only keeping a subset of the Eigenvectors we are reducing the number of features hence the notion of dimensionality reduction.\n\nn_components = 2 \nfeature_vector = sorted_eigenvectors[:,0:n_components]\n\nfeature_vector\n\narray([[-0.47947078, -0.06893769],\n       [-0.37534719, -0.0765846 ],\n       [-0.45437635, -0.03345835],\n       [-0.4380328 , -0.04237473],\n       [-0.45703414, -0.0961294 ],\n       [ 0.1308319 , -0.6852266 ],\n       [ 0.06996048, -0.71252436]])\n\n\n\n\nStep 5. Recast the Data\n\ncar_crash_reduced = -np.dot(\n    feature_vector.transpose(), car_crashes_std.transpose()\n).transpose()\ncars_pca = pd.DataFrame(data=car_crash_reduced, columns=[\"PC 1\", \"PC 2\"])\ncars_pca.head()\n\n\n\n\n\n\n\n\nPC 1\nPC 2\n\n\n\n\n0\n1.587871\n0.132134\n\n\n1\n1.132939\n0.849778\n\n\n2\n1.418062\n-0.416363\n\n\n3\n2.467035\n0.345530\n\n\n4\n-1.733390\n0.627382\n\n\n\n\n\n\n\n\n\nUsing scikit-learn package\nThe scikit-learn package comes with an API to calculate PCA without having to follow the above steps\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\n\npcs = pca.fit_transform(car_crashes_std)\n\nprincipal_cars = pd.DataFrame(data=pcs, columns=[\"PC 1\", \"PC 2\"])\n\nprincipal_cars.head()\n\n\n\n\n\n\n\n\nPC 1\nPC 2\n\n\n\n\n0\n1.587871\n0.132134\n\n\n1\n1.132939\n0.849778\n\n\n2\n1.418062\n-0.416363\n\n\n3\n2.467035\n0.345530\n\n\n4\n-1.733390\n0.627382\n\n\n\n\n\n\n\nOne advantage of the scikit-learn implementation of PCA is that it implements Probabilistic Principal Component Analysis 1. This enables us obtain the log maximum-likelihood of each sample. Maximum likelihood refers to the degree (in mean or standard deviation) the probablity distribution we’ve is going to represent most of the data we are trying to model. The PPCA implementation in scikit-learn gives as access to the log likelihood score which we can use to compare different distributions (in this case different number of principal components )\n\npca.score_samples(car_crashes_std)\n\narray([ -6.44954331,  -7.53883983,  -6.40958592, -11.35786508,\n        -6.61054168,  -5.03977718,  -7.12053256,  -5.96378314,\n       -10.50232239,  -8.20041332,  -6.42991259, -10.97177485,\n        -7.06959704,  -4.9868997 ,  -5.54486928,  -7.78448959,\n        -5.31230145,  -7.44998984,  -9.88242453,  -6.02864696,\n        -8.31301121,  -6.69510241,  -6.02183068,  -6.18648717,\n       -20.56341095,  -6.28341612, -12.20777983,  -8.11910771,\n        -4.96080412,  -5.43102784,  -8.87161592,  -8.04646994,\n        -7.11756172,  -5.79150497, -12.14066975,  -6.12536757,\n        -8.30172833,  -5.96072048,  -8.82359145,  -6.85850641,\n        -9.58963574,  -6.04225252,  -7.74060643,  -6.44307295,\n        -7.73434658,  -5.14398609,  -7.48886884,  -6.43042227,\n        -7.67794805,  -9.04404655,  -5.75015305])\n\n\nThese values represent how much of the data (likelihood) our PCA components are covering. To get an average value we can use a different method from PCA as follows\n\npca.score(car_crashes_std)\n\n-7.61880713471763\n\n\nThe log-likelihood value for a given model can range from negative infinity to positive infinity. The actual log-likelihood value for a given model is mostly meaningless, but it’s useful for comparing two or more models. We can compare our previous PCA model with one that has 3, 4, 5 components\n\npca3 = PCA(n_components=3)\n\npcs3 = pca3.fit_transform(car_crashes_std)\n\npca3.score(car_crashes_std)\n\n-7.408329581463735\n\n\n\npca4 = PCA(n_components=4)\n\npcs4 = pca4.fit_transform(car_crashes_std)\n\npca4.score(car_crashes_std)\n\n-7.297318147783873\n\n\n\npca5 = PCA(n_components=5)\n\npcs5 = pca5.fit_transform(car_crashes_std)\n\npca5.score(car_crashes_std)\n\n-7.149503033441305\n\n\nWhile there is an improvement in the actual score we are increasing the number of components. Since our objective is to reduce the number of dimensions (features) and adding more components works against that aim, the improvements gained in log-likelihood score are not worth the additional dimensions we are introducing."
  },
  {
    "objectID": "posts/Anomaly Detection.html",
    "href": "posts/Anomaly Detection.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Banner Image Credit Evidently AI"
  },
  {
    "objectID": "posts/Anomaly Detection.html#imports",
    "href": "posts/Anomaly Detection.html#imports",
    "title": "Anomaly Detection",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.cluster import DBSCAN\n\nLet’s see what comes with seaborn\n\nprint(sns.get_dataset_names())\n\n['anagrams', 'anscombe', 'attention', 'brain_networks', 'car_crashes', 'diamonds', 'dots', 'dowjones', 'exercise', 'flights', 'fmri', 'geyser', 'glue', 'healthexp', 'iris', 'mpg', 'penguins', 'planets', 'seaice', 'taxis', 'tips', 'titanic']"
  },
  {
    "objectID": "posts/Anomaly Detection.html#load-the-data",
    "href": "posts/Anomaly Detection.html#load-the-data",
    "title": "Anomaly Detection",
    "section": "Load the data",
    "text": "Load the data\nLet us load the car car titanic dataset\n\ntitanic = sns.load_dataset(\"titanic\")\ntitanic.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\nWe’ll use the age and fair columns in this case to make our demonstrations either. Let’s plot the data and get a sense of how it looks\n\nsns.scatterplot(data=titanic, x=\"fare\", y=\"age\")\n\n&lt;Axes: xlabel='fare', ylabel='age'&gt;"
  },
  {
    "objectID": "posts/Anomaly Detection.html#missing-values",
    "href": "posts/Anomaly Detection.html#missing-values",
    "title": "Anomaly Detection",
    "section": "Missing values",
    "text": "Missing values\nLet’s check if there are any missing values in our dataset\n\ntitanic_age_fare = titanic[[\"fare\", \"age\"]]\ntitanic_age_fare.isnull().values.any()\n\nTrue\n\n\nLooks like we’ve missing values, let’s see how many in each feature\n\nprint(\"Missing fare values\", titanic_age_fare[\"fare\"].isnull().sum())\nprint(\"Missing age values\", titanic_age_fare[\"age\"].isnull().sum())\n\nMissing fare values 0\nMissing age values 177\n\n\nGiven that there is a high number of missing age values we can’t simpply drop these rows, so let’s fill them with the avarage age\n\ntitanic_age_fare[\"age\"].fillna((titanic_age_fare[\"age\"].mean()), inplace=True)\ntitanic_age_fare.isnull().values.any()\n\nFalse"
  },
  {
    "objectID": "posts/clustering.html",
    "href": "posts/clustering.html",
    "title": "Music Data Clustering",
    "section": "",
    "text": "Banner Image Credit: University of Rochester\nIn this post I’ll explore the K Means algorithm to cluster data fetched from Spotify API. This data contains the following attributes"
  },
  {
    "objectID": "posts/clustering.html#task",
    "href": "posts/clustering.html#task",
    "title": "Music Data Clustering",
    "section": "Task",
    "text": "Task\nThe task for this blog post is to train a couple of regression models and predict how popular a song is given the other features about the song.\n# Preprocessing ## Imports\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans"
  },
  {
    "objectID": "posts/clustering.html#load-the-data",
    "href": "posts/clustering.html#load-the-data",
    "title": "Music Data Clustering",
    "section": "Load the data",
    "text": "Load the data\n\nsongs = pd.read_csv(\"../data/spotify_songs.csv\")\nsongs.head()\n\n\n\n\n\n\n\n\ntrack_id\ntrack_name\ntrack_artist\ntrack_popularity\ntrack_album_id\ntrack_album_name\ntrack_album_release_date\nplaylist_name\nplaylist_id\nplaylist_genre\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\n\n\n\n\n0\n6f807x0ima9a1j3VPbc7VN\nI Don't Care (with Justin Bieber) - Loud Luxur...\nEd Sheeran\n66\n2oCs0DGTsRO98Gh5ZSl2Cx\nI Don't Care (with Justin Bieber) [Loud Luxury...\n2019-06-14\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n6\n-2.634\n1\n0.0583\n0.1020\n0.000000\n0.0653\n0.518\n122.036\n194754\n\n\n1\n0r7CVbZTWZgbTCYdfa2P31\nMemories - Dillon Francis Remix\nMaroon 5\n67\n63rPSO264uRjW1X5E6cWv6\nMemories (Dillon Francis Remix)\n2019-12-13\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n11\n-4.969\n1\n0.0373\n0.0724\n0.004210\n0.3570\n0.693\n99.972\n162600\n\n\n2\n1z1Hg7Vb0AhHDiEmnDE79l\nAll the Time - Don Diablo Remix\nZara Larsson\n70\n1HoSmj2eLcsrR0vE9gThr4\nAll the Time (Don Diablo Remix)\n2019-07-05\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n1\n-3.432\n0\n0.0742\n0.0794\n0.000023\n0.1100\n0.613\n124.008\n176616\n\n\n3\n75FpbthrwQmzHlBJLuGdC7\nCall You Mine - Keanu Silva Remix\nThe Chainsmokers\n60\n1nqYsOef1yKKuGOVchbsk6\nCall You Mine - The Remixes\n2019-07-19\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n7\n-3.778\n1\n0.1020\n0.0287\n0.000009\n0.2040\n0.277\n121.956\n169093\n\n\n4\n1e8PAfcKUYoKkxPhrHqw4x\nSomeone You Loved - Future Humans Remix\nLewis Capaldi\n69\n7m7vv9wlQ4i0LFuJiE2zsQ\nSomeone You Loved (Future Humans Remix)\n2019-03-05\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n1\n-4.672\n1\n0.0359\n0.0803\n0.000000\n0.0833\n0.725\n123.976\n189052\n\n\n\n\n5 rows × 23 columns"
  },
  {
    "objectID": "posts/clustering.html#remove-features",
    "href": "posts/clustering.html#remove-features",
    "title": "Music Data Clustering",
    "section": "Remove Features",
    "text": "Remove Features\n\nidentifiers\nFeatures like track_id, track_name, track_artist, track_album_id, track_album_name, playlist_id,playlist_name will have little contribution to our model as they are unique identifiers. Hence, we’ll remove them from the data.\n\nsongs = songs.drop(\n    [\n        \"track_id\",\n        \"track_name\",\n        \"track_artist\",\n        \"track_album_id\",\n        \"track_album_name\",\n        \"playlist_id\",\n        \"playlist_name\",\n    ],\n    axis=\"columns\",\n)\nsongs.head()\n\n\n\n\n\n\n\n\ntrack_popularity\ntrack_album_release_date\nplaylist_genre\nplaylist_subgenre\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\n\n\n\n\n0\n66\n2019-06-14\npop\ndance pop\n0.748\n0.916\n6\n-2.634\n1\n0.0583\n0.1020\n0.000000\n0.0653\n0.518\n122.036\n194754\n\n\n1\n67\n2019-12-13\npop\ndance pop\n0.726\n0.815\n11\n-4.969\n1\n0.0373\n0.0724\n0.004210\n0.3570\n0.693\n99.972\n162600\n\n\n2\n70\n2019-07-05\npop\ndance pop\n0.675\n0.931\n1\n-3.432\n0\n0.0742\n0.0794\n0.000023\n0.1100\n0.613\n124.008\n176616\n\n\n3\n60\n2019-07-19\npop\ndance pop\n0.718\n0.930\n7\n-3.778\n1\n0.1020\n0.0287\n0.000009\n0.2040\n0.277\n121.956\n169093\n\n\n4\n69\n2019-03-05\npop\ndance pop\n0.650\n0.833\n1\n-4.672\n1\n0.0359\n0.0803\n0.000000\n0.0833\n0.725\n123.976\n189052\n\n\n\n\n\n\n\n\n\nDate Data\nIn this post we’ll also drop date data we’d like to focus on music features itself\n\nsongs = songs.drop([\"track_album_release_date\"], axis=\"columns\")\n\n\n\nGenre\nWe’ll also drop the genre info to ensure that we have a clustering based off undeclared labels to make it truly unsupervised learning\n\ngenres = songs[[\"playlist_genre\", \"playlist_subgenre\"]]\nsongs = songs.drop([\"playlist_genre\", \"playlist_subgenre\"], axis=\"columns\")"
  },
  {
    "objectID": "posts/clustering.html#missing-values",
    "href": "posts/clustering.html#missing-values",
    "title": "Music Data Clustering",
    "section": "Missing values",
    "text": "Missing values\nLet’s check if there are any missing values in our dataset\n\nsongs.isnull().values.any()\n\nFalse\n\n\nWe don’t have any missing values, which is good. Let’s revisit what’s remaining\n\nsongs.head()\n\n\n\n\n\n\n\n\ntrack_popularity\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\n\n\n\n\n0\n66\n0.748\n0.916\n6\n-2.634\n1\n0.0583\n0.1020\n0.000000\n0.0653\n0.518\n122.036\n194754\n\n\n1\n67\n0.726\n0.815\n11\n-4.969\n1\n0.0373\n0.0724\n0.004210\n0.3570\n0.693\n99.972\n162600\n\n\n2\n70\n0.675\n0.931\n1\n-3.432\n0\n0.0742\n0.0794\n0.000023\n0.1100\n0.613\n124.008\n176616\n\n\n3\n60\n0.718\n0.930\n7\n-3.778\n1\n0.1020\n0.0287\n0.000009\n0.2040\n0.277\n121.956\n169093\n\n\n4\n69\n0.650\n0.833\n1\n-4.672\n1\n0.0359\n0.0803\n0.000000\n0.0833\n0.725\n123.976\n189052"
  },
  {
    "objectID": "posts/clustering.html#visualization",
    "href": "posts/clustering.html#visualization",
    "title": "Music Data Clustering",
    "section": "Visualization",
    "text": "Visualization\nLet’s visualize the data and get a better understanding of the values\n\nsns.pairplot(songs)\n\n\n\n\nThis plots can say a lot about the data, e.g., songs in the lower end of danceability spectrum are less likely to be popular. Aside from that, most of the features seem to have a fairly normal distribution on the data. ## Normalization Let’s scales data to a similar range to ensure consistency in impact of the features.\n\nsongs_normalized = preprocessing.normalize(songs)"
  },
  {
    "objectID": "posts/regression.html",
    "href": "posts/regression.html",
    "title": "Song popularity prediction",
    "section": "",
    "text": "In this blog post I’ll try to explore linear and nonlinear regression ML algorithms to predict popularity of a song based on a data set of almost 30,000 songs from Spotify API collected using the spotifyr package. The package and original data set can be found here.\n\n\nThis dataset encompasses various attributes providing diverse information about songs, artists, popularity, musical features, and playlist characteristics, making it suitable for music-related analyses or applications in data analysis and machine learning.\n\nTrack_id: Unique ID for a song\nTrack_name: Name of the song\nTrack_artist: Artist of the song\nTrack_popularity: Song popularity rating (0-100)\nTrack_album_id: Unique ID for the album\nTrack_album_name: Name of the album\nTrack_album_release_date: Release date of the album\nPlaylist_name: Name of the playlist\nPlaylist_id: ID of the playlist\nPlaylist_genre: Genre of the playlist\nPlaylist_subgenre: Subgenre of the playlist\nDanceability: Describes how suitable a track is for dancing (0.0 to 1.0)\nEnergy: Represents intensity and activity of the track (0.0 to 1.0)\nKey: Estimated overall key of the track\nLoudness: Overall loudness of a track in decibels (dB)\nMode: Indicates the modality (major or minor) of a track (0 for minor, 1 for major)\nSpeechiness: Detects the presence of spoken words in a track (0.0 to 1.0)\nAcousticness: Confidence measure of whether the track is acoustic (0.0 to 1.0)\nInstrumentalness: Predicts whether a track contains no vocals (0.0 to 1.0)\nLiveness: Detects the presence of an audience in the recording\nValence: Describes the musical positiveness conveyed by a track (0.0 to 1.0)\nTempo: Estimated tempo of a track in beats per minute (BPM)\nDuration_ms: Duration of the song in milliseconds\n\n\n\n\n\nThe task for this blog post is to train a couple of regression models and predict how popular a song is given the other features about the song.\n# Preprocessing ## Imports\n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import (\n    AdaBoostRegressor,\n    RandomForestRegressor,\n    GradientBoostingRegressor,\n    BaggingRegressor,\n    ExtraTreesRegressor,\n)\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neighbors import RadiusNeighborsRegressor\nfrom sklearn import svm\n\nfrom sklearn.metrics import mean_squared_error\n\n\n\n\n\nsongs = pd.read_csv(\"../data/spotify_songs.csv\")\nsongs.head()\n\n\n\n\n\n\n\n\ntrack_id\ntrack_name\ntrack_artist\ntrack_popularity\ntrack_album_id\ntrack_album_name\ntrack_album_release_date\nplaylist_name\nplaylist_id\nplaylist_genre\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\n\n\n\n\n0\n6f807x0ima9a1j3VPbc7VN\nI Don't Care (with Justin Bieber) - Loud Luxur...\nEd Sheeran\n66\n2oCs0DGTsRO98Gh5ZSl2Cx\nI Don't Care (with Justin Bieber) [Loud Luxury...\n2019-06-14\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n6\n-2.634\n1\n0.0583\n0.1020\n0.000000\n0.0653\n0.518\n122.036\n194754\n\n\n1\n0r7CVbZTWZgbTCYdfa2P31\nMemories - Dillon Francis Remix\nMaroon 5\n67\n63rPSO264uRjW1X5E6cWv6\nMemories (Dillon Francis Remix)\n2019-12-13\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n11\n-4.969\n1\n0.0373\n0.0724\n0.004210\n0.3570\n0.693\n99.972\n162600\n\n\n2\n1z1Hg7Vb0AhHDiEmnDE79l\nAll the Time - Don Diablo Remix\nZara Larsson\n70\n1HoSmj2eLcsrR0vE9gThr4\nAll the Time (Don Diablo Remix)\n2019-07-05\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n1\n-3.432\n0\n0.0742\n0.0794\n0.000023\n0.1100\n0.613\n124.008\n176616\n\n\n3\n75FpbthrwQmzHlBJLuGdC7\nCall You Mine - Keanu Silva Remix\nThe Chainsmokers\n60\n1nqYsOef1yKKuGOVchbsk6\nCall You Mine - The Remixes\n2019-07-19\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n7\n-3.778\n1\n0.1020\n0.0287\n0.000009\n0.2040\n0.277\n121.956\n169093\n\n\n4\n1e8PAfcKUYoKkxPhrHqw4x\nSomeone You Loved - Future Humans Remix\nLewis Capaldi\n69\n7m7vv9wlQ4i0LFuJiE2zsQ\nSomeone You Loved (Future Humans Remix)\n2019-03-05\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n1\n-4.672\n1\n0.0359\n0.0803\n0.000000\n0.0833\n0.725\n123.976\n189052\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n\n\nFeatures like track_id, track_name, track_artist, track_album_id, track_album_name, playlist_id,playlist_name will have little contribution to our model as they are unique identifiers. Hence we’ll remove them from the data.\n\nsongs = songs.drop(\n    [\n        \"track_id\",\n        \"track_name\",\n        \"track_artist\",\n        \"track_album_id\",\n        \"track_album_name\",\n        \"playlist_id\",\n        \"playlist_name\",\n    ],\n    axis=\"columns\",\n)\nsongs.head()\n\n\n\n\n\n\n\n\ntrack_popularity\ntrack_album_release_date\nplaylist_genre\nplaylist_subgenre\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\n\n\n\n\n0\n66\n2019-06-14\npop\ndance pop\n0.748\n0.916\n6\n-2.634\n1\n0.0583\n0.1020\n0.000000\n0.0653\n0.518\n122.036\n194754\n\n\n1\n67\n2019-12-13\npop\ndance pop\n0.726\n0.815\n11\n-4.969\n1\n0.0373\n0.0724\n0.004210\n0.3570\n0.693\n99.972\n162600\n\n\n2\n70\n2019-07-05\npop\ndance pop\n0.675\n0.931\n1\n-3.432\n0\n0.0742\n0.0794\n0.000023\n0.1100\n0.613\n124.008\n176616\n\n\n3\n60\n2019-07-19\npop\ndance pop\n0.718\n0.930\n7\n-3.778\n1\n0.1020\n0.0287\n0.000009\n0.2040\n0.277\n121.956\n169093\n\n\n4\n69\n2019-03-05\npop\ndance pop\n0.650\n0.833\n1\n-4.672\n1\n0.0359\n0.0803\n0.000000\n0.0833\n0.725\n123.976\n189052\n\n\n\n\n\n\n\n\n\n\nTo capture seasonal popularity, I’ll split the date value into three columns with year, month and day values. We first convert the string representation of track_album_release_date to date datatype and takeout the individual values separately.\n\nsongs[\"track_album_release_date\"] = pd.to_datetime(\n    songs[\"track_album_release_date\"], format=\"%Y-%m-%d\", errors=\"coerce\"\n)\n\nsongs[\"release_year\"] = songs[\"track_album_release_date\"].dt.year\nsongs[\"release_month\"] = songs[\"track_album_release_date\"].dt.month\nsongs[\"release_day\"] = songs[\"track_album_release_date\"].dt.day\nsongs = songs.drop([\"track_album_release_date\"], axis=\"columns\")\n\n\n\n\nLet’s check if there are any missing values in our dataset\n\nsongs.isnull().values.any()\n\nTrue\n\n\nIt seems we’ve some\n\nsongs.isnull().sum()\n\ntrack_popularity        0\nplaylist_genre          0\nplaylist_subgenre       0\ndanceability            0\nenergy                  0\nkey                     0\nloudness                0\nmode                    0\nspeechiness             0\nacousticness            0\ninstrumentalness        0\nliveness                0\nvalence                 0\ntempo                   0\nduration_ms             0\nrelease_year         1886\nrelease_month        1886\nrelease_day          1886\ndtype: int64\n\n\nGiven the missing date values are around 17% our data simply dropping the rows does not yield an ideal outcome. Hence, let’s to fill the missing columns with the most common value in that feature\n\nsongs[\"release_year\"].fillna(\n    songs[\"release_year\"].value_counts().index[0], inplace=True\n)\nsongs[\"release_month\"].fillna(\n    songs[\"release_month\"].value_counts().index[0], inplace=True\n)\nsongs[\"release_day\"].fillna(songs[\"release_day\"].value_counts().index[0], inplace=True)\nsongs\n\n\n\n\n\n\n\n\ntrack_popularity\nplaylist_genre\nplaylist_subgenre\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\nrelease_year\nrelease_month\nrelease_day\n\n\n\n\n0\n66\npop\ndance pop\n0.748\n0.916\n6\n-2.634\n1\n0.0583\n0.102000\n0.000000\n0.0653\n0.5180\n122.036\n194754\n2019.0\n6.0\n14.0\n\n\n1\n67\npop\ndance pop\n0.726\n0.815\n11\n-4.969\n1\n0.0373\n0.072400\n0.004210\n0.3570\n0.6930\n99.972\n162600\n2019.0\n12.0\n13.0\n\n\n2\n70\npop\ndance pop\n0.675\n0.931\n1\n-3.432\n0\n0.0742\n0.079400\n0.000023\n0.1100\n0.6130\n124.008\n176616\n2019.0\n7.0\n5.0\n\n\n3\n60\npop\ndance pop\n0.718\n0.930\n7\n-3.778\n1\n0.1020\n0.028700\n0.000009\n0.2040\n0.2770\n121.956\n169093\n2019.0\n7.0\n19.0\n\n\n4\n69\npop\ndance pop\n0.650\n0.833\n1\n-4.672\n1\n0.0359\n0.080300\n0.000000\n0.0833\n0.7250\n123.976\n189052\n2019.0\n3.0\n5.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32828\n42\nedm\nprogressive electro house\n0.428\n0.922\n2\n-1.814\n1\n0.0936\n0.076600\n0.000000\n0.0668\n0.2100\n128.170\n204375\n2014.0\n4.0\n28.0\n\n\n32829\n20\nedm\nprogressive electro house\n0.522\n0.786\n0\n-4.462\n1\n0.0420\n0.001710\n0.004270\n0.3750\n0.4000\n128.041\n353120\n2013.0\n3.0\n8.0\n\n\n32830\n14\nedm\nprogressive electro house\n0.529\n0.821\n6\n-4.899\n0\n0.0481\n0.108000\n0.000001\n0.1500\n0.4360\n127.989\n210112\n2014.0\n4.0\n21.0\n\n\n32831\n15\nedm\nprogressive electro house\n0.626\n0.888\n2\n-3.361\n1\n0.1090\n0.007920\n0.127000\n0.3430\n0.3080\n128.008\n367432\n2014.0\n1.0\n1.0\n\n\n32832\n27\nedm\nprogressive electro house\n0.603\n0.884\n5\n-4.571\n0\n0.0385\n0.000133\n0.341000\n0.7420\n0.0894\n127.984\n337500\n2014.0\n3.0\n3.0\n\n\n\n\n32833 rows × 18 columns\n\n\n\nIf we check once more, we see there are no missing values anymore\n\nsongs.isnull().values.any()\n\nFalse\n\n\n\n\n\nLet’s visualize the data and get a better understanding of the values ### Genre\n\nsns.countplot(songs, x=\"playlist_subgenre\", hue=\"playlist_genre\")\nplt.xticks(rotation=90)\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n        17, 18, 19, 20, 21, 22, 23]),\n [Text(0, 0, 'dance pop'),\n  Text(1, 0, 'post-teen pop'),\n  Text(2, 0, 'electropop'),\n  Text(3, 0, 'indie poptimism'),\n  Text(4, 0, 'hip hop'),\n  Text(5, 0, 'southern hip hop'),\n  Text(6, 0, 'gangster rap'),\n  Text(7, 0, 'trap'),\n  Text(8, 0, 'album rock'),\n  Text(9, 0, 'classic rock'),\n  Text(10, 0, 'permanent wave'),\n  Text(11, 0, 'hard rock'),\n  Text(12, 0, 'tropical'),\n  Text(13, 0, 'latin pop'),\n  Text(14, 0, 'reggaeton'),\n  Text(15, 0, 'latin hip hop'),\n  Text(16, 0, 'urban contemporary'),\n  Text(17, 0, 'hip pop'),\n  Text(18, 0, 'new jack swing'),\n  Text(19, 0, 'neo soul'),\n  Text(20, 0, 'electro house'),\n  Text(21, 0, 'big room'),\n  Text(22, 0, 'pop edm'),\n  Text(23, 0, 'progressive electro house')])\n\n\n\n\n\nFrom this we can see playlist_subgenre is full contained in playlist_genre hence we can remote the playlist_genre as the playlist_subgenre has more fine-grained information\n\nsongs = songs.drop(\"playlist_genre\", axis=\"columns\")\n\n\n\n\nNotice how we have a categorical feature (i.e., playlist_subgenre), we need to one-hot encode this column.\n\nplaylist_subgenre = pd.get_dummies(songs[\"playlist_subgenre\"])\nsongs = pd.concat([songs, playlist_subgenre], axis=\"columns\")\nsongs = songs.drop(\"playlist_subgenre\", axis=\"columns\")\nsongs\n\n\n\n\n\n\n\n\ntrack_popularity\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\n...\nnew jack swing\npermanent wave\npop edm\npost-teen pop\nprogressive electro house\nreggaeton\nsouthern hip hop\ntrap\ntropical\nurban contemporary\n\n\n\n\n0\n66\n0.748\n0.916\n6\n-2.634\n1\n0.0583\n0.102000\n0.000000\n0.0653\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n67\n0.726\n0.815\n11\n-4.969\n1\n0.0373\n0.072400\n0.004210\n0.3570\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n70\n0.675\n0.931\n1\n-3.432\n0\n0.0742\n0.079400\n0.000023\n0.1100\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n60\n0.718\n0.930\n7\n-3.778\n1\n0.1020\n0.028700\n0.000009\n0.2040\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n69\n0.650\n0.833\n1\n-4.672\n1\n0.0359\n0.080300\n0.000000\n0.0833\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32828\n42\n0.428\n0.922\n2\n-1.814\n1\n0.0936\n0.076600\n0.000000\n0.0668\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32829\n20\n0.522\n0.786\n0\n-4.462\n1\n0.0420\n0.001710\n0.004270\n0.3750\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32830\n14\n0.529\n0.821\n6\n-4.899\n0\n0.0481\n0.108000\n0.000001\n0.1500\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32831\n15\n0.626\n0.888\n2\n-3.361\n1\n0.1090\n0.007920\n0.127000\n0.3430\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32832\n27\n0.603\n0.884\n5\n-4.571\n0\n0.0385\n0.000133\n0.341000\n0.7420\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n32833 rows × 40 columns\n\n\n\n\n\n\ncor = songs.corr()\nsns.heatmap(cor)\ncor = cor.abs()\n\n\n\n\nLet’s see the top 5 correclations\n\ncor_values = (\n    cor.where(np.triu(np.ones(cor.shape), k=1).astype(bool))\n    .stack()\n    .sort_values(ascending=False)\n)\ntop_n = 5\nprint(\"Top 5 correlation values are\")\nfor i, v in cor_values[:top_n].items():\n    print(\"{} - {} \".format(i, v))\n\nTop 5 correlation values are\n('energy', 'loudness') - 0.6766245234442312 \n('energy', 'acousticness') - 0.5397446301909388 \n('loudness', 'acousticness') - 0.36163816507093 \n('danceability', 'valence') - 0.3305232570910807 \n('release_month', 'release_day') - 0.3080926045288983 \n\n\nAs expected ‘energy’, ‘loudness’ are correlated to a degree but none of these correlation values are significantly high enough to warrant removal before the training so we keep all our features.\n\n\n\n\nLet’s now split features into X and Y with the latter being our target for prediction\n\ny = songs[\"track_popularity\"].to_numpy()\nx = songs.drop(\"track_popularity\", axis=\"columns\").to_numpy()\n\n\n\n\nNormalize the features to bring them all to 0-1 range\n\nx_normalized = preprocessing.normalize(x)\n\n\n\n\nLet’s split our data into training and test sets with 80 to 20 ratio.\n\nx_train, x_test, y_train, y_test = train_test_split(x_normalized, y, test_size=0.20)\nprint(\"Training X Shape: {}\".format(x_train.shape))\nprint(\"Training Y Shape: {}\".format(y_train.shape))\nprint(\"Test X Shape: {}\".format(x_test.shape))\nprint(\"Test y Shape: {}\".format(y_test.shape))\n\nTraining X Shape: (26266, 39)\nTraining Y Shape: (26266,)\nTest X Shape: (6567, 39)\nTest y Shape: (6567,)"
  },
  {
    "objectID": "posts/regression.html#dataset",
    "href": "posts/regression.html#dataset",
    "title": "Song popularity prediction",
    "section": "",
    "text": "In this blog post I’ll try to explore linear and nonlinear regression ML algorithms to predict popularity of a song based on a data set of almost 30,000 songs from Spotify API collected using the spotifyr package. The package and original data set can be found here.\n\n\nThis dataset encompasses various attributes providing diverse information about songs, artists, popularity, musical features, and playlist characteristics, making it suitable for music-related analyses or applications in data analysis and machine learning.\n\nTrack_id: Unique ID for a song\nTrack_name: Name of the song\nTrack_artist: Artist of the song\nTrack_popularity: Song popularity rating (0-100)\nTrack_album_id: Unique ID for the album\nTrack_album_name: Name of the album\nTrack_album_release_date: Release date of the album\nPlaylist_name: Name of the playlist\nPlaylist_id: ID of the playlist\nPlaylist_genre: Genre of the playlist\nPlaylist_subgenre: Subgenre of the playlist\nDanceability: Describes how suitable a track is for dancing (0.0 to 1.0)\nEnergy: Represents intensity and activity of the track (0.0 to 1.0)\nKey: Estimated overall key of the track\nLoudness: Overall loudness of a track in decibels (dB)\nMode: Indicates the modality (major or minor) of a track (0 for minor, 1 for major)\nSpeechiness: Detects the presence of spoken words in a track (0.0 to 1.0)\nAcousticness: Confidence measure of whether the track is acoustic (0.0 to 1.0)\nInstrumentalness: Predicts whether a track contains no vocals (0.0 to 1.0)\nLiveness: Detects the presence of an audience in the recording\nValence: Describes the musical positiveness conveyed by a track (0.0 to 1.0)\nTempo: Estimated tempo of a track in beats per minute (BPM)\nDuration_ms: Duration of the song in milliseconds"
  },
  {
    "objectID": "posts/regression.html#task",
    "href": "posts/regression.html#task",
    "title": "Song popularity prediction",
    "section": "",
    "text": "The task for this blog post is to train a couple of regression models and predict how popular a song is given the other features about the song.\n# Preprocessing ## Imports\n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import (\n    AdaBoostRegressor,\n    RandomForestRegressor,\n    GradientBoostingRegressor,\n    BaggingRegressor,\n    ExtraTreesRegressor,\n)\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neighbors import RadiusNeighborsRegressor\nfrom sklearn import svm\n\nfrom sklearn.metrics import mean_squared_error"
  },
  {
    "objectID": "posts/regression.html#load-the-data",
    "href": "posts/regression.html#load-the-data",
    "title": "Song popularity prediction",
    "section": "",
    "text": "songs = pd.read_csv(\"../data/spotify_songs.csv\")\nsongs.head()\n\n\n\n\n\n\n\n\ntrack_id\ntrack_name\ntrack_artist\ntrack_popularity\ntrack_album_id\ntrack_album_name\ntrack_album_release_date\nplaylist_name\nplaylist_id\nplaylist_genre\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\n\n\n\n\n0\n6f807x0ima9a1j3VPbc7VN\nI Don't Care (with Justin Bieber) - Loud Luxur...\nEd Sheeran\n66\n2oCs0DGTsRO98Gh5ZSl2Cx\nI Don't Care (with Justin Bieber) [Loud Luxury...\n2019-06-14\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n6\n-2.634\n1\n0.0583\n0.1020\n0.000000\n0.0653\n0.518\n122.036\n194754\n\n\n1\n0r7CVbZTWZgbTCYdfa2P31\nMemories - Dillon Francis Remix\nMaroon 5\n67\n63rPSO264uRjW1X5E6cWv6\nMemories (Dillon Francis Remix)\n2019-12-13\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n11\n-4.969\n1\n0.0373\n0.0724\n0.004210\n0.3570\n0.693\n99.972\n162600\n\n\n2\n1z1Hg7Vb0AhHDiEmnDE79l\nAll the Time - Don Diablo Remix\nZara Larsson\n70\n1HoSmj2eLcsrR0vE9gThr4\nAll the Time (Don Diablo Remix)\n2019-07-05\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n1\n-3.432\n0\n0.0742\n0.0794\n0.000023\n0.1100\n0.613\n124.008\n176616\n\n\n3\n75FpbthrwQmzHlBJLuGdC7\nCall You Mine - Keanu Silva Remix\nThe Chainsmokers\n60\n1nqYsOef1yKKuGOVchbsk6\nCall You Mine - The Remixes\n2019-07-19\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n7\n-3.778\n1\n0.1020\n0.0287\n0.000009\n0.2040\n0.277\n121.956\n169093\n\n\n4\n1e8PAfcKUYoKkxPhrHqw4x\nSomeone You Loved - Future Humans Remix\nLewis Capaldi\n69\n7m7vv9wlQ4i0LFuJiE2zsQ\nSomeone You Loved (Future Humans Remix)\n2019-03-05\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n1\n-4.672\n1\n0.0359\n0.0803\n0.000000\n0.0833\n0.725\n123.976\n189052\n\n\n\n\n5 rows × 23 columns"
  },
  {
    "objectID": "posts/regression.html#remove-identifiers",
    "href": "posts/regression.html#remove-identifiers",
    "title": "Song popularity prediction",
    "section": "",
    "text": "Features like track_id, track_name, track_artist, track_album_id, track_album_name, playlist_id,playlist_name will have little contribution to our model as they are unique identifiers. Hence we’ll remove them from the data.\n\nsongs = songs.drop(\n    [\n        \"track_id\",\n        \"track_name\",\n        \"track_artist\",\n        \"track_album_id\",\n        \"track_album_name\",\n        \"playlist_id\",\n        \"playlist_name\",\n    ],\n    axis=\"columns\",\n)\nsongs.head()\n\n\n\n\n\n\n\n\ntrack_popularity\ntrack_album_release_date\nplaylist_genre\nplaylist_subgenre\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\n\n\n\n\n0\n66\n2019-06-14\npop\ndance pop\n0.748\n0.916\n6\n-2.634\n1\n0.0583\n0.1020\n0.000000\n0.0653\n0.518\n122.036\n194754\n\n\n1\n67\n2019-12-13\npop\ndance pop\n0.726\n0.815\n11\n-4.969\n1\n0.0373\n0.0724\n0.004210\n0.3570\n0.693\n99.972\n162600\n\n\n2\n70\n2019-07-05\npop\ndance pop\n0.675\n0.931\n1\n-3.432\n0\n0.0742\n0.0794\n0.000023\n0.1100\n0.613\n124.008\n176616\n\n\n3\n60\n2019-07-19\npop\ndance pop\n0.718\n0.930\n7\n-3.778\n1\n0.1020\n0.0287\n0.000009\n0.2040\n0.277\n121.956\n169093\n\n\n4\n69\n2019-03-05\npop\ndance pop\n0.650\n0.833\n1\n-4.672\n1\n0.0359\n0.0803\n0.000000\n0.0833\n0.725\n123.976\n189052"
  },
  {
    "objectID": "posts/regression.html#date-data",
    "href": "posts/regression.html#date-data",
    "title": "Song popularity prediction",
    "section": "",
    "text": "To capture seasonal popularity, I’ll split the date value into three columns with year, month and day values. We first convert the string representation of track_album_release_date to date datatype and takeout the individual values separately.\n\nsongs[\"track_album_release_date\"] = pd.to_datetime(\n    songs[\"track_album_release_date\"], format=\"%Y-%m-%d\", errors=\"coerce\"\n)\n\nsongs[\"release_year\"] = songs[\"track_album_release_date\"].dt.year\nsongs[\"release_month\"] = songs[\"track_album_release_date\"].dt.month\nsongs[\"release_day\"] = songs[\"track_album_release_date\"].dt.day\nsongs = songs.drop([\"track_album_release_date\"], axis=\"columns\")"
  },
  {
    "objectID": "posts/regression.html#missing-values",
    "href": "posts/regression.html#missing-values",
    "title": "Song popularity prediction",
    "section": "",
    "text": "Let’s check if there are any missing values in our dataset\n\nsongs.isnull().values.any()\n\nTrue\n\n\nIt seems we’ve some\n\nsongs.isnull().sum()\n\ntrack_popularity        0\nplaylist_genre          0\nplaylist_subgenre       0\ndanceability            0\nenergy                  0\nkey                     0\nloudness                0\nmode                    0\nspeechiness             0\nacousticness            0\ninstrumentalness        0\nliveness                0\nvalence                 0\ntempo                   0\nduration_ms             0\nrelease_year         1886\nrelease_month        1886\nrelease_day          1886\ndtype: int64\n\n\nGiven the missing date values are around 17% our data simply dropping the rows does not yield an ideal outcome. Hence, let’s to fill the missing columns with the most common value in that feature\n\nsongs[\"release_year\"].fillna(\n    songs[\"release_year\"].value_counts().index[0], inplace=True\n)\nsongs[\"release_month\"].fillna(\n    songs[\"release_month\"].value_counts().index[0], inplace=True\n)\nsongs[\"release_day\"].fillna(songs[\"release_day\"].value_counts().index[0], inplace=True)\nsongs\n\n\n\n\n\n\n\n\ntrack_popularity\nplaylist_genre\nplaylist_subgenre\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\nrelease_year\nrelease_month\nrelease_day\n\n\n\n\n0\n66\npop\ndance pop\n0.748\n0.916\n6\n-2.634\n1\n0.0583\n0.102000\n0.000000\n0.0653\n0.5180\n122.036\n194754\n2019.0\n6.0\n14.0\n\n\n1\n67\npop\ndance pop\n0.726\n0.815\n11\n-4.969\n1\n0.0373\n0.072400\n0.004210\n0.3570\n0.6930\n99.972\n162600\n2019.0\n12.0\n13.0\n\n\n2\n70\npop\ndance pop\n0.675\n0.931\n1\n-3.432\n0\n0.0742\n0.079400\n0.000023\n0.1100\n0.6130\n124.008\n176616\n2019.0\n7.0\n5.0\n\n\n3\n60\npop\ndance pop\n0.718\n0.930\n7\n-3.778\n1\n0.1020\n0.028700\n0.000009\n0.2040\n0.2770\n121.956\n169093\n2019.0\n7.0\n19.0\n\n\n4\n69\npop\ndance pop\n0.650\n0.833\n1\n-4.672\n1\n0.0359\n0.080300\n0.000000\n0.0833\n0.7250\n123.976\n189052\n2019.0\n3.0\n5.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32828\n42\nedm\nprogressive electro house\n0.428\n0.922\n2\n-1.814\n1\n0.0936\n0.076600\n0.000000\n0.0668\n0.2100\n128.170\n204375\n2014.0\n4.0\n28.0\n\n\n32829\n20\nedm\nprogressive electro house\n0.522\n0.786\n0\n-4.462\n1\n0.0420\n0.001710\n0.004270\n0.3750\n0.4000\n128.041\n353120\n2013.0\n3.0\n8.0\n\n\n32830\n14\nedm\nprogressive electro house\n0.529\n0.821\n6\n-4.899\n0\n0.0481\n0.108000\n0.000001\n0.1500\n0.4360\n127.989\n210112\n2014.0\n4.0\n21.0\n\n\n32831\n15\nedm\nprogressive electro house\n0.626\n0.888\n2\n-3.361\n1\n0.1090\n0.007920\n0.127000\n0.3430\n0.3080\n128.008\n367432\n2014.0\n1.0\n1.0\n\n\n32832\n27\nedm\nprogressive electro house\n0.603\n0.884\n5\n-4.571\n0\n0.0385\n0.000133\n0.341000\n0.7420\n0.0894\n127.984\n337500\n2014.0\n3.0\n3.0\n\n\n\n\n32833 rows × 18 columns\n\n\n\nIf we check once more, we see there are no missing values anymore\n\nsongs.isnull().values.any()\n\nFalse"
  },
  {
    "objectID": "posts/regression.html#visualization",
    "href": "posts/regression.html#visualization",
    "title": "Song popularity prediction",
    "section": "",
    "text": "Let’s visualize the data and get a better understanding of the values ### Genre\n\nsns.countplot(songs, x=\"playlist_subgenre\", hue=\"playlist_genre\")\nplt.xticks(rotation=90)\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n        17, 18, 19, 20, 21, 22, 23]),\n [Text(0, 0, 'dance pop'),\n  Text(1, 0, 'post-teen pop'),\n  Text(2, 0, 'electropop'),\n  Text(3, 0, 'indie poptimism'),\n  Text(4, 0, 'hip hop'),\n  Text(5, 0, 'southern hip hop'),\n  Text(6, 0, 'gangster rap'),\n  Text(7, 0, 'trap'),\n  Text(8, 0, 'album rock'),\n  Text(9, 0, 'classic rock'),\n  Text(10, 0, 'permanent wave'),\n  Text(11, 0, 'hard rock'),\n  Text(12, 0, 'tropical'),\n  Text(13, 0, 'latin pop'),\n  Text(14, 0, 'reggaeton'),\n  Text(15, 0, 'latin hip hop'),\n  Text(16, 0, 'urban contemporary'),\n  Text(17, 0, 'hip pop'),\n  Text(18, 0, 'new jack swing'),\n  Text(19, 0, 'neo soul'),\n  Text(20, 0, 'electro house'),\n  Text(21, 0, 'big room'),\n  Text(22, 0, 'pop edm'),\n  Text(23, 0, 'progressive electro house')])\n\n\n\n\n\nFrom this we can see playlist_subgenre is full contained in playlist_genre hence we can remote the playlist_genre as the playlist_subgenre has more fine-grained information\n\nsongs = songs.drop(\"playlist_genre\", axis=\"columns\")"
  },
  {
    "objectID": "posts/regression.html#one-hot-encoding",
    "href": "posts/regression.html#one-hot-encoding",
    "title": "Song popularity prediction",
    "section": "",
    "text": "Notice how we have a categorical feature (i.e., playlist_subgenre), we need to one-hot encode this column.\n\nplaylist_subgenre = pd.get_dummies(songs[\"playlist_subgenre\"])\nsongs = pd.concat([songs, playlist_subgenre], axis=\"columns\")\nsongs = songs.drop(\"playlist_subgenre\", axis=\"columns\")\nsongs\n\n\n\n\n\n\n\n\ntrack_popularity\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\n...\nnew jack swing\npermanent wave\npop edm\npost-teen pop\nprogressive electro house\nreggaeton\nsouthern hip hop\ntrap\ntropical\nurban contemporary\n\n\n\n\n0\n66\n0.748\n0.916\n6\n-2.634\n1\n0.0583\n0.102000\n0.000000\n0.0653\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n67\n0.726\n0.815\n11\n-4.969\n1\n0.0373\n0.072400\n0.004210\n0.3570\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n70\n0.675\n0.931\n1\n-3.432\n0\n0.0742\n0.079400\n0.000023\n0.1100\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n60\n0.718\n0.930\n7\n-3.778\n1\n0.1020\n0.028700\n0.000009\n0.2040\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n69\n0.650\n0.833\n1\n-4.672\n1\n0.0359\n0.080300\n0.000000\n0.0833\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32828\n42\n0.428\n0.922\n2\n-1.814\n1\n0.0936\n0.076600\n0.000000\n0.0668\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32829\n20\n0.522\n0.786\n0\n-4.462\n1\n0.0420\n0.001710\n0.004270\n0.3750\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32830\n14\n0.529\n0.821\n6\n-4.899\n0\n0.0481\n0.108000\n0.000001\n0.1500\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32831\n15\n0.626\n0.888\n2\n-3.361\n1\n0.1090\n0.007920\n0.127000\n0.3430\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32832\n27\n0.603\n0.884\n5\n-4.571\n0\n0.0385\n0.000133\n0.341000\n0.7420\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n32833 rows × 40 columns\n\n\n\n\n\n\ncor = songs.corr()\nsns.heatmap(cor)\ncor = cor.abs()\n\n\n\n\nLet’s see the top 5 correclations\n\ncor_values = (\n    cor.where(np.triu(np.ones(cor.shape), k=1).astype(bool))\n    .stack()\n    .sort_values(ascending=False)\n)\ntop_n = 5\nprint(\"Top 5 correlation values are\")\nfor i, v in cor_values[:top_n].items():\n    print(\"{} - {} \".format(i, v))\n\nTop 5 correlation values are\n('energy', 'loudness') - 0.6766245234442312 \n('energy', 'acousticness') - 0.5397446301909388 \n('loudness', 'acousticness') - 0.36163816507093 \n('danceability', 'valence') - 0.3305232570910807 \n('release_month', 'release_day') - 0.3080926045288983 \n\n\nAs expected ‘energy’, ‘loudness’ are correlated to a degree but none of these correlation values are significantly high enough to warrant removal before the training so we keep all our features."
  },
  {
    "objectID": "posts/regression.html#target-and-data",
    "href": "posts/regression.html#target-and-data",
    "title": "Song popularity prediction",
    "section": "",
    "text": "Let’s now split features into X and Y with the latter being our target for prediction\n\ny = songs[\"track_popularity\"].to_numpy()\nx = songs.drop(\"track_popularity\", axis=\"columns\").to_numpy()"
  },
  {
    "objectID": "posts/regression.html#normalization",
    "href": "posts/regression.html#normalization",
    "title": "Song popularity prediction",
    "section": "",
    "text": "Normalize the features to bring them all to 0-1 range\n\nx_normalized = preprocessing.normalize(x)"
  },
  {
    "objectID": "posts/regression.html#training-and-test-split-and-data",
    "href": "posts/regression.html#training-and-test-split-and-data",
    "title": "Song popularity prediction",
    "section": "",
    "text": "Let’s split our data into training and test sets with 80 to 20 ratio.\n\nx_train, x_test, y_train, y_test = train_test_split(x_normalized, y, test_size=0.20)\nprint(\"Training X Shape: {}\".format(x_train.shape))\nprint(\"Training Y Shape: {}\".format(y_train.shape))\nprint(\"Test X Shape: {}\".format(x_test.shape))\nprint(\"Test y Shape: {}\".format(y_test.shape))\n\nTraining X Shape: (26266, 39)\nTraining Y Shape: (26266,)\nTest X Shape: (6567, 39)\nTest y Shape: (6567,)"
  },
  {
    "objectID": "posts/regression.html#regression-models",
    "href": "posts/regression.html#regression-models",
    "title": "Song popularity prediction",
    "section": "Regression models",
    "text": "Regression models\nWe will try to train on multiple algorithms and perform hyperparameter tuning on each to find the best performing model\n\nmodels = {\n    \"ExtraTreesRegressor\": {\n        \"model\": ExtraTreesRegressor(),\n        \"params\": {\n            \"n_estimators\": [25, 50, 75, 100],\n            \"max_depth\": [5, 10, 20],\n            \"min_samples_split\": [3, 5, 7],\n            \"min_samples_leaf\": [3, 4, 5],\n        },\n    },\n    \"adaboost\": {\n        \"model\": AdaBoostRegressor(),\n        \"params\": {\n            \"n_estimators\": [25, 50, 75, 100],\n            \"loss\": [\"linear\", \"square\", \"exponential\"],\n        },\n    },\n    \"svm\": {\n        \"model\": svm.SVR(),\n        \"params\": {\n            \"degree\": [2, 4, 6],\n            \"C\": [0.5, 1.0, 1.5],\n        },\n    },\n    \"knr\": {\n        \"model\": KNeighborsRegressor(),\n        \"params\": {\n            \"n_neighbors\": [3, 5, 8],\n            \"weights\": [\"uniform\", \"distance\"],\n            \"leaf_size\": [20, 40, 60],\n        },\n    },\n    \"rnr\": {\n        \"model\": RadiusNeighborsRegressor(),\n        \"params\": {\n            \"radius\": [0.5, 1.0, 1.5],\n            \"weights\": [\"uniform\", \"distance\"],\n            \"leaf_size\": [20, 40, 60],\n        },\n    },\n    \"RandomForestRegressor\": {\n        \"model\": RandomForestRegressor(),\n        \"params\": {\n            \"n_estimators\": [25, 50, 75, 100],\n            \"max_depth\": [5, 10, 20],\n            \"min_samples_split\": [3, 5, 7],\n            \"min_samples_leaf\": [3, 4, 5],\n        },\n    },\n    \"boosting\": {\n        \"model\": GradientBoostingRegressor(),\n        \"params\": {\n            \"n_estimators\": [25, 50, 75, 100],\n            \"loss\": [\"linear\", \"square\", \"exponential\"],\n            \"min_samples_split\": [3, 5, 7],\n        },\n    },\n}\n\nNow let’s perform the training and hyperparameter tuning. Start by preparing a variable to store the best model and it’s param values in\n\nbest_model, best_params, best_performance = None, None, float(\"inf\")\n\nLoop through each model and perform randomized search. Note: Running the following code takes a lot of time so what I did was run it on google colab and bring the results here.\nfor name, info in models.items():\n    random_search = RandomizedSearchCV(\n        info[\"model\"],\n        info[\"params\"],\n        scoring=\"neg_mean_squared_error\",\n        cv=5,\n        n_jobs=-1,\n    )\n\n    random_search.fit(x_train, y_train)\n\n    # Get the best model and its parameters\n    winning_model = random_search.best_estimator_\n    winning_model_params = random_search.best_params_\n    # Check the performance of the best model using the test data.\n    # We need to round the predicted values because the target feature is integer\n    y_pred = np.round(winning_model.predict(x_test))\n    error = mean_squared_error(y_test, y_pred)\n    # Update the best_model_info if the current model has lower MSE\n    if error &lt; best_performance:\n        best_performance = error\n        best_model = winning_model\n        best_params = winning_model_params\n\n# Print the best model information\nprint(f\"Best Model: {best_model}\")\nprint(f\"Best Parameters: {best_params}\")"
  },
  {
    "objectID": "posts/regression.html#other-sample-runs-are-as-follows",
    "href": "posts/regression.html#other-sample-runs-are-as-follows",
    "title": "Song popularity prediction",
    "section": "Other sample runs are as follows",
    "text": "Other sample runs are as follows\n\nKNeighborsRegressor\n\nModel: KNeighborsRegressor(leaf_size=20, n_neighbors=8)\nParameters: {‘n_neighbors’: 8, ‘leaf_size’: 20}\nMean Squared Error: 619.6920968478757\n\n\n\nRadiusNeighborsRegressor\n\nModel: RadiusNeighborsRegressor(leaf_size=40, radius=0.5, weights=‘distance’)\nParameters: {‘weights’: ‘distance’, ‘radius’: 0.5, ‘leaf_size’: 40}\nMean Squared Error: 615.0418760469012"
  },
  {
    "objectID": "posts/seaborn_demo.html",
    "href": "posts/seaborn_demo.html",
    "title": "Seaborn Examples",
    "section": "",
    "text": "Seaborn python package build on top of matplotlib for plotting statistical data. It has great support for the commonly used pandas data structure in handling tabular data. It’s more opinionated than matplotlib which makes it easier to get started with and provides higher level API for plotting which will help us create plots with fewer number of lines and configuration that we need in matplotlib for the same graphics.\n\nVisualizing data using tools like seaborn help us explore and understand the data. In this post we’ll explore some of the available graphics demonstrate how we can use to plot different data types we might have.\n\nTo help with the clarity of this post, I’ve organized it into six sections. Following this introductory section, we’ll see the prerequisite setup and imports. In the setup section I’ll discuss the sample datasets that come with seaborn. The third and fourth sections are about seaborn plots for Categorical and Continues Data, respectively. Then I’ll discuss how we can plot comparative plots followed by a section on utilities such as styling and saving plots."
  },
  {
    "objectID": "posts/seaborn_demo.html#setup",
    "href": "posts/seaborn_demo.html#setup",
    "title": "Seaborn Examples",
    "section": "Setup",
    "text": "Setup\n\nInstallation\nFirst step is t install the seaborn package. For that, we can use pip as\n\n%pip install seaborn\n\nRequirement already satisfied: seaborn in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (0.13.0)\nRequirement already satisfied: numpy!=1.24.0,&gt;=1.20 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from seaborn) (1.23.5)\nRequirement already satisfied: pandas&gt;=1.2 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from seaborn) (2.0.3)\nRequirement already satisfied: matplotlib!=3.6.1,&gt;=3.3 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from seaborn) (3.7.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.0.5)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (4.25.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.4.4)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (23.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (9.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2022.7)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nImports\nTo get started with seaborn we need to import the following packages. We import pyplot form matplotlib to use the lower level APIs later for styling and customization. While working with Jupyter notebooks we might want to suppers warnings in the output.\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\n\n\nJupyter Configurations\nAsk Jupyter to display plots within the notebook\n\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\n\nSuppress warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\nData\nPlotting only makes sense if we have some data to visualize. We can plot any data stored in pandas data structure using seaborn. This let’s us load and process data using pandas and use seaborn for visualization in parallel. For this post, I’ll make use of sample datasets shipped with seaborn. To get the full list of datasets available in our installation we can as seaborn itself\n\nprint(sns.get_dataset_names())\n\n['anagrams', 'anscombe', 'attention', 'brain_networks', 'car_crashes', 'diamonds', 'dots', 'dowjones', 'exercise', 'flights', 'fmri', 'geyser', 'glue', 'healthexp', 'iris', 'mpg', 'penguins', 'planets', 'seaice', 'taxis', 'tips', 'titanic']\n\n\nLet’s load two datasets and examine their content. More information on the datasets can be found here and here\n\ncrashes = sns.load_dataset('car_crashes')\ntitanic = sns.load_dataset('titanic')\n\nExamine the content of the data\n\ncrashes.head()\n\n\n\n\n\n\n\n\ntotal\nspeeding\nalcohol\nnot_distracted\nno_previous\nins_premium\nins_losses\nabbrev\n\n\n\n\n0\n18.8\n7.332\n5.640\n18.048\n15.040\n784.55\n145.08\nAL\n\n\n1\n18.1\n7.421\n4.525\n16.290\n17.014\n1053.48\n133.93\nAK\n\n\n2\n18.6\n6.510\n5.208\n15.624\n17.856\n899.47\n110.35\nAZ\n\n\n3\n22.4\n4.032\n5.824\n21.056\n21.280\n827.34\n142.39\nAR\n\n\n4\n12.0\n4.200\n3.360\n10.920\n10.680\n878.41\n165.63\nCA\n\n\n\n\n\n\n\n\ntitanic.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue"
  },
  {
    "objectID": "posts/seaborn_demo.html#categorical-data",
    "href": "posts/seaborn_demo.html#categorical-data",
    "title": "Seaborn Examples",
    "section": "Categorical Data",
    "text": "Categorical Data\nCategorical data is form of qualitative data that can be stored and identified based on distinct labels. Instead of being measured numerically, it is a type of information that can be grouped into categories. For example, the sex column in our titanic dataset has two labels: male and female. Similarly the abbrev column is a list of state name abbreviations in the US which has a total of 50 labels.  In this section, we will see bar and count plots from seaborn as two graphics to visualize categorical data.\n\nIn this section, we will see bar and count plots from seaborn as two graphics to visualize categorical data.\n\nBar Plots\nAlso referred to as bar chart, bar plot is, a bar plot allows us to visualize the comparisons between the discrete labels or categories in our data. Bar chart is a graph that represents the category of data with horizontally or vertically rectangular bars with lengths and heights that is proportional to the values which they represent. One of the axis of the plot represents the specific categories being compared, while the other axis represents the measured values corresponding to those categories.\n\nsns.barplot(data = titanic, x = 'sex', y='fare')\n\n&lt;Axes: xlabel='sex', ylabel='fare'&gt;\n\n\n\n\n\nBy Default the data is aggregated by mean of y\n\nsns.barplot(data = titanic, x = 'sex', y='fare', estimator=np.median)\n\n&lt;Axes: xlabel='sex', ylabel='fare'&gt;\n\n\n\n\n\n\n\nCount Plots\nSimilar to bar plot but uses count as the estimator\n\nsns.countplot(data = titanic, x = 'sex')\n\n&lt;Axes: xlabel='sex', ylabel='count'&gt;\n\n\n\n\n\nWe can display a count of the number rows in each category of values in the alive column\n\nsns.countplot(data = titanic, x = 'alive')\n\n&lt;Axes: xlabel='alive', ylabel='count'&gt;"
  },
  {
    "objectID": "posts/seaborn_demo.html#distribution-plots",
    "href": "posts/seaborn_demo.html#distribution-plots",
    "title": "Seaborn Examples",
    "section": "Distribution Plots",
    "text": "Distribution Plots\nDistribution plots for continuous data variables.In the past, Seaborn had a distplot method which supported displaying a histogram plot by with kde on top default. distplot is deprecated and it is recommended we use displot or histplot for find grained control. distplot allow as to display a histogram of univariate or bivariate distribution of the data in a dataset.\n\nsns.distplot(a=crashes['alcohol'])\n \n\n&lt;Axes: xlabel='alcohol', ylabel='Density'&gt;\n\n\n\n\n\nIf we don’t what the kde plot to be visible we can tell seaborn not to show it\n\nsns.distplot(a=crashes['alcohol'],kde=False)\n\n&lt;Axes: xlabel='alcohol'&gt;\n\n\n\n\n\n\nsns.displot(data=crashes['alcohol'])\n\n\n\n\nThe equivalent plot can be displayed using the new displot method\n\n# We can specify name of the data column in the dataset if there are more than one\nsns.displot(data=crashes, x='alcohol')\n\n\n\n\ndisplot combines a histogram with optional components, such as a Kernel Density Estimation (KDE) line or rug plot. We can specify which type we want to plot using the kind key (default is hist)\n\nsns.displot(data=crashes['alcohol'], kind='kde')\n\n\n\n\nWe can enable an overlay of other visualization on top of the default. We can do this by passing a boolean value for the parameters hist, ecdf, kde, rug\n\nsns.displot(data=crashes['alcohol'], kde=True, rug=True)\n\n\n\n\n\nsns.displot(data=titanic, x='age', hue='sex', kind='kde', multiple='stack')\n\n\n\n\n\nKDE Plot\nWe can display kde plots using the kdeplot function as well\n\nsns.kdeplot(data=titanic, x='fare')\n\n&lt;Axes: xlabel='fare', ylabel='Density'&gt;"
  },
  {
    "objectID": "posts/seaborn_demo.html#categorical-plots",
    "href": "posts/seaborn_demo.html#categorical-plots",
    "title": "Seaborn Examples",
    "section": "Categorical Plots",
    "text": "Categorical Plots\n\nBox Plot\nCompare different variables\n\nsns.boxplot(data = titanic, x = 'alive', y='fare', hue='sex')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\n\nViolin Plot\nCompare different variables in a different visualization\n\nsns.violinplot(data = titanic, x = 'alive', y='fare', hue='sex')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.violinplot(data = titanic, x = 'alive', y='fare', hue='sex', split=True)\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\n#survived   pclass  sex age sibsp   parch   fare    embarked    class   who adult_male  deck    embark_town alive   alone\nsns.violinplot(data = titanic, x = 'alive', y='fare')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\n\nStrip Plot\n\nsns.stripplot(data = titanic, x = 'class', y='fare')\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.stripplot(data = titanic, x = 'class', y='fare', hue='sex')\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\n\nSwarm Plot\n\nsns.swarmplot(data = titanic, x = 'alive', y='fare')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;"
  },
  {
    "objectID": "posts/seaborn_demo.html#comparing-data",
    "href": "posts/seaborn_demo.html#comparing-data",
    "title": "Seaborn Examples",
    "section": "Comparing Data",
    "text": "Comparing Data\n\nsns.displot(data=titanic, x='age', col='survived',  kind='kde')\n\n\n\n\n\nsns.displot(data=titanic, x='age', col='survived', hue='sex', kind='kde', multiple='stack')\n\n\n\n\n\nJoint Plot\nUsed for comparing two distributions. By default it uses scatter plot\n\nsns.jointplot(data=crashes, x='speeding',y='alcohol')\n\n\n\n\n\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='kde')\n\n\n\n\n\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg')\n\n\n\n\n\nsns.jointplot(data=titanic, x='fare',y='age')\n\n\n\n\n\n\nPair Plot\nWe can display pair plots across the entire dataset for each pair of numeric attributes\n\nsns.pairplot(data=crashes)\n\n\n\n\nWe can use hue to have color palettes of categorical data\n\nsns.pairplot(data=titanic, hue='sex')\n\n\n\n\n\n\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\n\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.stripplot(data = titanic, x = 'class', y='fare', hue='sex',jitter=True)\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.stripplot(data = titanic, x = 'class', y='fare', hue='sex',jitter=True, dodge=True)\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\n\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.swarmplot(data = titanic, x = 'alive', y='fare', color='red')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;"
  },
  {
    "objectID": "posts/seaborn_demo.html#other-feature",
    "href": "posts/seaborn_demo.html#other-feature",
    "title": "Seaborn Examples",
    "section": "Other Feature",
    "text": "Other Feature\n\nResizing\nWe can resize the plot using height, width and aspect parameters\n\nsns.displot(data = crashes, x = 'total', height = 2 , aspect = 1.6)\n\n\n\n\n\n\nStyling\n\nsns.set_style('darkgrid')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\nsns.set_style('whitegrid')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\nsns.set_style('ticks')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\n\nLabel Styling\n\nsns.set_context('poster')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\nsns.set_context('paper')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\nsns.despine(left=True, bottom=True) # False turns off the boundary \n\n\n\n\n\n\nSave Plot\nSince seaboarn is built on top of the matplotlib package, we can use matplotlib’s savefig() function to save the generated plot into image file.\nNote: The savefig() function should come before the show() function since the later closes and deletes the image from the memory to save space.\n\nsns.displot(crashes['alcohol'])\nplt.savefig('picture.png')\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yoseph’s ML Blog",
    "section": "",
    "text": "Seaborn Examples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMusic Data Clustering\n\n\n\nKMeans\n\n\nClustering\n\n\n\nK-Means Clustering\n\n\n\n\n\n\nDec 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly Detection\n\n\n\nAnomaly Detection\n\n\nDBSCAN\n\n\n\nUsing DBSCAN\n\n\n\n\n\n\nDec 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Component Analysis\n\n\n\nProbability theory\n\n\nPCA\n\n\n\nWhat’s PCA, and how to calculate it\n\n\n\nYoseph Berhanu\n\n\nDec 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudent Grade Prediction\n\n\n\nConfusion Matrix\n\n\nClassification\n\n\n\nUsing three ML classifier to predict student grade\n\n\n\nYoseph Berhanu\n\n\nDec 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSong popularity prediction\n\n\n\nRegression\n\n\n\nLinear and non linear regration\n\n\n\n\n\n\nNov 6, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/pca.html#footnotes",
    "href": "posts/pca.html#footnotes",
    "title": "Principal Component Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Mixtures of Probabilistic Principal Component Analysers” - Michael E. Tipping and Christopher M. Bishop http://www.miketipping.com/papers/met-mppca.pdf↩︎"
  }
]