[
  {
    "objectID": "CS5805.html",
    "href": "CS5805.html",
    "title": "CS5805",
    "section": "",
    "text": "The blog posts presented here are created as part of the requirmetns for CS5805 offerd for fall 24 at Virginia Tech. More infomration about me can be found on my personal website"
  },
  {
    "objectID": "posts/pca.html",
    "href": "posts/pca.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Banner Image Credit: Jernej Furman"
  },
  {
    "objectID": "posts/pca.html#introduction",
    "href": "posts/pca.html#introduction",
    "title": "Principal Component Analysis",
    "section": "Introduction",
    "text": "Introduction\nPrincipal component analysis (PCA) is a widely used statistical method for examining extensive datasets characterized by numerous dimensions or features per observation. PCA is a relatively well-established theory with its roots dating back to the beginning of the 1900s. Its primary aim is to enhance data interpretability while conserving the maximum information possible, thereby enabling the visualization of multidimensional data. Essentially, PCA serves as a statistical approach to diminish dataset dimensionality. This is achieved by transforming the data through linear methods into a fresh coordinate system where a reduced number of dimensions can effectively encapsulate (most of) the data variation compared to the original dataset.\nIn various studies, researchers often employ the initial two principal components to plot data in a two-dimensional space, facilitating the identification of clusters among closely associated data points. The applications of principal component analysis span across diverse fields including population genetics, microbiome studies, and atmospheric science. As Machine Learning(ML) is a data-driven approach to building algorithms, PCA plays an important role in ML for Dimensionality Reduction, Data Preprocessing, Visualization, Decorrelation and Feature Extraction, Speeding Up Learning Algorithms, and Noise Filtering.\nThe idea behind PCA is, that given a dataset with some statistical distribution (i.e., not deterministic), we would like to find features that can best describe as much of the data as possible. By doing so we will be able to explain the data with fewer sets of features than the actual data has. In data involving numerous variables and dimensions, not all variables hold equal importance. Some variables are key while others are less critical. The Principal Component Analysis (PCA) method offers a systematic way to identify and eliminate less significant variables. By transforming the original variables into uncorrelated principal components, PCA retains essential information while discarding less important variables. This process simplifies complex datasets by focusing on the principal components that capture the majority of the dataset’s variance. Consequently, PCA enhances data transparency by emphasizing the critical factors while reducing unnecessary complexity.\nIn this blog, we will go through the steps for calculating PCA with an example data set demonstrating PCA in action to perform dimensionality reduction.\n\nWhat Exactly is Principal Component\nThe principal component represents a novel feature or feature formed by combining the original features linearly. By crafting one or more of these new features, the objective is to ensure that these combinations result in uncorrelated variables, known as principal components. This process involves condensing or compressing most of the information from the initial variables into the first component. In essence, when dealing with n-dimensional data, PCA generates n principal components. However, the primary goal of PCA is to maximize information encapsulation in the initial component, followed by retaining the maximum remaining information in subsequent components. This systematic approach prioritizes the encapsulation of significant data details within these newly constructed components in a step-by-step manner.\nArranging information within principal components enables effective dimensionality reduction without significant loss of information. This process involves discarding components that hold minimal information while regarding the remaining components as the new variables. By prioritizing the retention of informative components and disregarding those with lower significance, one can streamline the dataset and create a new set of variables that effectively captures the essential information from the original dataset. This approach facilitates a more concise representation of the data while preserving the most critical information contained within the retained components. While representing data with less number of features reduces the complexity of the data aiding in different aspects of machine learning it complicates understanding of the data as the newly formed features (i.e., principal components) do not have a one-to-one mapping with the original feature set. ## Data The data we will use in this post comes with seaborn plotting package. Specifically, we’ll use the car_crashes dataset from seaborn package. If you’re new to seaborn, I’ve another post that demonstrates different plot types available in seaborn.\nThe car_crashes dataset is data about car accidents, their cause, and cost to insurance companies in the states of the USA. It consists of the following features.\n\ntotal: Number of drivers involved in fatal collisions per billion miles (5.900–23.900)\nspeeding: Percentage Of Drivers Involved In Fatal Collisions Who Were Speeding (1.792–9.450)\nalcohol: Percentage Of Drivers Involved In Fatal Collisions Who Were Alcohol-Impaired (1.593–10.038)\nnot_distracted: Percentage Of Drivers Involved In Fatal Collisions Who Were Not Distracted (1.760–23.661)\nno_previous: Percentage Of Drivers Involved In Fatal Collisions Who Had Not Been Involved In Any Previous Accidents (5.900–21.280)\nins_premium: Car Insurance Premiums (641.960–1301.520)\nins_losses: Losses incurred by insurance companies for collisions per insured driver (82.75–194.780)\nabbrev: A two-letter abbreviation of US state name the data stands for\n\n\nImport the required packages\nLet’s start by importing the necessary packages\n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n\n\n\n\nLoad the data\nWe can now load the data and try to understand it\n\ncar_crashes = sns.load_dataset(\"car_crashes\")\ncar_crashes.head()\n\n\n\n\n\n\n\n\ntotal\nspeeding\nalcohol\nnot_distracted\nno_previous\nins_premium\nins_losses\nabbrev\n\n\n\n\n0\n18.8\n7.332\n5.640\n18.048\n15.040\n784.55\n145.08\nAL\n\n\n1\n18.1\n7.421\n4.525\n16.290\n17.014\n1053.48\n133.93\nAK\n\n\n2\n18.6\n6.510\n5.208\n15.624\n17.856\n899.47\n110.35\nAZ\n\n\n3\n22.4\n4.032\n5.824\n21.056\n21.280\n827.34\n142.39\nAR\n\n\n4\n12.0\n4.200\n3.360\n10.920\n10.680\n878.41\n165.63\nCA\n\n\n\n\n\n\n\nWe can see all features except the abbreviation (which represents the US state the sample stands for) are continuous-valued.\n\ncar_crashes.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 51 entries, 0 to 50\nData columns (total 8 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   total           51 non-null     float64\n 1   speeding        51 non-null     float64\n 2   alcohol         51 non-null     float64\n 3   not_distracted  51 non-null     float64\n 4   no_previous     51 non-null     float64\n 5   ins_premium     51 non-null     float64\n 6   ins_losses      51 non-null     float64\n 7   abbrev          51 non-null     object \ndtypes: float64(7), object(1)\nmemory usage: 3.3+ KB\n\n\nWe’ll remove the apprev column as it is not relevant to our demonstration in this post\n\ncar_crashes = car_crashes.drop('abbrev', axis=1)\n\n\n\nMaximum, Minimum, Standard deviation and Average Values\nThe maximum value in each of the columns\n\ncar_crashes.max()\n\ntotal               23.900\nspeeding             9.450\nalcohol             10.038\nnot_distracted      23.661\nno_previous         21.280\nins_premium       1301.520\nins_losses         194.780\ndtype: float64\n\n\nThe minimum value in each of the columns\n\ncar_crashes.min()\n\ntotal               5.900\nspeeding            1.792\nalcohol             1.593\nnot_distracted      1.760\nno_previous         5.900\nins_premium       641.960\nins_losses         82.750\ndtype: float64\n\n\nThe range in each column is the difference between the max and minimum values\n\ncar_crashes_range = car_crashes.max() - car_crashes.min()\ncar_crashes_range\n\ntotal              18.000\nspeeding            7.658\nalcohol             8.445\nnot_distracted     21.901\nno_previous        15.380\nins_premium       659.560\nins_losses        112.030\ndtype: float64\n\n\nWe can visualize the difference in the range\n\np1 = car_crashes_range.plot(\n    legend=False,\n    kind=\"bar\",\n    rot=45,\n    color=\"blue\",\n    fontsize=16,\n)\np1.set_title(\"Range of values in our car crash data\", fontsize=16)\np1.set_xlabel(\"Feature\", fontsize=14)\np1.set_ylabel(\"Range\", fontsize=14)\n\nText(0, 0.5, 'Range')\n\n\n\n\n\nWe can get the mean value and standard deviation of each column as well\n\ncar_crashes.mean(axis=0)\n\ntotal              15.790196\nspeeding            4.998196\nalcohol             4.886784\nnot_distracted     13.573176\nno_previous        14.004882\nins_premium       886.957647\nins_losses        134.493137\ndtype: float64\n\n\n\ncar_crashes.std(axis=0)\n\ntotal               4.122002\nspeeding            2.017747\nalcohol             1.729133\nnot_distracted      4.508977\nno_previous         3.764672\nins_premium       178.296285\nins_losses         24.835922\ndtype: float64"
  },
  {
    "objectID": "posts/pca.html#data",
    "href": "posts/pca.html#data",
    "title": "Principal Component Analysis",
    "section": "Data",
    "text": "Data\nThe data we will use in this post comes wih seaborn ploting package. Sepecificaly, we’ll use the car_crashes dataset from seaborn package. If you’re new to seaborn, I’ve another post that demonstratos different plot types available in seaborn.\nThe car_crashes dataset is data about car accidents, their cause and cost to insurance companies in the states of the USA. It conists of the following features.\n\ntotal: Number of drivers involved in fatal collisions per billion miles (5.900–23.900)\nspeeding: Percentage Of Drivers Involved In Fatal Collisions Who Were Speeding (1.792–9.450)\nalcohol: Percentage Of Drivers Involved In Fatal Collisions Who Were Alcohol-Impaired (1.593–10.038)\nnot_distracted: Percentage Of Drivers Involved In Fatal Collisions Who Were Not Distracted (1.760–23.661)\nno_previous: Percentage Of Drivers Involved In Fatal Collisions Who Had Not Been Involved In Any Previous Accidents (5.900–21.280)\nins_premium: Car Insurance Premiums (641.960–1301.520)\nins_losses: Losses incurred by insurance companies for collisions per insured driver (82.75–194.780)\nabbrev: A two letter abbrivation of US state name the data stands for\n\n\nImport the required packages\nLet’s start by importing the necessary packages\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\nLoad the data\nWe can now load the data and try to understand it\n\ncar_crashes = sns.load_dataset(\"car_crashes\")\ncar_crashes.head()\n\n\n\n\n\n\n\n\ntotal\nspeeding\nalcohol\nnot_distracted\nno_previous\nins_premium\nins_losses\nabbrev\n\n\n\n\n0\n18.8\n7.332\n5.640\n18.048\n15.040\n784.55\n145.08\nAL\n\n\n1\n18.1\n7.421\n4.525\n16.290\n17.014\n1053.48\n133.93\nAK\n\n\n2\n18.6\n6.510\n5.208\n15.624\n17.856\n899.47\n110.35\nAZ\n\n\n3\n22.4\n4.032\n5.824\n21.056\n21.280\n827.34\n142.39\nAR\n\n\n4\n12.0\n4.200\n3.360\n10.920\n10.680\n878.41\n165.63\nCA\n\n\n\n\n\n\n\nWe can see all features exccept the abbrevation(which represents the US state the sample stand for) are continues valued.\n\ncar_crashes.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 51 entries, 0 to 50\nData columns (total 8 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   total           51 non-null     float64\n 1   speeding        51 non-null     float64\n 2   alcohol         51 non-null     float64\n 3   not_distracted  51 non-null     float64\n 4   no_previous     51 non-null     float64\n 5   ins_premium     51 non-null     float64\n 6   ins_losses      51 non-null     float64\n 7   abbrev          51 non-null     object \ndtypes: float64(7), object(1)\nmemory usage: 3.3+ KB\n\n\nWe’ll remove the apprev column as it is not relevant to our demonstration in this post\n\ncar_crashes = car_crashes.drop('abbrev', axis=1)\n\n\n\nMaximum, Minimum, Standard diviation and Avarage Values\nThe maximum value in each of the columns\n\ncar_crashes.max()\n\ntotal               23.900\nspeeding             9.450\nalcohol             10.038\nnot_distracted      23.661\nno_previous         21.280\nins_premium       1301.520\nins_losses         194.780\ndtype: float64\n\n\nThe minimum value in each of the columns\n\ncar_crashes.min()\n\ntotal               5.900\nspeeding            1.792\nalcohol             1.593\nnot_distracted      1.760\nno_previous         5.900\nins_premium       641.960\nins_losses         82.750\ndtype: float64\n\n\nThe range in each column is the difference between the max and minimum values\n\ncar_crashes_range = car_crashes.max() - car_crashes.min()\ncar_crashes_range\n\ntotal              18.000\nspeeding            7.658\nalcohol             8.445\nnot_distracted     21.901\nno_previous        15.380\nins_premium       659.560\nins_losses        112.030\ndtype: float64\n\n\nWe can visualize the difference in range\n\np1 = car_crashes_range.plot(\n    legend=False,\n    kind=\"bar\",\n    rot=45,\n    color=\"blue\",\n    fontsize=16,\n)\np1.set_title(\"Range of values in our car crash data\", fontsize=16)\np1.set_xlabel(\"Feature\", fontsize=14)\np1.set_ylabel(\"Range\", fontsize=14)\n\nText(0, 0.5, 'Range')\n\n\n\n\n\nWe can get the mean value and standard deviation of each column as well\n\ncar_crashes.mean(axis=0)\n\ntotal              15.790196\nspeeding            4.998196\nalcohol             4.886784\nnot_distracted     13.573176\nno_previous        14.004882\nins_premium       886.957647\nins_losses        134.493137\ndtype: float64\n\n\n\ncar_crashes.std(axis=0)\n\ntotal               4.122002\nspeeding            2.017747\nalcohol             1.729133\nnot_distracted      4.508977\nno_previous         3.764672\nins_premium       178.296285\nins_losses         24.835922\ndtype: float64"
  },
  {
    "objectID": "posts/regression.html",
    "href": "posts/regression.html",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "Adipisicing sunt nisi nulla pariatur ex ipsum mollit. Dolor commodo cillum sint amet commodo. Nulla culpa aliqua enim ad anim. Aliquip ex duis minim laborum enim aliquip ad aliqua. Sit consectetur excepteur ea aliqua exercitation Lorem ex elit incididunt consequat ea. Excepteur qui velit commodo non cillum enim duis occaecat laborum tempor sint do anim. In nostrud ex aliqua commodo sunt quis velit. Enim laborum officia officia nostrud sunt velit dolore Lorem occaecat ipsum.\nPariatur mollit in eiusmod irure velit veniam aliquip tempor culpa minim. Nulla proident eiusmod ipsum aute eiusmod aliqua. Amet cupidatat adipisicing nostrud exercitation amet anim aliqua sint mollit in dolor culpa. Adipisicing nostrud culpa excepteur ullamco. Eiusmod eu laboris excepteur sint eu excepteur et. Ullamco eu cupidatat consectetur fugiat minim fugiat sunt ut minim. Pariatur dolor exercitation sunt ipsum Lorem velit sit Lorem laboris Lorem.\nQui Lorem fugiat do commodo exercitation anim commodo incididunt sint aute deserunt nulla. Ex incididunt do ut ullamco exercitation in laborum cupidatat. Dolor laboris cupidatat anim sint. Culpa elit voluptate officia ex elit ullamco fugiat pariatur mollit labore eiusmod id anim."
  },
  {
    "objectID": "posts/clustering.html",
    "href": "posts/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Banner Image Credit: University of Rochester\nAdipisicing sunt nisi nulla pariatur ex ipsum mollit. Dolor commodo cillum sint amet commodo. Nulla culpa aliqua enim ad anim. Aliquip ex duis minim laborum enim aliquip ad aliqua. Sit consectetur excepteur ea aliqua exercitation Lorem ex elit incididunt consequat ea. Excepteur qui velit commodo non cillum enim duis occaecat laborum tempor sint do anim. In nostrud ex aliqua commodo sunt quis velit. Enim laborum officia officia nostrud sunt velit dolore Lorem occaecat ipsum.\nPariatur mollit in eiusmod irure velit veniam aliquip tempor culpa minim. Nulla proident eiusmod ipsum aute eiusmod aliqua. Amet cupidatat adipisicing nostrud exercitation amet anim aliqua sint mollit in dolor culpa. Adipisicing nostrud culpa excepteur ullamco. Eiusmod eu laboris excepteur sint eu excepteur et. Ullamco eu cupidatat consectetur fugiat minim fugiat sunt ut minim. Pariatur dolor exercitation sunt ipsum Lorem velit sit Lorem laboris Lorem.\nQui Lorem fugiat do commodo exercitation anim commodo incididunt sint aute deserunt nulla. Ex incididunt do ut ullamco exercitation in laborum cupidatat. Dolor laboris cupidatat anim sint. Culpa elit voluptate officia ex elit ullamco fugiat pariatur mollit labore eiusmod id anim."
  },
  {
    "objectID": "posts/classification.html",
    "href": "posts/classification.html",
    "title": "Classification",
    "section": "",
    "text": "Banner Image Credit: Jernej Furman\nAdipisicing sunt nisi nulla pariatur ex ipsum mollit. Dolor commodo cillum sint amet commodo. Nulla culpa aliqua enim ad anim. Aliquip ex duis minim laborum enim aliquip ad aliqua. Sit consectetur excepteur ea aliqua exercitation Lorem ex elit incididunt consequat ea. Excepteur qui velit commodo non cillum enim duis occaecat laborum tempor sint do anim. In nostrud ex aliqua commodo sunt quis velit. Enim laborum officia officia nostrud sunt velit dolore Lorem occaecat ipsum.\nPariatur mollit in eiusmod irure velit veniam aliquip tempor culpa minim. Nulla proident eiusmod ipsum aute eiusmod aliqua. Amet cupidatat adipisicing nostrud exercitation amet anim aliqua sint mollit in dolor culpa. Adipisicing nostrud culpa excepteur ullamco. Eiusmod eu laboris excepteur sint eu excepteur et. Ullamco eu cupidatat consectetur fugiat minim fugiat sunt ut minim. Pariatur dolor exercitation sunt ipsum Lorem velit sit Lorem laboris Lorem.\nQui Lorem fugiat do commodo exercitation anim commodo incididunt sint aute deserunt nulla. Ex incididunt do ut ullamco exercitation in laborum cupidatat. Dolor laboris cupidatat anim sint. Culpa elit voluptate officia ex elit ullamco fugiat pariatur mollit labore eiusmod id anim."
  },
  {
    "objectID": "posts/post5.html",
    "href": "posts/post5.html",
    "title": "Setup",
    "section": "",
    "text": "Chapter 1 – The Machine Learning landscape\nThis notebook contains the code examples in chapter 1. You’ll also find the exercise solutions at the end of the notebook. The rest of this notebook is used to generate lifesat.csv from the original data sources, and some of this chapter’s figures.\nYou’re welcome to go through the code in this notebook if you want, but the real action starts in the next chapter.\nThis project requires Python 3.7 or above:\nimport sys\n\nassert sys.version_info &gt;= (3, 7)\nScikit-Learn ≥1.0.1 is required:\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\nLet’s define the default font sizes, to plot pretty figures:\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=12)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=12)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\nMake this notebook’s output stable across runs:\nimport numpy as np\n\nnp.random.seed(42)"
  },
  {
    "objectID": "posts/post5.html#load-and-prepare-life-satisfaction-data",
    "href": "posts/post5.html#load-and-prepare-life-satisfaction-data",
    "title": "Setup",
    "section": "Load and prepare Life satisfaction data",
    "text": "Load and prepare Life satisfaction data\nTo create lifesat.csv, I downloaded the Better Life Index (BLI) data from OECD’s website (to get the Life Satisfaction for each country), and World Bank GDP per capita data from OurWorldInData.org. The BLI data is in datasets/lifesat/oecd_bli.csv (data from 2020), and the GDP per capita data is in datasets/lifesat/gdp_per_capita.csv (data up to 2020).\nIf you want to grab the latest versions, please feel free to do so. However, there may be some changes (e.g., in the column names, or different countries missing data), so be prepared to have to tweak the code.\n\nimport urllib.request\n\ndatapath = Path() / \"datasets\" / \"lifesat\"\ndatapath.mkdir(parents=True, exist_ok=True)\n\ndata_root = \"https://github.com/ageron/data/raw/main/\"\nfor filename in (\"oecd_bli.csv\", \"gdp_per_capita.csv\"):\n    if not (datapath / filename).is_file():\n        print(\"Downloading\", filename)\n        url = data_root + \"lifesat/\" + filename\n        urllib.request.urlretrieve(url, datapath / filename)\n\nDownloading oecd_bli.csv\nDownloading gdp_per_capita.csv\n\n\n\noecd_bli = pd.read_csv(datapath / \"oecd_bli.csv\")\ngdp_per_capita = pd.read_csv(datapath / \"gdp_per_capita.csv\")\n\nPreprocess the GDP per capita data to keep only the year 2020:\n\ngdp_year = 2020\ngdppc_col = \"GDP per capita (USD)\"\nlifesat_col = \"Life satisfaction\"\n\ngdp_per_capita = gdp_per_capita[gdp_per_capita[\"Year\"] == gdp_year]\ngdp_per_capita = gdp_per_capita.drop([\"Code\", \"Year\"], axis=1)\ngdp_per_capita.columns = [\"Country\", gdppc_col]\ngdp_per_capita.set_index(\"Country\", inplace=True)\n\ngdp_per_capita.head()\n\n\n\n\n\n\n\n\nGDP per capita (USD)\n\n\nCountry\n\n\n\n\n\nAfghanistan\n1978.961579\n\n\nAfrica Eastern and Southern\n3387.594670\n\n\nAfrica Western and Central\n4003.158913\n\n\nAlbania\n13295.410885\n\n\nAlgeria\n10681.679297\n\n\n\n\n\n\n\nPreprocess the OECD BLI data to keep only the Life satisfaction column:\n\noecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\noecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n\noecd_bli.head()\n\n\n\n\n\n\n\nIndicator\nAir pollution\nDwellings without basic facilities\nEducational attainment\nEmployees working very long hours\nEmployment rate\nFeeling safe walking alone at night\nHomicide rate\nHousehold net adjusted disposable income\nHousehold net wealth\nHousing expenditure\n...\nPersonal earnings\nQuality of support network\nRooms per person\nSelf-reported health\nStakeholder engagement for developing regulations\nStudent skills\nTime devoted to leisure and personal care\nVoter turnout\nWater quality\nYears in education\n\n\nCountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAustralia\n5.0\nNaN\n81.0\n13.04\n73.0\n63.5\n1.1\n32759.0\n427064.0\n20.0\n...\n49126.0\n95.0\nNaN\n85.0\n2.7\n502.0\n14.35\n91.0\n93.0\n21.0\n\n\nAustria\n16.0\n0.9\n85.0\n6.66\n72.0\n80.6\n0.5\n33541.0\n308325.0\n21.0\n...\n50349.0\n92.0\n1.6\n70.0\n1.3\n492.0\n14.55\n80.0\n92.0\n17.0\n\n\nBelgium\n15.0\n1.9\n77.0\n4.75\n63.0\n70.1\n1.0\n30364.0\n386006.0\n21.0\n...\n49675.0\n91.0\n2.2\n74.0\n2.0\n503.0\n15.70\n89.0\n84.0\n19.3\n\n\nBrazil\n10.0\n6.7\n49.0\n7.13\n61.0\n35.6\n26.7\nNaN\nNaN\nNaN\n...\nNaN\n90.0\nNaN\nNaN\n2.2\n395.0\nNaN\n79.0\n73.0\n16.2\n\n\nCanada\n7.0\n0.2\n91.0\n3.69\n73.0\n82.2\n1.3\n30854.0\n423849.0\n22.0\n...\n47622.0\n93.0\n2.6\n88.0\n2.9\n523.0\n14.56\n68.0\n91.0\n17.3\n\n\n\n\n5 rows × 24 columns\n\n\n\nNow let’s merge the life satisfaction data and the GDP per capita data, keeping only the GDP per capita and Life satisfaction columns:\n\nfull_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,\n                              left_index=True, right_index=True)\nfull_country_stats.sort_values(by=gdppc_col, inplace=True)\nfull_country_stats = full_country_stats[[gdppc_col, lifesat_col]]\n\nfull_country_stats.head()\n\n\n\n\n\n\n\n\nGDP per capita (USD)\nLife satisfaction\n\n\nCountry\n\n\n\n\n\n\nSouth Africa\n11466.189672\n4.7\n\n\nColombia\n13441.492952\n6.3\n\n\nBrazil\n14063.982505\n6.4\n\n\nMexico\n17887.750736\n6.5\n\n\nChile\n23324.524751\n6.5\n\n\n\n\n\n\n\nTo illustrate the risk of overfitting, I use only part of the data in most figures (all countries with a GDP per capita between min_gdp and max_gdp). Later in the chapter I reveal the missing countries, and show that they don’t follow the same linear trend at all.\n\nmin_gdp = 23_500\nmax_gdp = 62_500\n\ncountry_stats = full_country_stats[(full_country_stats[gdppc_col] &gt;= min_gdp) &\n                                   (full_country_stats[gdppc_col] &lt;= max_gdp)]\ncountry_stats.head()\n\n\n\n\n\n\n\n\nGDP per capita (USD)\nLife satisfaction\n\n\nCountry\n\n\n\n\n\n\nRussia\n26456.387938\n5.8\n\n\nGreece\n27287.083401\n5.4\n\n\nTurkey\n28384.987785\n5.5\n\n\nLatvia\n29932.493910\n5.9\n\n\nHungary\n31007.768407\n5.6\n\n\n\n\n\n\n\n\ncountry_stats.to_csv(datapath / \"lifesat.csv\")\nfull_country_stats.to_csv(datapath / \"lifesat_full.csv\")\n\n\ncountry_stats.plot(kind='scatter', figsize=(5, 3), grid=True,\n                   x=gdppc_col, y=lifesat_col)\n\nmin_life_sat = 4\nmax_life_sat = 9\n\nposition_text = {\n    \"Turkey\": (29_500, 4.2),\n    \"Hungary\": (28_000, 6.9),\n    \"France\": (40_000, 5),\n    \"New Zealand\": (28_000, 8.2),\n    \"Australia\": (50_000, 5.5),\n    \"United States\": (59_000, 5.3),\n    \"Denmark\": (46_000, 8.5)\n}\n\nfor country, pos_text in position_text.items():\n    pos_data_x = country_stats[gdppc_col].loc[country]\n    pos_data_y = country_stats[lifesat_col].loc[country]\n    country = \"U.S.\" if country == \"United States\" else country\n    plt.annotate(country, xy=(pos_data_x, pos_data_y),\n                 xytext=pos_text, fontsize=12,\n                 arrowprops=dict(facecolor='black', width=0.5,\n                                 shrink=0.08, headwidth=5))\n    plt.plot(pos_data_x, pos_data_y, \"ro\")\n\nplt.axis([min_gdp, max_gdp, min_life_sat, max_life_sat])\n\nsave_fig('money_happy_scatterplot')\nplt.show()\n\n\n\n\n\nhighlighted_countries = country_stats.loc[list(position_text.keys())]\nhighlighted_countries[[gdppc_col, lifesat_col]].sort_values(by=gdppc_col)\n\n\n\n\n\n\n\n\nGDP per capita (USD)\nLife satisfaction\n\n\nCountry\n\n\n\n\n\n\nTurkey\n28384.987785\n5.5\n\n\nHungary\n31007.768407\n5.6\n\n\nFrance\n42025.617373\n6.5\n\n\nNew Zealand\n42404.393738\n7.3\n\n\nAustralia\n48697.837028\n7.3\n\n\nDenmark\n55938.212809\n7.6\n\n\nUnited States\n60235.728492\n6.9\n\n\n\n\n\n\n\n\ncountry_stats.plot(kind='scatter', figsize=(5, 3), grid=True,\n                   x=gdppc_col, y=lifesat_col)\n\nX = np.linspace(min_gdp, max_gdp, 1000)\n\nw1, w2 = 4.2, 0\nplt.plot(X, w1 + w2 * 1e-5 * X, \"r\")\nplt.text(40_000, 4.9, fr\"$\\theta_0 = {w1}$\", color=\"r\")\nplt.text(40_000, 4.4, fr\"$\\theta_1 = {w2}$\", color=\"r\")\n\nw1, w2 = 10, -9\nplt.plot(X, w1 + w2 * 1e-5 * X, \"g\")\nplt.text(26_000, 8.5, fr\"$\\theta_0 = {w1}$\", color=\"g\")\nplt.text(26_000, 8.0, fr\"$\\theta_1 = {w2} \\times 10^{{-5}}$\", color=\"g\")\n\nw1, w2 = 3, 8\nplt.plot(X, w1 + w2 * 1e-5 * X, \"b\")\nplt.text(48_000, 8.5, fr\"$\\theta_0 = {w1}$\", color=\"b\")\nplt.text(48_000, 8.0, fr\"$\\theta_1 = {w2} \\times 10^{{-5}}$\", color=\"b\")\n\nplt.axis([min_gdp, max_gdp, min_life_sat, max_life_sat])\n\nsave_fig('tweaking_model_params_plot')\nplt.show()\n\n\n\n\n\nfrom sklearn import linear_model\n\nX_sample = country_stats[[gdppc_col]].values\ny_sample = country_stats[[lifesat_col]].values\n\nlin1 = linear_model.LinearRegression()\nlin1.fit(X_sample, y_sample)\n\nt0, t1 = lin1.intercept_[0], lin1.coef_[0][0]\nprint(f\"θ0={t0:.2f}, θ1={t1:.2e}\")\n\nθ0=3.75, θ1=6.78e-05\n\n\n\ncountry_stats.plot(kind='scatter', figsize=(5, 3), grid=True,\n                   x=gdppc_col, y=lifesat_col)\n\nX = np.linspace(min_gdp, max_gdp, 1000)\nplt.plot(X, t0 + t1 * X, \"b\")\n\nplt.text(max_gdp - 20_000, min_life_sat + 1.9,\n         fr\"$\\theta_0 = {t0:.2f}$\", color=\"b\")\nplt.text(max_gdp - 20_000, min_life_sat + 1.3,\n         fr\"$\\theta_1 = {t1 * 1e5:.2f} \\times 10^{{-5}}$\", color=\"b\")\n\nplt.axis([min_gdp, max_gdp, min_life_sat, max_life_sat])\n\nsave_fig('best_fit_model_plot')\nplt.show()\n\n\n\n\n\ncyprus_gdp_per_capita = gdp_per_capita[gdppc_col].loc[\"Cyprus\"]\ncyprus_gdp_per_capita\n\n37655.1803457421\n\n\n\ncyprus_predicted_life_satisfaction = lin1.predict([[cyprus_gdp_per_capita]])[0, 0]\ncyprus_predicted_life_satisfaction\n\n6.301656332738056\n\n\n\ncountry_stats.plot(kind='scatter', figsize=(5, 3), grid=True,\n                   x=gdppc_col, y=lifesat_col)\n\nX = np.linspace(min_gdp, max_gdp, 1000)\nplt.plot(X, t0 + t1 * X, \"b\")\n\nplt.text(min_gdp + 22_000, max_life_sat - 1.1,\n         fr\"$\\theta_0 = {t0:.2f}$\", color=\"b\")\nplt.text(min_gdp + 22_000, max_life_sat - 0.6,\n         fr\"$\\theta_1 = {t1 * 1e5:.2f} \\times 10^{{-5}}$\", color=\"b\")\n\nplt.plot([cyprus_gdp_per_capita, cyprus_gdp_per_capita],\n         [min_life_sat, cyprus_predicted_life_satisfaction], \"r--\")\nplt.text(cyprus_gdp_per_capita + 1000, 5.0,\n         fr\"Prediction = {cyprus_predicted_life_satisfaction:.2f}\", color=\"r\")\nplt.plot(cyprus_gdp_per_capita, cyprus_predicted_life_satisfaction, \"ro\")\n\nplt.axis([min_gdp, max_gdp, min_life_sat, max_life_sat])\n\nplt.show()\n\n\n\n\n\nmissing_data = full_country_stats[(full_country_stats[gdppc_col] &lt; min_gdp) |\n                                  (full_country_stats[gdppc_col] &gt; max_gdp)]\nmissing_data\n\n\n\n\n\n\n\n\nGDP per capita (USD)\nLife satisfaction\n\n\nCountry\n\n\n\n\n\n\nSouth Africa\n11466.189672\n4.7\n\n\nColombia\n13441.492952\n6.3\n\n\nBrazil\n14063.982505\n6.4\n\n\nMexico\n17887.750736\n6.5\n\n\nChile\n23324.524751\n6.5\n\n\nNorway\n63585.903514\n7.6\n\n\nSwitzerland\n68393.306004\n7.5\n\n\nIreland\n89688.956958\n7.0\n\n\nLuxembourg\n110261.157353\n6.9\n\n\n\n\n\n\n\n\nposition_text_missing_countries = {\n    \"South Africa\": (20_000, 4.2),\n    \"Colombia\": (6_000, 8.2),\n    \"Brazil\": (18_000, 7.8),\n    \"Mexico\": (24_000, 7.4),\n    \"Chile\": (30_000, 7.0),\n    \"Norway\": (51_000, 6.2),\n    \"Switzerland\": (62_000, 5.7),\n    \"Ireland\": (81_000, 5.2),\n    \"Luxembourg\": (92_000, 4.7),\n}\n\n\nfull_country_stats.plot(kind='scatter', figsize=(8, 3),\n                        x=gdppc_col, y=lifesat_col, grid=True)\n\nfor country, pos_text in position_text_missing_countries.items():\n    pos_data_x, pos_data_y = missing_data.loc[country]\n    plt.annotate(country, xy=(pos_data_x, pos_data_y),\n                 xytext=pos_text, fontsize=12,\n                 arrowprops=dict(facecolor='black', width=0.5,\n                                 shrink=0.08, headwidth=5))\n    plt.plot(pos_data_x, pos_data_y, \"rs\")\n\nX = np.linspace(0, 115_000, 1000)\nplt.plot(X, t0 + t1 * X, \"b:\")\n\nlin_reg_full = linear_model.LinearRegression()\nXfull = np.c_[full_country_stats[gdppc_col]]\nyfull = np.c_[full_country_stats[lifesat_col]]\nlin_reg_full.fit(Xfull, yfull)\n\nt0full, t1full = lin_reg_full.intercept_[0], lin_reg_full.coef_[0][0]\nX = np.linspace(0, 115_000, 1000)\nplt.plot(X, t0full + t1full * X, \"k\")\n\nplt.axis([0, 115_000, min_life_sat, max_life_sat])\n\nsave_fig('representative_training_data_scatterplot')\nplt.show()\n\n\n\n\n\nfrom sklearn import preprocessing\nfrom sklearn import pipeline\n\nfull_country_stats.plot(kind='scatter', figsize=(8, 3),\n                        x=gdppc_col, y=lifesat_col, grid=True)\n\npoly = preprocessing.PolynomialFeatures(degree=10, include_bias=False)\nscaler = preprocessing.StandardScaler()\nlin_reg2 = linear_model.LinearRegression()\n\npipeline_reg = pipeline.Pipeline([\n    ('poly', poly),\n    ('scal', scaler),\n    ('lin', lin_reg2)])\npipeline_reg.fit(Xfull, yfull)\ncurve = pipeline_reg.predict(X[:, np.newaxis])\nplt.plot(X, curve)\n\nplt.axis([0, 115_000, min_life_sat, max_life_sat])\n\nsave_fig('overfitting_model_plot')\nplt.show()\n\n\n\n\n\nw_countries = [c for c in full_country_stats.index if \"W\" in c.upper()]\nfull_country_stats.loc[w_countries][lifesat_col]\n\nCountry\nNew Zealand    7.3\nSweden         7.3\nNorway         7.6\nSwitzerland    7.5\nName: Life satisfaction, dtype: float64\n\n\n\nall_w_countries = [c for c in gdp_per_capita.index if \"W\" in c.upper()]\ngdp_per_capita.loc[all_w_countries].sort_values(by=gdppc_col)\n\n\n\n\n\n\n\n\nGDP per capita (USD)\n\n\nCountry\n\n\n\n\n\nMalawi\n1486.778248\n\n\nRwanda\n2098.710362\n\n\nZimbabwe\n2744.690758\n\n\nAfrica Western and Central\n4003.158913\n\n\nPapua New Guinea\n4101.218882\n\n\nLower middle income\n6722.809932\n\n\nEswatini\n8392.717564\n\n\nLow & middle income\n10293.855325\n\n\nArab World\n13753.707307\n\n\nBotswana\n16040.008473\n\n\nWorld\n16194.040310\n\n\nNew Zealand\n42404.393738\n\n\nSweden\n50683.323510\n\n\nNorway\n63585.903514\n\n\nSwitzerland\n68393.306004\n\n\n\n\n\n\n\n\ncountry_stats.plot(kind='scatter', x=gdppc_col, y=lifesat_col, figsize=(8, 3))\nmissing_data.plot(kind='scatter', x=gdppc_col, y=lifesat_col,\n                  marker=\"s\", color=\"r\", grid=True, ax=plt.gca())\n\nX = np.linspace(0, 115_000, 1000)\nplt.plot(X, t0 + t1*X, \"b:\", label=\"Linear model on partial data\")\nplt.plot(X, t0full + t1full * X, \"k-\", label=\"Linear model on all data\")\n\nridge = linear_model.Ridge(alpha=10**9.5)\nX_sample = country_stats[[gdppc_col]]\ny_sample = country_stats[[lifesat_col]]\nridge.fit(X_sample, y_sample)\nt0ridge, t1ridge = ridge.intercept_[0], ridge.coef_[0][0]\nplt.plot(X, t0ridge + t1ridge * X, \"b--\",\n         label=\"Regularized linear model on partial data\")\nplt.legend(loc=\"lower right\")\n\nplt.axis([0, 115_000, min_life_sat, max_life_sat])\n\nsave_fig('ridge_model_plot')\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yoseph’s ML Blog",
    "section": "",
    "text": "Setup\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeaborn Examples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\nDBScan\n\n\nClustering\n\n\n\nDBSCAN labels for scatter plot\n\n\n\n\n\n\nDec 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification\n\n\n\nROC\n\n\nPR\n\n\nConfusion Matrix\n\n\nClassification\n\n\n\nROC, PR, Confusion Matrix\n\n\n\n\n\n\nDec 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Component Analysis\n\n\n\nProbability theory\n\n\nPCA\n\n\n\nWhat’s PCA, and how to calculate it\n\n\n\nYoseph Berhanu\n\n\nDec 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear and nonlinear regression\n\n\n\nRegression\n\n\n\nline on scatter plot\n\n\n\n\n\n\nNov 6, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/pca.html#pca-in-action",
    "href": "posts/pca.html#pca-in-action",
    "title": "Principal Component Analysis",
    "section": "PCA in Action",
    "text": "PCA in Action\n\nStep 1. Standardization\nPCA is sensitive to variance, meaning if features different range in values the PCA calculation will be dominated by features with a larger range. In our dataset, we have features that represent a percentage value (hence the potential values range from 0 to 100, but the actual maximum value in the dataset is way below 100). We also have other features that have a more inconsistent range. For example, the ins_premium has a range of 659.560 while speading has a range of 7.658. As a result, the PCA analysis will be greatly (and incorrectly) influenced by a change in ins_premium value.\nTo address this issue we perform standardization before PCA, that is we bring values of all features to a standard range. The one common way to standardize any data is called mean centering where we subtract the mean from each value.\n\\[\nz = v - \\mu\n\\] Where \\(z\\) is the standardized value, \\(v\\) is the original value, \\(\\mu\\) stands for mean and \\(\\sigma\\) is the standard deviation\nWe can perform this calculation in python as follows\n\ncar_crashes_std = (car_crashes - car_crashes.mean(axis=0)) / car_crashes.std(axis=0)\ncar_crashes_std.head()\n\n\n\n\n\n\n\n\ntotal\nspeeding\nalcohol\nnot_distracted\nno_previous\nins_premium\nins_losses\n\n\n\n\n0\n0.730180\n1.156638\n0.435603\n0.992425\n0.274956\n-0.574368\n0.426272\n\n\n1\n0.560360\n1.200747\n-0.209229\n0.602537\n0.799304\n0.933964\n-0.022674\n\n\n2\n0.681660\n0.749253\n0.185767\n0.454831\n1.022962\n0.070177\n-0.972106\n\n\n3\n1.603542\n-0.478849\n0.542015\n1.659539\n1.932471\n-0.334374\n0.317961\n\n\n4\n-0.919504\n-0.395588\n-0.882977\n-0.588421\n-0.883180\n-0.047941\n1.253703\n\n\n\n\n\n\n\nLet’s perform a similar visualization on the range as we did earlier\n\np2 = (car_crashes_std.max() - car_crashes_std.min()).plot(\n    legend=False, kind=\"bar\", rot=45, color=\"blue\", fontsize=16\n)\np2.set_title(\"Range of values in our car crash data after standardization\", fontsize=16)\np2.set_xlabel(\"Features\", fontsize=14)\np2.set_ylabel(\"Range\", fontsize=14)\n\nText(0, 0.5, 'Range')\n\n\n\n\n\nWe can see that now the range is more consistent across the features\n\n\nStep 2. Covariance Matrix\nCovariance Matrix To get a sense of how the values of the input dataset vary from the mean to each other we compute the covariance matrix. The covariance matrix is a \\(m × m\\) symmetric matrix (where m is the number of features we’ve in our input dataset). Features might be highly correlated and through computing the covariance matrix we can identify this relationship.\nFor example, a covariance matrix of an input dataset with 4 features \\((f_1,f_2,f_3,f_4)\\) will be a 4x4 matrix of the following form\n\\[\nCovariance Matrix = \\begin{bmatrix}\n      cov(f_1,f_1) & cov(f_1,f_2) & cov(f_1,f_3) & cov(f_1,f_4) \\\\\n      cov(f_2,f_1) & cov(f_2,f_2) & cov(f_2,f_3) & cov(f_2,f_4) \\\\\n      cov(f_3,f_1) & cov(f_3,f_2) & cov(f_3,f_3) & cov(f_3,f_4) \\\\\n      cov(f_4,f_1) & cov(f_4,f_2) & cov(f_3,f_3) & cov(f_4,f_4)\n\\end{bmatrix}\n\\]\nWe compute the covariance between to features \\(f_x\\) and \\(f_y\\) as follows \\[\ncov(f_x,f_y) = \\dfrac{\\sum(x_i-\\mu_x) (y_i-\\mu_y)}{n}\n\\] Where \\(x_i\\) and \\(y_i\\) are the \\(i^{th}\\) values for feature \\(x\\) and \\(y\\) and \\(n\\) is the total number of data points.\nWith this equation notice how\n\ncovariance of a feature f with itself (i.e., \\(Cov(f_x,f_x)\\)) the variance of the feature (i.e., \\(Var(f_x)\\))\ncovariance is symmetric, i.e., \\(Cov(fx,fy) = Cov(f_y,f_x)\\)\n\nNow that we know that the covariance matrix is not more than a table that summarizes the correlations between all the possible pairs of variables, let’s move to the next step. While we can use these equations to calculate the covariance matrix pandas provide a built-in method for calculating it in one go.\n\ncov_mat = car_crashes_std.cov()\n\nWe can visualize this matrix using seaborn’s heatmap\n\nsns.heatmap(cov_mat, annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nStep 3. Eigenvectors and Eigenvalues\nEigenvectors and eigenvalues, fundamental concepts from linear algebra, play a crucial role in computing the principal components of a dataset from its covariance matrix. In an n-dimensional dataset comprising n features, there exist n eigenvectors accompanied by n corresponding eigenvalues.\nEigenvectors derived from the Covariance matrix represent the axes’ directions where the highest variance or most information is concentrated. These eigenvectors are termed Principal Components. Eigenvalues, on the other hand, serve as coefficients associated with eigenvectors, signifying the quantity of variance encapsulated within each Principal Component. Essentially, eigenvalues provide a measure of the amount of variance carried by the respective eigenvectors, highlighting their significance in determining the variance distribution across the dataset’s principal axes.\nArranging your eigenvectors based on their corresponding eigenvalues, from highest to lowest, provides the principal components in descending order of significance. This ranking scheme ensures that the principal components are prioritized based on the amount of variance or information they encapsulate. The eigenvector associated with the highest eigenvalue represents the most significant principal component, followed by subsequent components in decreasing order of importance, allowing for a systematic determination of the most influential axes within the dataset.\nWe can use numpy’s linalg.eigh( ) method to calcuate eigenvectors and eigenvalues a matrix.\n\neigen_values, eigen_vectors = np.linalg.eigh(cov_mat)\n\n\neigen_values\n\narray([0.02747434, 0.19865996, 0.28077   , 0.350529  , 0.55060199,\n       1.57801295, 4.01395176])\n\n\n\neigen_vectors\n\narray([[ 0.80082487, -0.16970508, -0.14597659, -0.0389558 ,  0.26908802,\n        -0.06893769, -0.47947078],\n       [ 0.01784783, -0.22479246,  0.02282818,  0.36374887, -0.81826935,\n        -0.0765846 , -0.37534719],\n       [-0.15285774,  0.7837677 , -0.35479821, -0.14834351, -0.08293253,\n        -0.03345835, -0.45437635],\n       [-0.14247844,  0.08510479,  0.85646854, -0.1712655 ,  0.12616845,\n        -0.04237473, -0.4380328 ],\n       [-0.55875371, -0.50401185, -0.33611019, -0.03948141,  0.31798812,\n        -0.0961294 , -0.45703414],\n       [ 0.04126619, -0.11577348, -0.04214531, -0.65639617, -0.25614247,\n        -0.6852266 ,  0.1308319 ],\n       [-0.02804966,  0.17805184,  0.06327152,  0.61839859,  0.26173503,\n        -0.71252436,  0.06996048]])\n\n\nWe should now sort the Eigenvalues in the descending order along with their corresponding Eigenvector.\n\n# sort the eigenvalues in descending order\nsorted_index = np.argsort(eigen_values)[::-1]\n\nsorted_eigenvalue = eigen_values[sorted_index]\n\n# Similarly sort the eigenvectors\nsorted_eigenvectors = eigen_vectors[:, sorted_index]\n\n\n\nStep 4. Feature Vector\nOnce sorted, we can select the subset of the Eigenvalue as per our requirement. In this case, since we are interested in the two principal components we take the first two values (i.e. n_components = 2). This forms our feature vector which is a matrix that has as columns the eigenvectors of the components that we decide to keep. By only keeping a subset of the Eigenvectors we are reducing the number of features hence the notion of dimensionality reduction.\n\nn_components = 2 \nfeature_vector = sorted_eigenvectors[:,0:n_components]\n\nfeature_vector\n\narray([[-0.47947078, -0.06893769],\n       [-0.37534719, -0.0765846 ],\n       [-0.45437635, -0.03345835],\n       [-0.4380328 , -0.04237473],\n       [-0.45703414, -0.0961294 ],\n       [ 0.1308319 , -0.6852266 ],\n       [ 0.06996048, -0.71252436]])\n\n\n\n\nStep 5. Recast the Data\n\ncar_crash_reduced = -np.dot(\n    feature_vector.transpose(), car_crashes_std.transpose()\n).transpose()\ncars_pca = pd.DataFrame(data=car_crash_reduced, columns=[\"PC 1\", \"PC 2\"])\ncars_pca.head()\n\n\n\n\n\n\n\n\nPC 1\nPC 2\n\n\n\n\n0\n1.587871\n0.132134\n\n\n1\n1.132939\n0.849778\n\n\n2\n1.418062\n-0.416363\n\n\n3\n2.467035\n0.345530\n\n\n4\n-1.733390\n0.627382\n\n\n\n\n\n\n\n\n\nUsing sklearn package\nThe sklearn package comes with an API to calculate PCA without having to follow the above steps\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\n\npcs = pca.fit_transform(car_crashes_std)\n\nprincipal_cars = pd.DataFrame(data=pcs, columns=[\"PC 1\", \"PC 2\"])\n\nprincipal_cars.head()\n\n\n\n\n\n\n\n\nPC 1\nPC 2\n\n\n\n\n0\n1.587871\n0.132134\n\n\n1\n1.132939\n0.849778\n\n\n2\n1.418062\n-0.416363\n\n\n3\n2.467035\n0.345530\n\n\n4\n-1.733390\n0.627382"
  },
  {
    "objectID": "posts/pca.html#computing-pca",
    "href": "posts/pca.html#computing-pca",
    "title": "Principal Component Analysis",
    "section": "Computing PCA",
    "text": "Computing PCA\nCalculating PCA involves five steps. These are\n\nStandardization\nCovariance Matrix Computation\nEigenvectors and Eigenvalues\nFeature Vector\nRecast the Data\n\nNow that we’ve explained the data we’ll use for demonstrating PCA and the steps involved, let’s go through each step with the aforementioned data and see PCA in action."
  },
  {
    "objectID": "posts/seaborn_demo.html",
    "href": "posts/seaborn_demo.html",
    "title": "Seaborn Examples",
    "section": "",
    "text": "Seaborn python package build on top of matplotlib for plotting statistical data. It has great support for the commonly used pandas data structure in handling tabular data. It’s more opinionated than matplotlib which makes it easier to get started with and provides higher level API for plotting which will help us create plots with fewer number of lines and configuration that we need in matplotlib for the same graphics.\n\nVisualizing data using tools like seaborn help us explore and understand the data. In this post we’ll explore some of the available graphics demonstrate how we can use to plot different data types we might have.\n\nTo help with the clarity of this post, I’ve organized it into six sections. Following this introductory section, we’ll see the prerequisite setup and imports. In the setup section I’ll discuss the sample datasets that come with seaborn. The third and fourth sections are about seaborn plots for Categorical and Continues Data, respectively. Then I’ll discuss how we can plot comparative plots followed by a section on utilities such as styling and saving plots."
  },
  {
    "objectID": "posts/seaborn_demo.html#setup",
    "href": "posts/seaborn_demo.html#setup",
    "title": "Seaborn Examples",
    "section": "Setup",
    "text": "Setup\n\nInstallation\nFirst step is t install the seaborn package. For that, we can use pip as\n\n%pip install seaborn\n\nRequirement already satisfied: seaborn in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (0.13.0)\nRequirement already satisfied: numpy!=1.24.0,&gt;=1.20 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from seaborn) (1.23.5)\nRequirement already satisfied: pandas&gt;=1.2 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from seaborn) (2.0.3)\nRequirement already satisfied: matplotlib!=3.6.1,&gt;=3.3 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from seaborn) (3.7.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.0.5)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (4.25.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.4.4)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (23.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (9.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2022.7)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in /Users/yoseph/anaconda3/envs/CS5805/lib/python3.11/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib!=3.6.1,&gt;=3.3-&gt;seaborn) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nImports\nTo get started with seaborn we need to import the following packages. We import pyplot form matplotlib to use the lower level APIs later for styling and customization. While working with Jupyter notebooks we might want to suppers warnings in the output.\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\n\n\nJupyter Configurations\nAsk Jupyter to display plots within the notebook\n\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\n\nSuppress warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\nData\nPlotting only makes sense if we have some data to visualize. We can plot any data stored in pandas data structure using seaborn. This let’s us load and process data using pandas and use seaborn for visualization in parallel. For this post, I’ll make use of sample datasets shipped with seaborn. To get the full list of datasets available in our installation we can as seaborn itself\n\nprint(sns.get_dataset_names())\n\n['anagrams', 'anscombe', 'attention', 'brain_networks', 'car_crashes', 'diamonds', 'dots', 'dowjones', 'exercise', 'flights', 'fmri', 'geyser', 'glue', 'healthexp', 'iris', 'mpg', 'penguins', 'planets', 'seaice', 'taxis', 'tips', 'titanic']\n\n\nLet’s load two datasets and examine their content. More information on the datasets can be found here and here\n\ncrashes = sns.load_dataset('car_crashes')\ntitanic = sns.load_dataset('titanic')\n\nExamine the content of the data\n\ncrashes.head()\n\n\n\n\n\n\n\n\ntotal\nspeeding\nalcohol\nnot_distracted\nno_previous\nins_premium\nins_losses\nabbrev\n\n\n\n\n0\n18.8\n7.332\n5.640\n18.048\n15.040\n784.55\n145.08\nAL\n\n\n1\n18.1\n7.421\n4.525\n16.290\n17.014\n1053.48\n133.93\nAK\n\n\n2\n18.6\n6.510\n5.208\n15.624\n17.856\n899.47\n110.35\nAZ\n\n\n3\n22.4\n4.032\n5.824\n21.056\n21.280\n827.34\n142.39\nAR\n\n\n4\n12.0\n4.200\n3.360\n10.920\n10.680\n878.41\n165.63\nCA\n\n\n\n\n\n\n\n\ntitanic.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue"
  },
  {
    "objectID": "posts/seaborn_demo.html#categorical-data",
    "href": "posts/seaborn_demo.html#categorical-data",
    "title": "Seaborn Examples",
    "section": "Categorical Data",
    "text": "Categorical Data\nCategorical data is form of qualitative data that can be stored and identified based on distinct labels. Instead of being measured numerically, it is a type of information that can be grouped into categories. For example, the sex column in our titanic dataset has two labels: male and female. Similarly the abbrev column is a list of state name abbreviations in the US which has a total of 50 labels.  In this section, we will see bar and count plots from seaborn as two graphics to visualize categorical data.\n\nIn this section, we will see bar and count plots from seaborn as two graphics to visualize categorical data.\n\nBar Plots\nAlso referred to as bar chart, bar plot is, a bar plot allows us to visualize the comparisons between the discrete labels or categories in our data. Bar chart is a graph that represents the category of data with horizontally or vertically rectangular bars with lengths and heights that is proportional to the values which they represent. One of the axis of the plot represents the specific categories being compared, while the other axis represents the measured values corresponding to those categories.\n\nsns.barplot(data = titanic, x = 'sex', y='fare')\n\n&lt;Axes: xlabel='sex', ylabel='fare'&gt;\n\n\n\n\n\nBy Default the data is aggregated by mean of y\n\nsns.barplot(data = titanic, x = 'sex', y='fare', estimator=np.median)\n\n&lt;Axes: xlabel='sex', ylabel='fare'&gt;\n\n\n\n\n\n\n\nCount Plots\nSimilar to bar plot but uses count as the estimator\n\nsns.countplot(data = titanic, x = 'sex')\n\n&lt;Axes: xlabel='sex', ylabel='count'&gt;\n\n\n\n\n\nWe can display a count of the number rows in each category of values in the alive column\n\nsns.countplot(data = titanic, x = 'alive')\n\n&lt;Axes: xlabel='alive', ylabel='count'&gt;"
  },
  {
    "objectID": "posts/seaborn_demo.html#distribution-plots",
    "href": "posts/seaborn_demo.html#distribution-plots",
    "title": "Seaborn Examples",
    "section": "Distribution Plots",
    "text": "Distribution Plots\nDistribution plots for continuous data variables.In the past, Seaborn had a distplot method which supported displaying a histogram plot by with kde on top default. distplot is deprecated and it is recommended we use displot or histplot for find grained control. distplot allow as to display a histogram of univariate or bivariate distribution of the data in a dataset.\n\nsns.distplot(a=crashes['alcohol'])\n \n\n&lt;Axes: xlabel='alcohol', ylabel='Density'&gt;\n\n\n\n\n\nIf we don’t what the kde plot to be visible we can tell seaborn not to show it\n\nsns.distplot(a=crashes['alcohol'],kde=False)\n\n&lt;Axes: xlabel='alcohol'&gt;\n\n\n\n\n\n\nsns.displot(data=crashes['alcohol'])\n\n\n\n\nThe equivalent plot can be displayed using the new displot method\n\n# We can specify name of the data column in the dataset if there are more than one\nsns.displot(data=crashes, x='alcohol')\n\n\n\n\ndisplot combines a histogram with optional components, such as a Kernel Density Estimation (KDE) line or rug plot. We can specify which type we want to plot using the kind key (default is hist)\n\nsns.displot(data=crashes['alcohol'], kind='kde')\n\n\n\n\nWe can enable an overlay of other visualization on top of the default. We can do this by passing a boolean value for the parameters hist, ecdf, kde, rug\n\nsns.displot(data=crashes['alcohol'], kde=True, rug=True)\n\n\n\n\n\nsns.displot(data=titanic, x='age', hue='sex', kind='kde', multiple='stack')\n\n\n\n\n\nKDE Plot\nWe can display kde plots using the kdeplot function as well\n\nsns.kdeplot(data=titanic, x='fare')\n\n&lt;Axes: xlabel='fare', ylabel='Density'&gt;"
  },
  {
    "objectID": "posts/seaborn_demo.html#categorical-plots",
    "href": "posts/seaborn_demo.html#categorical-plots",
    "title": "Seaborn Examples",
    "section": "Categorical Plots",
    "text": "Categorical Plots\n\nBox Plot\nCompare different variables\n\nsns.boxplot(data = titanic, x = 'alive', y='fare', hue='sex')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\n\nViolin Plot\nCompare different variables in a different visualization\n\nsns.violinplot(data = titanic, x = 'alive', y='fare', hue='sex')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.violinplot(data = titanic, x = 'alive', y='fare', hue='sex', split=True)\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\n#survived   pclass  sex age sibsp   parch   fare    embarked    class   who adult_male  deck    embark_town alive   alone\nsns.violinplot(data = titanic, x = 'alive', y='fare')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\n\nStrip Plot\n\nsns.stripplot(data = titanic, x = 'class', y='fare')\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.stripplot(data = titanic, x = 'class', y='fare', hue='sex')\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\n\nSwarm Plot\n\nsns.swarmplot(data = titanic, x = 'alive', y='fare')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;"
  },
  {
    "objectID": "posts/seaborn_demo.html#comparing-data",
    "href": "posts/seaborn_demo.html#comparing-data",
    "title": "Seaborn Examples",
    "section": "Comparing Data",
    "text": "Comparing Data\n\nsns.displot(data=titanic, x='age', col='survived',  kind='kde')\n\n\n\n\n\nsns.displot(data=titanic, x='age', col='survived', hue='sex', kind='kde', multiple='stack')\n\n\n\n\n\nJoint Plot\nUsed for comparing two distributions. By default it uses scatter plot\n\nsns.jointplot(data=crashes, x='speeding',y='alcohol')\n\n\n\n\n\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='kde')\n\n\n\n\n\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg')\n\n\n\n\n\nsns.jointplot(data=titanic, x='fare',y='age')\n\n\n\n\n\n\nPair Plot\nWe can display pair plots across the entire dataset for each pair of numeric attributes\n\nsns.pairplot(data=crashes)\n\n\n\n\nWe can use hue to have color palettes of categorical data\n\nsns.pairplot(data=titanic, hue='sex')\n\n\n\n\n\n\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\n\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.stripplot(data = titanic, x = 'class', y='fare', hue='sex',jitter=True)\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.stripplot(data = titanic, x = 'class', y='fare', hue='sex',jitter=True, dodge=True)\n\n&lt;Axes: xlabel='class', ylabel='fare'&gt;\n\n\n\n\n\n\n\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;\n\n\n\n\n\n\nsns.swarmplot(data = titanic, x = 'alive', y='fare', color='red')\n\n&lt;Axes: xlabel='alive', ylabel='fare'&gt;"
  },
  {
    "objectID": "posts/seaborn_demo.html#other-feature",
    "href": "posts/seaborn_demo.html#other-feature",
    "title": "Seaborn Examples",
    "section": "Other Feature",
    "text": "Other Feature\n\nResizing\nWe can resize the plot using height, width and aspect parameters\n\nsns.displot(data = crashes, x = 'total', height = 2 , aspect = 1.6)\n\n\n\n\n\n\nStyling\n\nsns.set_style('darkgrid')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\nsns.set_style('whitegrid')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\nsns.set_style('ticks')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\n\nLabel Styling\n\nsns.set_context('poster')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\nsns.set_context('paper')\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\n\n\n\n\n\nsns.jointplot(data=crashes, x='speeding',y='alcohol', kind='reg', height = 4 )\nsns.despine(left=True, bottom=True) # False turns off the boundary \n\n\n\n\n\n\nSave Plot\nSince seaboarn is built on top of the matplotlib package, we can use matplotlib’s savefig() function to save the generated plot into image file.\nNote: The savefig() function should come before the show() function since the later closes and deletes the image from the memory to save space.\n\nsns.displot(crashes['alcohol'])\nplt.savefig('picture.png')\nplt.show()"
  }
]