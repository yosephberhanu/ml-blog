[
  {
    "objectID": "posts/regression.html",
    "href": "posts/regression.html",
    "title": "Song popularity prediction",
    "section": "",
    "text": "In this blog post I’ll try to explore linear and nonlinear regression ML algorithms to predict popularity of a song based on a data set of almost 30,000 songs from Spotify API collected using the spotifyr package. The package and orgnial data set can be found here.\n\n\nThis dataset encompasses various attributes providing diverse information about songs, artists, popularity, musical features, and playlist characteristics, making it suitable for music-related analyses or applications in data analysis and machine learning.\n\nTrack_id: Unique ID for a song\nTrack_name: Name of the song\nTrack_artist: Artist of the song\nTrack_popularity: Song popularity rating (0-100)\nTrack_album_id: Unique ID for the album\nTrack_album_name: Name of the album\nTrack_album_release_date: Release date of the album\nPlaylist_name: Name of the playlist\nPlaylist_id: ID of the playlist\nPlaylist_genre: Genre of the playlist\nPlaylist_subgenre: Subgenre of the playlist\nDanceability: Describes how suitable a track is for dancing (0.0 to 1.0)\nEnergy: Represents intensity and activity of the track (0.0 to 1.0)\nKey: Estimated overall key of the track\nLoudness: Overall loudness of a track in decibels (dB)\nMode: Indicates the modality (major or minor) of a track (0 for minor, 1 for major)\nSpeechiness: Detects the presence of spoken words in a track (0.0 to 1.0)\nAcousticness: Confidence measure of whether the track is acoustic (0.0 to 1.0)\nInstrumentalness: Predicts whether a track contains no vocals (0.0 to 1.0)\nLiveness: Detects the presence of an audience in the recording\nValence: Describes the musical positiveness conveyed by a track (0.0 to 1.0)\nTempo: Estimated tempo of a track in beats per minute (BPM)\nDuration_ms: Duration of the song in milliseconds\n\n\n\n\n\nThe task for this blog post is to train a couple of regression models and predict how pouplar a song is given the other features about the song.\n# Preprocessing ## Imports\n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import (\n    AdaBoostRegressor,\n    RandomForestRegressor,\n    GradientBoostingRegressor,\n    BaggingRegressor,\n    ExtraTreesRegressor,\n)\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neighbors import RadiusNeighborsRegressor\nfrom sklearn import svm\n\nfrom sklearn.metrics import mean_squared_error\n\n\n\n\n\nsongs = pd.read_csv(\"../data/spotify_songs.csv\")\nsongs.head()\n\n\n\n\n\n\n\n\ntrack_id\ntrack_name\ntrack_artist\ntrack_popularity\ntrack_album_id\ntrack_album_name\ntrack_album_release_date\nplaylist_name\nplaylist_id\nplaylist_genre\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\n\n\n\n\n0\n6f807x0ima9a1j3VPbc7VN\nI Don't Care (with Justin Bieber) - Loud Luxur...\nEd Sheeran\n66\n2oCs0DGTsRO98Gh5ZSl2Cx\nI Don't Care (with Justin Bieber) [Loud Luxury...\n2019-06-14\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n6\n-2.634\n1\n0.0583\n0.1020\n0.000000\n0.0653\n0.518\n122.036\n194754\n\n\n1\n0r7CVbZTWZgbTCYdfa2P31\nMemories - Dillon Francis Remix\nMaroon 5\n67\n63rPSO264uRjW1X5E6cWv6\nMemories (Dillon Francis Remix)\n2019-12-13\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n11\n-4.969\n1\n0.0373\n0.0724\n0.004210\n0.3570\n0.693\n99.972\n162600\n\n\n2\n1z1Hg7Vb0AhHDiEmnDE79l\nAll the Time - Don Diablo Remix\nZara Larsson\n70\n1HoSmj2eLcsrR0vE9gThr4\nAll the Time (Don Diablo Remix)\n2019-07-05\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n1\n-3.432\n0\n0.0742\n0.0794\n0.000023\n0.1100\n0.613\n124.008\n176616\n\n\n3\n75FpbthrwQmzHlBJLuGdC7\nCall You Mine - Keanu Silva Remix\nThe Chainsmokers\n60\n1nqYsOef1yKKuGOVchbsk6\nCall You Mine - The Remixes\n2019-07-19\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n7\n-3.778\n1\n0.1020\n0.0287\n0.000009\n0.2040\n0.277\n121.956\n169093\n\n\n4\n1e8PAfcKUYoKkxPhrHqw4x\nSomeone You Loved - Future Humans Remix\nLewis Capaldi\n69\n7m7vv9wlQ4i0LFuJiE2zsQ\nSomeone You Loved (Future Humans Remix)\n2019-03-05\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n1\n-4.672\n1\n0.0359\n0.0803\n0.000000\n0.0833\n0.725\n123.976\n189052\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n\n\nFeatures like track_id, track_name, track_artist, track_album_id, track_album_name, playlist_id,playlist_name will have little contribution to our model as they are unique identifiers. Hence we’ll remove them from the data.\n\nsongs = songs.drop(\n    [\n        \"track_id\",\n        \"track_name\",\n        \"track_artist\",\n        \"track_album_id\",\n        \"track_album_name\",\n        \"playlist_id\",\n        \"playlist_name\",\n    ],\n    axis=\"columns\",\n)\nsongs.head()\n\n\n\n\n\n\n\n\ntrack_popularity\ntrack_album_release_date\nplaylist_genre\nplaylist_subgenre\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\n\n\n\n\n0\n66\n2019-06-14\npop\ndance pop\n0.748\n0.916\n6\n-2.634\n1\n0.0583\n0.1020\n0.000000\n0.0653\n0.518\n122.036\n194754\n\n\n1\n67\n2019-12-13\npop\ndance pop\n0.726\n0.815\n11\n-4.969\n1\n0.0373\n0.0724\n0.004210\n0.3570\n0.693\n99.972\n162600\n\n\n2\n70\n2019-07-05\npop\ndance pop\n0.675\n0.931\n1\n-3.432\n0\n0.0742\n0.0794\n0.000023\n0.1100\n0.613\n124.008\n176616\n\n\n3\n60\n2019-07-19\npop\ndance pop\n0.718\n0.930\n7\n-3.778\n1\n0.1020\n0.0287\n0.000009\n0.2040\n0.277\n121.956\n169093\n\n\n4\n69\n2019-03-05\npop\ndance pop\n0.650\n0.833\n1\n-4.672\n1\n0.0359\n0.0803\n0.000000\n0.0833\n0.725\n123.976\n189052\n\n\n\n\n\n\n\n\n\n\nTo capture seasonal popularity I’ll split the date value into three columns with year, month and day values. We first convert the string representation of track_album_release_date to date datatype and takeout the individual values separately.\n\nsongs[\"track_album_release_date\"] = pd.to_datetime(\n    songs[\"track_album_release_date\"], format=\"%Y-%m-%d\", errors=\"coerce\"\n)\n\nsongs[\"release_year\"] = songs[\"track_album_release_date\"].dt.year\nsongs[\"release_month\"] = songs[\"track_album_release_date\"].dt.month\nsongs[\"release_day\"] = songs[\"track_album_release_date\"].dt.day\nsongs = songs.drop([\"track_album_release_date\"], axis=\"columns\")\n\n\n\n\nLet’s check if there are any missing values in our dataset\n\nsongs.isnull().values.any()\n\nTrue\n\n\nIt seems we’ve some\n\nsongs.isnull().sum()\n\ntrack_popularity        0\nplaylist_genre          0\nplaylist_subgenre       0\ndanceability            0\nenergy                  0\nkey                     0\nloudness                0\nmode                    0\nspeechiness             0\nacousticness            0\ninstrumentalness        0\nliveness                0\nvalence                 0\ntempo                   0\nduration_ms             0\nrelease_year         1886\nrelease_month        1886\nrelease_day          1886\ndtype: int64\n\n\nGiven the missing date values are arround 17% our data simply dropping the rows does not yeild an ideal outcome. Hence, let’s to fill the missing columns with the most common value in that feature\n\nsongs[\"release_year\"].fillna(\n    songs[\"release_year\"].value_counts().index[0], inplace=True\n)\nsongs[\"release_month\"].fillna(\n    songs[\"release_month\"].value_counts().index[0], inplace=True\n)\nsongs[\"release_day\"].fillna(songs[\"release_day\"].value_counts().index[0], inplace=True)\nsongs\n\n\n\n\n\n\n\n\ntrack_popularity\nplaylist_genre\nplaylist_subgenre\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\nrelease_year\nrelease_month\nrelease_day\n\n\n\n\n0\n66\npop\ndance pop\n0.748\n0.916\n6\n-2.634\n1\n0.0583\n0.102000\n0.000000\n0.0653\n0.5180\n122.036\n194754\n2019.0\n6.0\n14.0\n\n\n1\n67\npop\ndance pop\n0.726\n0.815\n11\n-4.969\n1\n0.0373\n0.072400\n0.004210\n0.3570\n0.6930\n99.972\n162600\n2019.0\n12.0\n13.0\n\n\n2\n70\npop\ndance pop\n0.675\n0.931\n1\n-3.432\n0\n0.0742\n0.079400\n0.000023\n0.1100\n0.6130\n124.008\n176616\n2019.0\n7.0\n5.0\n\n\n3\n60\npop\ndance pop\n0.718\n0.930\n7\n-3.778\n1\n0.1020\n0.028700\n0.000009\n0.2040\n0.2770\n121.956\n169093\n2019.0\n7.0\n19.0\n\n\n4\n69\npop\ndance pop\n0.650\n0.833\n1\n-4.672\n1\n0.0359\n0.080300\n0.000000\n0.0833\n0.7250\n123.976\n189052\n2019.0\n3.0\n5.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32828\n42\nedm\nprogressive electro house\n0.428\n0.922\n2\n-1.814\n1\n0.0936\n0.076600\n0.000000\n0.0668\n0.2100\n128.170\n204375\n2014.0\n4.0\n28.0\n\n\n32829\n20\nedm\nprogressive electro house\n0.522\n0.786\n0\n-4.462\n1\n0.0420\n0.001710\n0.004270\n0.3750\n0.4000\n128.041\n353120\n2013.0\n3.0\n8.0\n\n\n32830\n14\nedm\nprogressive electro house\n0.529\n0.821\n6\n-4.899\n0\n0.0481\n0.108000\n0.000001\n0.1500\n0.4360\n127.989\n210112\n2014.0\n4.0\n21.0\n\n\n32831\n15\nedm\nprogressive electro house\n0.626\n0.888\n2\n-3.361\n1\n0.1090\n0.007920\n0.127000\n0.3430\n0.3080\n128.008\n367432\n2014.0\n1.0\n1.0\n\n\n32832\n27\nedm\nprogressive electro house\n0.603\n0.884\n5\n-4.571\n0\n0.0385\n0.000133\n0.341000\n0.7420\n0.0894\n127.984\n337500\n2014.0\n3.0\n3.0\n\n\n\n\n32833 rows × 18 columns\n\n\n\nIf we check once more, we see there are no missing values anymore\n\nsongs.isnull().values.any()\n\nFalse\n\n\n\n\n\nLet’s visualize the data and get a better understanding of the values ### Genre\n\nsns.countplot(songs, x=\"playlist_subgenre\", hue=\"playlist_genre\")\nplt.xticks(rotation=90)\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n        17, 18, 19, 20, 21, 22, 23]),\n [Text(0, 0, 'dance pop'),\n  Text(1, 0, 'post-teen pop'),\n  Text(2, 0, 'electropop'),\n  Text(3, 0, 'indie poptimism'),\n  Text(4, 0, 'hip hop'),\n  Text(5, 0, 'southern hip hop'),\n  Text(6, 0, 'gangster rap'),\n  Text(7, 0, 'trap'),\n  Text(8, 0, 'album rock'),\n  Text(9, 0, 'classic rock'),\n  Text(10, 0, 'permanent wave'),\n  Text(11, 0, 'hard rock'),\n  Text(12, 0, 'tropical'),\n  Text(13, 0, 'latin pop'),\n  Text(14, 0, 'reggaeton'),\n  Text(15, 0, 'latin hip hop'),\n  Text(16, 0, 'urban contemporary'),\n  Text(17, 0, 'hip pop'),\n  Text(18, 0, 'new jack swing'),\n  Text(19, 0, 'neo soul'),\n  Text(20, 0, 'electro house'),\n  Text(21, 0, 'big room'),\n  Text(22, 0, 'pop edm'),\n  Text(23, 0, 'progressive electro house')])\n\n\n\n\n\nFrom this we can see playlist_subgenre is full contained in playlist_genre hence we can remote the playlist_genre as the playlist_subgenre has more fine grained information\n\nsongs = songs.drop(\"playlist_genre\", axis=\"columns\")\n\n\n\n\nNotice how we have a categorical feature (i.e., playlist_subgenre), we need to one-hot encode this column.\n\nplaylist_subgenre = pd.get_dummies(songs[\"playlist_subgenre\"])\nsongs = pd.concat([songs, playlist_subgenre], axis=\"columns\")\nsongs = songs.drop(\"playlist_subgenre\", axis=\"columns\")\nsongs\n\n\n\n\n\n\n\n\ntrack_popularity\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\n...\nnew jack swing\npermanent wave\npop edm\npost-teen pop\nprogressive electro house\nreggaeton\nsouthern hip hop\ntrap\ntropical\nurban contemporary\n\n\n\n\n0\n66\n0.748\n0.916\n6\n-2.634\n1\n0.0583\n0.102000\n0.000000\n0.0653\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n67\n0.726\n0.815\n11\n-4.969\n1\n0.0373\n0.072400\n0.004210\n0.3570\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n70\n0.675\n0.931\n1\n-3.432\n0\n0.0742\n0.079400\n0.000023\n0.1100\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n60\n0.718\n0.930\n7\n-3.778\n1\n0.1020\n0.028700\n0.000009\n0.2040\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n69\n0.650\n0.833\n1\n-4.672\n1\n0.0359\n0.080300\n0.000000\n0.0833\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32828\n42\n0.428\n0.922\n2\n-1.814\n1\n0.0936\n0.076600\n0.000000\n0.0668\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32829\n20\n0.522\n0.786\n0\n-4.462\n1\n0.0420\n0.001710\n0.004270\n0.3750\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32830\n14\n0.529\n0.821\n6\n-4.899\n0\n0.0481\n0.108000\n0.000001\n0.1500\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32831\n15\n0.626\n0.888\n2\n-3.361\n1\n0.1090\n0.007920\n0.127000\n0.3430\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32832\n27\n0.603\n0.884\n5\n-4.571\n0\n0.0385\n0.000133\n0.341000\n0.7420\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n32833 rows × 40 columns\n\n\n\n\n\n\ncor = songs.corr()\nsns.heatmap(cor)\ncor = cor.abs()\n\n\n\n\nLet’s see the top 5 correclations\n\ncor_values = (\n    cor.where(np.triu(np.ones(cor.shape), k=1).astype(bool))\n    .stack()\n    .sort_values(ascending=False)\n)\ntop_n = 5\nprint(\"Top 5 correlation values are\")\nfor i, v in cor_values[:top_n].items():\n    print(\"{} - {} \".format(i, v))\n\nTop 5 correlation values are\n('energy', 'loudness') - 0.6766245234442312 \n('energy', 'acousticness') - 0.5397446301909388 \n('loudness', 'acousticness') - 0.36163816507093 \n('danceability', 'valence') - 0.3305232570910807 \n('release_month', 'release_day') - 0.3080926045288983 \n\n\nAs expected ‘energy’, ‘loudness’ are correlated to a degree but none of these correlation values are significantly high enought to warrant removal befor the training so we keep all our features.\n\n\n\n\nLet’s now split features into X and Y with the later being our target for prediction\n\ny = songs[\"track_popularity\"].to_numpy()\nx = songs.drop(\"track_popularity\", axis=\"columns\").to_numpy()\n\n\n\n\nNormalize the features to bring them all to 0-1 range\n\nx_normalized = preprocessing.normalize(x)\n\n\n\n\nLet’s split our data into training and test sets with 80 to 20 ratio.\n\nx_train, x_test, y_train, y_test = train_test_split(x_normalized, y, test_size=0.20)\nprint(\"Training X Shape: {}\".format(x_train.shape))\nprint(\"Training Y Shape: {}\".format(y_train.shape))\nprint(\"Test X Shape: {}\".format(x_test.shape))\nprint(\"Test y Shape: {}\".format(y_test.shape))\n\nTraining X Shape: (26266, 39)\nTraining Y Shape: (26266,)\nTest X Shape: (6567, 39)\nTest y Shape: (6567,)"
  },
  {
    "objectID": "posts/regression.html#dataset",
    "href": "posts/regression.html#dataset",
    "title": "Song popularity prediction",
    "section": "",
    "text": "In this blog post I’ll try to explore linear and nonlinear regression ML algorithms to predict popularity of a song based on a data set of almost 30,000 songs from Spotify API collected using the spotifyr package. The package and orgnial data set can be found here.\n\n\nThis dataset encompasses various attributes providing diverse information about songs, artists, popularity, musical features, and playlist characteristics, making it suitable for music-related analyses or applications in data analysis and machine learning.\n\nTrack_id: Unique ID for a song\nTrack_name: Name of the song\nTrack_artist: Artist of the song\nTrack_popularity: Song popularity rating (0-100)\nTrack_album_id: Unique ID for the album\nTrack_album_name: Name of the album\nTrack_album_release_date: Release date of the album\nPlaylist_name: Name of the playlist\nPlaylist_id: ID of the playlist\nPlaylist_genre: Genre of the playlist\nPlaylist_subgenre: Subgenre of the playlist\nDanceability: Describes how suitable a track is for dancing (0.0 to 1.0)\nEnergy: Represents intensity and activity of the track (0.0 to 1.0)\nKey: Estimated overall key of the track\nLoudness: Overall loudness of a track in decibels (dB)\nMode: Indicates the modality (major or minor) of a track (0 for minor, 1 for major)\nSpeechiness: Detects the presence of spoken words in a track (0.0 to 1.0)\nAcousticness: Confidence measure of whether the track is acoustic (0.0 to 1.0)\nInstrumentalness: Predicts whether a track contains no vocals (0.0 to 1.0)\nLiveness: Detects the presence of an audience in the recording\nValence: Describes the musical positiveness conveyed by a track (0.0 to 1.0)\nTempo: Estimated tempo of a track in beats per minute (BPM)\nDuration_ms: Duration of the song in milliseconds"
  },
  {
    "objectID": "posts/regression.html#task",
    "href": "posts/regression.html#task",
    "title": "Song popularity prediction",
    "section": "",
    "text": "The task for this blog post is to train a couple of regression models and predict how pouplar a song is given the other features about the song.\n# Preprocessing ## Imports\n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import (\n    AdaBoostRegressor,\n    RandomForestRegressor,\n    GradientBoostingRegressor,\n    BaggingRegressor,\n    ExtraTreesRegressor,\n)\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neighbors import RadiusNeighborsRegressor\nfrom sklearn import svm\n\nfrom sklearn.metrics import mean_squared_error"
  },
  {
    "objectID": "posts/regression.html#load-the-data",
    "href": "posts/regression.html#load-the-data",
    "title": "Song popularity prediction",
    "section": "",
    "text": "songs = pd.read_csv(\"../data/spotify_songs.csv\")\nsongs.head()\n\n\n\n\n\n\n\n\ntrack_id\ntrack_name\ntrack_artist\ntrack_popularity\ntrack_album_id\ntrack_album_name\ntrack_album_release_date\nplaylist_name\nplaylist_id\nplaylist_genre\n...\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\n\n\n\n\n0\n6f807x0ima9a1j3VPbc7VN\nI Don't Care (with Justin Bieber) - Loud Luxur...\nEd Sheeran\n66\n2oCs0DGTsRO98Gh5ZSl2Cx\nI Don't Care (with Justin Bieber) [Loud Luxury...\n2019-06-14\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n6\n-2.634\n1\n0.0583\n0.1020\n0.000000\n0.0653\n0.518\n122.036\n194754\n\n\n1\n0r7CVbZTWZgbTCYdfa2P31\nMemories - Dillon Francis Remix\nMaroon 5\n67\n63rPSO264uRjW1X5E6cWv6\nMemories (Dillon Francis Remix)\n2019-12-13\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n11\n-4.969\n1\n0.0373\n0.0724\n0.004210\n0.3570\n0.693\n99.972\n162600\n\n\n2\n1z1Hg7Vb0AhHDiEmnDE79l\nAll the Time - Don Diablo Remix\nZara Larsson\n70\n1HoSmj2eLcsrR0vE9gThr4\nAll the Time (Don Diablo Remix)\n2019-07-05\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n1\n-3.432\n0\n0.0742\n0.0794\n0.000023\n0.1100\n0.613\n124.008\n176616\n\n\n3\n75FpbthrwQmzHlBJLuGdC7\nCall You Mine - Keanu Silva Remix\nThe Chainsmokers\n60\n1nqYsOef1yKKuGOVchbsk6\nCall You Mine - The Remixes\n2019-07-19\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n7\n-3.778\n1\n0.1020\n0.0287\n0.000009\n0.2040\n0.277\n121.956\n169093\n\n\n4\n1e8PAfcKUYoKkxPhrHqw4x\nSomeone You Loved - Future Humans Remix\nLewis Capaldi\n69\n7m7vv9wlQ4i0LFuJiE2zsQ\nSomeone You Loved (Future Humans Remix)\n2019-03-05\nPop Remix\n37i9dQZF1DXcZDD7cfEKhW\npop\n...\n1\n-4.672\n1\n0.0359\n0.0803\n0.000000\n0.0833\n0.725\n123.976\n189052\n\n\n\n\n5 rows × 23 columns"
  },
  {
    "objectID": "posts/regression.html#remove-identifiers",
    "href": "posts/regression.html#remove-identifiers",
    "title": "Song popularity prediction",
    "section": "",
    "text": "Features like track_id, track_name, track_artist, track_album_id, track_album_name, playlist_id,playlist_name will have little contribution to our model as they are unique identifiers. Hence we’ll remove them from the data.\n\nsongs = songs.drop(\n    [\n        \"track_id\",\n        \"track_name\",\n        \"track_artist\",\n        \"track_album_id\",\n        \"track_album_name\",\n        \"playlist_id\",\n        \"playlist_name\",\n    ],\n    axis=\"columns\",\n)\nsongs.head()\n\n\n\n\n\n\n\n\ntrack_popularity\ntrack_album_release_date\nplaylist_genre\nplaylist_subgenre\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\n\n\n\n\n0\n66\n2019-06-14\npop\ndance pop\n0.748\n0.916\n6\n-2.634\n1\n0.0583\n0.1020\n0.000000\n0.0653\n0.518\n122.036\n194754\n\n\n1\n67\n2019-12-13\npop\ndance pop\n0.726\n0.815\n11\n-4.969\n1\n0.0373\n0.0724\n0.004210\n0.3570\n0.693\n99.972\n162600\n\n\n2\n70\n2019-07-05\npop\ndance pop\n0.675\n0.931\n1\n-3.432\n0\n0.0742\n0.0794\n0.000023\n0.1100\n0.613\n124.008\n176616\n\n\n3\n60\n2019-07-19\npop\ndance pop\n0.718\n0.930\n7\n-3.778\n1\n0.1020\n0.0287\n0.000009\n0.2040\n0.277\n121.956\n169093\n\n\n4\n69\n2019-03-05\npop\ndance pop\n0.650\n0.833\n1\n-4.672\n1\n0.0359\n0.0803\n0.000000\n0.0833\n0.725\n123.976\n189052"
  },
  {
    "objectID": "posts/regression.html#date-data",
    "href": "posts/regression.html#date-data",
    "title": "Song popularity prediction",
    "section": "",
    "text": "To capture seasonal popularity I’ll split the date value into three columns with year, month and day values. We first convert the string representation of track_album_release_date to date datatype and takeout the individual values separately.\n\nsongs[\"track_album_release_date\"] = pd.to_datetime(\n    songs[\"track_album_release_date\"], format=\"%Y-%m-%d\", errors=\"coerce\"\n)\n\nsongs[\"release_year\"] = songs[\"track_album_release_date\"].dt.year\nsongs[\"release_month\"] = songs[\"track_album_release_date\"].dt.month\nsongs[\"release_day\"] = songs[\"track_album_release_date\"].dt.day\nsongs = songs.drop([\"track_album_release_date\"], axis=\"columns\")"
  },
  {
    "objectID": "posts/regression.html#missing-values",
    "href": "posts/regression.html#missing-values",
    "title": "Song popularity prediction",
    "section": "",
    "text": "Let’s check if there are any missing values in our dataset\n\nsongs.isnull().values.any()\n\nTrue\n\n\nIt seems we’ve some\n\nsongs.isnull().sum()\n\ntrack_popularity        0\nplaylist_genre          0\nplaylist_subgenre       0\ndanceability            0\nenergy                  0\nkey                     0\nloudness                0\nmode                    0\nspeechiness             0\nacousticness            0\ninstrumentalness        0\nliveness                0\nvalence                 0\ntempo                   0\nduration_ms             0\nrelease_year         1886\nrelease_month        1886\nrelease_day          1886\ndtype: int64\n\n\nGiven the missing date values are arround 17% our data simply dropping the rows does not yeild an ideal outcome. Hence, let’s to fill the missing columns with the most common value in that feature\n\nsongs[\"release_year\"].fillna(\n    songs[\"release_year\"].value_counts().index[0], inplace=True\n)\nsongs[\"release_month\"].fillna(\n    songs[\"release_month\"].value_counts().index[0], inplace=True\n)\nsongs[\"release_day\"].fillna(songs[\"release_day\"].value_counts().index[0], inplace=True)\nsongs\n\n\n\n\n\n\n\n\ntrack_popularity\nplaylist_genre\nplaylist_subgenre\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\nrelease_year\nrelease_month\nrelease_day\n\n\n\n\n0\n66\npop\ndance pop\n0.748\n0.916\n6\n-2.634\n1\n0.0583\n0.102000\n0.000000\n0.0653\n0.5180\n122.036\n194754\n2019.0\n6.0\n14.0\n\n\n1\n67\npop\ndance pop\n0.726\n0.815\n11\n-4.969\n1\n0.0373\n0.072400\n0.004210\n0.3570\n0.6930\n99.972\n162600\n2019.0\n12.0\n13.0\n\n\n2\n70\npop\ndance pop\n0.675\n0.931\n1\n-3.432\n0\n0.0742\n0.079400\n0.000023\n0.1100\n0.6130\n124.008\n176616\n2019.0\n7.0\n5.0\n\n\n3\n60\npop\ndance pop\n0.718\n0.930\n7\n-3.778\n1\n0.1020\n0.028700\n0.000009\n0.2040\n0.2770\n121.956\n169093\n2019.0\n7.0\n19.0\n\n\n4\n69\npop\ndance pop\n0.650\n0.833\n1\n-4.672\n1\n0.0359\n0.080300\n0.000000\n0.0833\n0.7250\n123.976\n189052\n2019.0\n3.0\n5.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32828\n42\nedm\nprogressive electro house\n0.428\n0.922\n2\n-1.814\n1\n0.0936\n0.076600\n0.000000\n0.0668\n0.2100\n128.170\n204375\n2014.0\n4.0\n28.0\n\n\n32829\n20\nedm\nprogressive electro house\n0.522\n0.786\n0\n-4.462\n1\n0.0420\n0.001710\n0.004270\n0.3750\n0.4000\n128.041\n353120\n2013.0\n3.0\n8.0\n\n\n32830\n14\nedm\nprogressive electro house\n0.529\n0.821\n6\n-4.899\n0\n0.0481\n0.108000\n0.000001\n0.1500\n0.4360\n127.989\n210112\n2014.0\n4.0\n21.0\n\n\n32831\n15\nedm\nprogressive electro house\n0.626\n0.888\n2\n-3.361\n1\n0.1090\n0.007920\n0.127000\n0.3430\n0.3080\n128.008\n367432\n2014.0\n1.0\n1.0\n\n\n32832\n27\nedm\nprogressive electro house\n0.603\n0.884\n5\n-4.571\n0\n0.0385\n0.000133\n0.341000\n0.7420\n0.0894\n127.984\n337500\n2014.0\n3.0\n3.0\n\n\n\n\n32833 rows × 18 columns\n\n\n\nIf we check once more, we see there are no missing values anymore\n\nsongs.isnull().values.any()\n\nFalse"
  },
  {
    "objectID": "posts/regression.html#visualization",
    "href": "posts/regression.html#visualization",
    "title": "Song popularity prediction",
    "section": "",
    "text": "Let’s visualize the data and get a better understanding of the values ### Genre\n\nsns.countplot(songs, x=\"playlist_subgenre\", hue=\"playlist_genre\")\nplt.xticks(rotation=90)\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n        17, 18, 19, 20, 21, 22, 23]),\n [Text(0, 0, 'dance pop'),\n  Text(1, 0, 'post-teen pop'),\n  Text(2, 0, 'electropop'),\n  Text(3, 0, 'indie poptimism'),\n  Text(4, 0, 'hip hop'),\n  Text(5, 0, 'southern hip hop'),\n  Text(6, 0, 'gangster rap'),\n  Text(7, 0, 'trap'),\n  Text(8, 0, 'album rock'),\n  Text(9, 0, 'classic rock'),\n  Text(10, 0, 'permanent wave'),\n  Text(11, 0, 'hard rock'),\n  Text(12, 0, 'tropical'),\n  Text(13, 0, 'latin pop'),\n  Text(14, 0, 'reggaeton'),\n  Text(15, 0, 'latin hip hop'),\n  Text(16, 0, 'urban contemporary'),\n  Text(17, 0, 'hip pop'),\n  Text(18, 0, 'new jack swing'),\n  Text(19, 0, 'neo soul'),\n  Text(20, 0, 'electro house'),\n  Text(21, 0, 'big room'),\n  Text(22, 0, 'pop edm'),\n  Text(23, 0, 'progressive electro house')])\n\n\n\n\n\nFrom this we can see playlist_subgenre is full contained in playlist_genre hence we can remote the playlist_genre as the playlist_subgenre has more fine grained information\n\nsongs = songs.drop(\"playlist_genre\", axis=\"columns\")"
  },
  {
    "objectID": "posts/regression.html#one-hot-encoding",
    "href": "posts/regression.html#one-hot-encoding",
    "title": "Song popularity prediction",
    "section": "",
    "text": "Notice how we have a categorical feature (i.e., playlist_subgenre), we need to one-hot encode this column.\n\nplaylist_subgenre = pd.get_dummies(songs[\"playlist_subgenre\"])\nsongs = pd.concat([songs, playlist_subgenre], axis=\"columns\")\nsongs = songs.drop(\"playlist_subgenre\", axis=\"columns\")\nsongs\n\n\n\n\n\n\n\n\ntrack_popularity\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\n...\nnew jack swing\npermanent wave\npop edm\npost-teen pop\nprogressive electro house\nreggaeton\nsouthern hip hop\ntrap\ntropical\nurban contemporary\n\n\n\n\n0\n66\n0.748\n0.916\n6\n-2.634\n1\n0.0583\n0.102000\n0.000000\n0.0653\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n67\n0.726\n0.815\n11\n-4.969\n1\n0.0373\n0.072400\n0.004210\n0.3570\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n70\n0.675\n0.931\n1\n-3.432\n0\n0.0742\n0.079400\n0.000023\n0.1100\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n60\n0.718\n0.930\n7\n-3.778\n1\n0.1020\n0.028700\n0.000009\n0.2040\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n69\n0.650\n0.833\n1\n-4.672\n1\n0.0359\n0.080300\n0.000000\n0.0833\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32828\n42\n0.428\n0.922\n2\n-1.814\n1\n0.0936\n0.076600\n0.000000\n0.0668\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32829\n20\n0.522\n0.786\n0\n-4.462\n1\n0.0420\n0.001710\n0.004270\n0.3750\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32830\n14\n0.529\n0.821\n6\n-4.899\n0\n0.0481\n0.108000\n0.000001\n0.1500\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32831\n15\n0.626\n0.888\n2\n-3.361\n1\n0.1090\n0.007920\n0.127000\n0.3430\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32832\n27\n0.603\n0.884\n5\n-4.571\n0\n0.0385\n0.000133\n0.341000\n0.7420\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n32833 rows × 40 columns\n\n\n\n\n\n\ncor = songs.corr()\nsns.heatmap(cor)\ncor = cor.abs()\n\n\n\n\nLet’s see the top 5 correclations\n\ncor_values = (\n    cor.where(np.triu(np.ones(cor.shape), k=1).astype(bool))\n    .stack()\n    .sort_values(ascending=False)\n)\ntop_n = 5\nprint(\"Top 5 correlation values are\")\nfor i, v in cor_values[:top_n].items():\n    print(\"{} - {} \".format(i, v))\n\nTop 5 correlation values are\n('energy', 'loudness') - 0.6766245234442312 \n('energy', 'acousticness') - 0.5397446301909388 \n('loudness', 'acousticness') - 0.36163816507093 \n('danceability', 'valence') - 0.3305232570910807 \n('release_month', 'release_day') - 0.3080926045288983 \n\n\nAs expected ‘energy’, ‘loudness’ are correlated to a degree but none of these correlation values are significantly high enought to warrant removal befor the training so we keep all our features."
  },
  {
    "objectID": "posts/regression.html#target-and-data",
    "href": "posts/regression.html#target-and-data",
    "title": "Song popularity prediction",
    "section": "",
    "text": "Let’s now split features into X and Y with the later being our target for prediction\n\ny = songs[\"track_popularity\"].to_numpy()\nx = songs.drop(\"track_popularity\", axis=\"columns\").to_numpy()"
  },
  {
    "objectID": "posts/regression.html#normalization",
    "href": "posts/regression.html#normalization",
    "title": "Song popularity prediction",
    "section": "",
    "text": "Normalize the features to bring them all to 0-1 range\n\nx_normalized = preprocessing.normalize(x)"
  },
  {
    "objectID": "posts/regression.html#training-and-test-split-and-data",
    "href": "posts/regression.html#training-and-test-split-and-data",
    "title": "Song popularity prediction",
    "section": "",
    "text": "Let’s split our data into training and test sets with 80 to 20 ratio.\n\nx_train, x_test, y_train, y_test = train_test_split(x_normalized, y, test_size=0.20)\nprint(\"Training X Shape: {}\".format(x_train.shape))\nprint(\"Training Y Shape: {}\".format(y_train.shape))\nprint(\"Test X Shape: {}\".format(x_test.shape))\nprint(\"Test y Shape: {}\".format(y_test.shape))\n\nTraining X Shape: (26266, 39)\nTraining Y Shape: (26266,)\nTest X Shape: (6567, 39)\nTest y Shape: (6567,)"
  },
  {
    "objectID": "posts/regression.html#regression-models",
    "href": "posts/regression.html#regression-models",
    "title": "Song popularity prediction",
    "section": "Regression models",
    "text": "Regression models\nWe will try to train on multiple algorithms and perform hyperparamter tuining on each to find the best performing model\n\nmodels = {\n    # \"ExtraTreesRegressor\": {\n    #     \"model\": ExtraTreesRegressor(),\n    #     \"params\": {\n    #         \"n_estimators\": [25, 50, 75, 100],\n    #         \"max_depth\": [5, 10, 20],\n    #         \"min_samples_split\": [3, 5, 7],\n    #         \"min_samples_leaf\": [3, 4, 5],\n    #     },\n    # },\n    \"adaboost\": {\n        \"model\": AdaBoostRegressor(),\n        \"params\": {\n            \"n_estimators\": [25, 50, 75, 100],\n            \"loss\": [\"linear\", \"square\", \"exponential\"],\n        },\n    },\n    # \"svm\": {\n    #     \"model\": svm.SVR(),\n    #     \"params\": {\n    #         \"degree\": [2, 4, 6],\n    #         \"C\": [0.5, 1.0, 1.5],\n    #     },\n    # },\n    # \"knr\": {\n    #     \"model\": KNeighborsRegressor(),\n    #     \"params\": {\n    #         \"n_neighbors\": [3, 5, 8],\n    #         \"weights\": [\"uniform\", \"distance\"],\n    #         \"leaf_size\": [20, 40, 60],\n    #     },\n    # },\n    # \"rnr\": {\n    #     \"model\": RadiusNeighborsRegressor(),\n    #     \"params\": {\n    #         \"radius\": [0.5, 1.0, 1.5],\n    #         \"weights\": [\"uniform\", \"distance\"],\n    #         \"leaf_size\": [20, 40, 60],\n    #     },\n    # },\n    # \"RandomForestRegressor\": {\n    #     \"model\": RandomForestRegressor(),\n    #     \"params\": {\n    #         \"n_estimators\": [25, 50, 75, 100],\n    #         \"max_depth\": [5, 10, 20],\n    #         \"min_samples_split\": [3, 5, 7],\n    #         \"min_samples_leaf\": [3, 4, 5],\n    #     },\n    # },\n    # \"boosting\": {\n    #     \"model\": GradientBoostingRegressor(),\n    #     \"params\": {\n    #         \"n_estimators\": [25, 50, 75, 100],\n    #         \"loss\": [\"linear\", \"square\", \"exponential\"],\n    #         \"min_samples_split\": [3, 5, 7],\n    #     },\n    # },\n}\n\nNow let’s peform the training and hyperparamter tuining. Start by preparing a variable to store the best model and it’s param values in\n\nbest_model, best_params, best_performance = None, None, float(\"inf\")\n\nLoop through each model and perform randomized search. Note: Running the following code takes a lot of time so what I did was run it on google colab and bring the results here.\n\nfor name, info in models.items():\n    random_search = RandomizedSearchCV(\n        info[\"model\"],\n        info[\"params\"],\n        scoring=\"neg_mean_squared_error\",\n        cv=5,\n        n_jobs=-1,\n    )\n\n    random_search.fit(x_train, y_train)\n\n    # Get the best model and its parameters\n    winning_model = random_search.best_estimator_\n    winning_model_params = random_search.best_params_\n    # Check the performance of the best model using the test data.\n    # We need to round the predicted values because the target feature is integer\n    y_pred = np.round(winning_model.predict(x_test))\n    error = mean_squared_error(y_test, y_pred)\n    # Update the best_model_info if the current model has lower MSE\n    if error &lt; best_performance:\n        best_performance = error\n        best_model = winning_model\n        best_params = winning_model_params\n\n# Print the best model information\nprint(f\"Best Model: {best_model}\")\nprint(f\"Best Parameters: {best_params}\")\n\nBest Model: AdaBoostRegressor()\nBest Parameters: {'n_estimators': 50, 'loss': 'linear'}"
  }
]